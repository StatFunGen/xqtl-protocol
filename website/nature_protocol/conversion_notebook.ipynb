{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4061b995-e011-4978-a36e-3e60e0cb8a1b",
   "metadata": {},
   "source": [
    "# Nature Protocol Manuscript Conversion\n",
    "Code to convert xQTL pipeline notebooks to format for Nature Protocol paper    \n",
    "Information on format [here](https://www.nature.com/nprot/for-authors/protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cfc19551-4713-4abe-8da5-677608ba06b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import yaml\n",
    "import nbformat\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8500932-a127-478d-b3b1-245ad5c23568",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f0db4a23-9d6b-4023-9231-995ac333b574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_to_markdown(output_markdown_file, content):\n",
    "    with open(output_markdown_file, 'a') as md_file:\n",
    "        md_file.write(content + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "dafd9eba-8de2-4fd9-b54e-ef334a8f44bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manuscript_format_notebook = f\"example_manuscript.ipynb\"\n",
    "output_markdown_file = f\"output_markdown.md\"\n",
    "output_latex_file = f\"output_latex.tex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2b546971-49f3-4758-aa83-b37172c829d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "major_sections_keep = [\n",
    "    'Reference data',\n",
    "    'Molecular Phenotypes',\n",
    "    'Data Pre-processing',\n",
    "    'QTL Association Testing',\n",
    "    'Multivariate Mixture Model',\n",
    "    'Multiomics Regression Models',\n",
    "    'GWAS Integration',\n",
    "    'Enrichment and Validation'\n",
    "]\n",
    "\n",
    "miniprotocol_keep = [\n",
    "    '../../code/reference_data/reference_data.ipynb',\n",
    "    '../../code/molecular_phenotypes/bulk_expression.ipynb',\n",
    "    '../../code/molecular_phenotypes/splicing.ipynb',\n",
    "    '../../code/data_preprocessing/genotype_preprocessing.ipynb',\n",
    "    '../../code/data_preprocessing/phenotype_preprocessing.ipynb',\n",
    "    '../../code/data_preprocessing/covariate_preprocessing.ipynb',\n",
    "    '../../code/association_scan/qtl_association_testing.ipynb',\n",
    "    '../../code/multivariate_genome/multivariate_mixture_vignette.ipynb',\n",
    "    '../../code/mnm_analysis/mnm_miniprotocol.ipynb',\n",
    "    '../../code/pecotmr_integration/SuSiE_enloc.ipynb',\n",
    "    '../../code/pecotmr_integration/twas_ctwas.ipynb',\n",
    "    '../../code/mnm_analysis/mnm_methods/colocboost.ipynb',\n",
    "    '../../code/enrichment/eoo_enrichment.ipynb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "11eac698-c7e7-45fa-a8ce-3c99ea96f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_skip = [\n",
    "    '../../code/association_scan/APEX/APEX.ipynb',\n",
    "    #'../../code/data_preprocessing/genotype/genotype_formatting.ipynb',\n",
    "    '../../code/data_preprocessing/genotype/GRM.ipynb',\n",
    "    '../../code/data_preprocessing/genotype/GWAS_QC.ipynb',\n",
    "    '../../code/data_preprocessing/genotype/PCA.ipynb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f7a51bb5-c3f7-4e60-a6c1-b44819aa93d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#list of miniprotocols to simplify for each section of the manuscript\n",
    "miniprotocol_simplify = [\n",
    "    #'../../code/reference_data/reference_data.ipynb',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "972eeb04-3019-4673-b9cf-66950842aa25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to your YAML file\n",
    "yaml_file_path = \"../_toc.yml\"\n",
    "\n",
    "# Load the YAML file\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    yaml_data = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c37b6088-aa95-4804-95f2-90ac50b28e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference data\n",
      "[{'file': 'code/reference_data/reference_data.ipynb', 'sections': [{'file': 'code/reference_data/reference_data_preparation.ipynb'}, {'file': 'code/reference_data/generalized_TADB.ipynb'}, {'file': 'code/reference_data/ld_prune_reference.ipynb'}, {'file': 'code/reference_data/ld_reference_generation.ipynb'}]}]\n",
      "Molecular Phenotypes\n",
      "[{'file': 'code/molecular_phenotypes/bulk_expression.ipynb', 'sections': [{'file': 'code/molecular_phenotypes/calling/RNA_calling.ipynb'}, {'file': 'code/molecular_phenotypes/QC/bulk_expression_QC.ipynb'}, {'file': 'code/molecular_phenotypes/QC/bulk_expression_normalization.ipynb'}]}, {'file': 'code/molecular_phenotypes/methylation.ipynb', 'sections': [{'file': 'code/molecular_phenotypes/calling/methylation_calling.ipynb'}]}, {'file': 'code/molecular_phenotypes/splicing.ipynb', 'sections': [{'file': 'code/molecular_phenotypes/calling/splicing_calling.ipynb'}, {'file': 'code/molecular_phenotypes/QC/splicing_normalization.ipynb'}]}]\n",
      "Data Pre-processing\n",
      "[{'file': 'code/data_preprocessing/genotype_preprocessing.ipynb', 'sections': [{'file': 'code/data_preprocessing/genotype/VCF_QC.ipynb'}, {'file': 'code/data_preprocessing/genotype/GWAS_QC.ipynb'}, {'file': 'code/data_preprocessing/genotype/PCA.ipynb'}, {'file': 'code/data_preprocessing/genotype/GRM.ipynb'}, {'file': 'code/data_preprocessing/genotype/genotype_formatting.ipynb'}]}, {'file': 'code/data_preprocessing/phenotype_preprocessing.ipynb', 'sections': [{'file': 'code/data_preprocessing/phenotype/gene_annotation.ipynb'}, {'file': 'code/data_preprocessing/phenotype/phenotype_imputation.ipynb'}, {'file': 'code/data_preprocessing/phenotype/phenotype_formatting.ipynb'}]}, {'file': 'code/data_preprocessing/covariate_preprocessing.ipynb', 'sections': [{'file': 'code/data_preprocessing/covariate/covariate_formatting.ipynb'}, {'file': 'code/data_preprocessing/covariate/covariate_hidden_factor.ipynb'}]}]\n",
      "QTL Association Testing\n",
      "[{'file': 'code/association_scan/qtl_association_testing.ipynb', 'sections': [{'file': 'code/association_scan/TensorQTL/TensorQTL.ipynb'}, {'file': 'code/association_scan/quantile_models/qr_and_twas.ipynb'}]}, {'file': 'code/association_scan/qtl_association_postprocessing.ipynb'}]\n",
      "Multivariate Mixture Model\n",
      "[{'file': 'code/multivariate_genome/multivariate_mixture_vignette.ipynb', 'sections': [{'file': 'code/multivariate_genome/MASH/mixture_prior.ipynb'}, {'file': 'code/multivariate_genome/MASH/mash_fit.ipynb'}]}]\n",
      "Multiomics Regression Models\n",
      "[{'file': 'code/mnm_analysis/mnm_miniprotocol.ipynb', 'sections': [{'file': 'code/mnm_analysis/univariate_fine_mapping_twas_vignette.ipynb'}, {'file': 'code/mnm_analysis/multivariate_multigene_fine_mapping_vignette.ipynb'}, {'file': 'code/mnm_analysis/univariate_fine_mapping_fsusie_vignette.ipynb'}, {'file': 'code/mnm_analysis/multivariate_fine_mapping_vignette.ipynb'}, {'file': 'code/mnm_analysis/summary_stats_finemapping_vignette.ipynb'}, {'file': 'code/mnm_analysis/mnm_methods/mnm_regression.ipynb'}, {'file': 'code/mnm_analysis/mnm_methods/rss_analysis.ipynb'}]}, {'file': 'code/mnm_analysis/mnm_postprocessing.ipynb'}]\n",
      "GWAS Integration\n",
      "[{'file': 'code/pecotmr_integration/SuSiE_enloc.ipynb'}, {'file': 'code/pecotmr_integration/twas_ctwas.ipynb'}, {'file': 'code/mnm_analysis/mnm_methods/colocboost.ipynb'}]\n",
      "Enrichment and Validation\n",
      "[{'file': 'code/enrichment/eoo_enrichment.ipynb'}, {'file': 'code/enrichment/gsea.ipynb'}, {'file': 'code/enrichment/gregor.ipynb'}, {'file': 'code/enrichment/sldsc_enrichment.ipynb'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Reference data': ['../../code/reference_data/reference_data.ipynb'],\n",
       " 'Molecular Phenotypes': ['../../code/molecular_phenotypes/bulk_expression.ipynb',\n",
       "  '../../code/molecular_phenotypes/splicing.ipynb'],\n",
       " 'Data Pre-processing': ['../../code/data_preprocessing/genotype_preprocessing.ipynb',\n",
       "  '../../code/data_preprocessing/phenotype_preprocessing.ipynb',\n",
       "  '../../code/data_preprocessing/covariate_preprocessing.ipynb'],\n",
       " 'QTL Association Testing': ['../../code/association_scan/qtl_association_testing.ipynb'],\n",
       " 'Multivariate Mixture Model': ['../../code/multivariate_genome/multivariate_mixture_vignette.ipynb'],\n",
       " 'Multiomics Regression Models': ['../../code/mnm_analysis/mnm_miniprotocol.ipynb'],\n",
       " 'GWAS Integration': ['../../code/pecotmr_integration/SuSiE_enloc.ipynb',\n",
       "  '../../code/pecotmr_integration/twas_ctwas.ipynb',\n",
       "  '../../code/mnm_analysis/mnm_methods/colocboost.ipynb'],\n",
       " 'Enrichment and Validation': ['../../code/enrichment/eoo_enrichment.ipynb']}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictionary with keys being the names of the major sections and \n",
    "#values being lists of the miniprotocol notebooks for that major section\n",
    "#these values should match the keys used in the 'miniprotocol_dict' below\n",
    "major_section_dict = {}\n",
    "for part in yaml_data['parts']:\n",
    "    caption = part['caption']\n",
    "    #filter\n",
    "    if caption in major_sections_keep: \n",
    "        print(caption)\n",
    "        print(part['chapters'])\n",
    "        miniprotocols = [f\"../../{file['file']}\" for file in part['chapters'] if f\"../../{file['file']}\" in miniprotocol_keep]\n",
    "        major_section_dict[caption] = miniprotocols\n",
    "major_section_dict\n",
    "\n",
    "###used to be in this format:\n",
    "#major_section_dict =  {\n",
    "#    \"Molecular Phenotype Quantification\": [\n",
    "#        f\"{WRKDIR}/bulk_expression/bulk_expression.ipynb\",\n",
    "#        f\"{WRKDIR}/splicing/splicing.ipynb\"\n",
    "#    ],\n",
    "#    \"Data Pre-Processing\":[\n",
    "#        f\"{WRKDIR}/data_preprocessing/covariate/covariate_preprocessing.ipynb\",\n",
    "#        ],\n",
    "#    \"QTL Association Analysis\":[],\n",
    "#    \"Integrative Analysis\":[]\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c5bcec57-0cd5-428e-9959-a1cc418ea172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'../../code/reference_data/reference_data.ipynb': ['../../code/reference_data/reference_data_preparation.ipynb',\n",
       "  '../../code/reference_data/generalized_TADB.ipynb',\n",
       "  '../../code/reference_data/ld_prune_reference.ipynb',\n",
       "  '../../code/reference_data/ld_reference_generation.ipynb'],\n",
       " '../../code/molecular_phenotypes/bulk_expression.ipynb': ['../../code/molecular_phenotypes/calling/RNA_calling.ipynb',\n",
       "  '../../code/molecular_phenotypes/QC/bulk_expression_QC.ipynb',\n",
       "  '../../code/molecular_phenotypes/QC/bulk_expression_normalization.ipynb'],\n",
       " '../../code/molecular_phenotypes/splicing.ipynb': ['../../code/molecular_phenotypes/calling/splicing_calling.ipynb',\n",
       "  '../../code/molecular_phenotypes/QC/splicing_normalization.ipynb'],\n",
       " '../../code/data_preprocessing/genotype_preprocessing.ipynb': ['../../code/data_preprocessing/genotype/VCF_QC.ipynb',\n",
       "  '../../code/data_preprocessing/genotype/genotype_formatting.ipynb'],\n",
       " '../../code/data_preprocessing/phenotype_preprocessing.ipynb': ['../../code/data_preprocessing/phenotype/gene_annotation.ipynb',\n",
       "  '../../code/data_preprocessing/phenotype/phenotype_imputation.ipynb',\n",
       "  '../../code/data_preprocessing/phenotype/phenotype_formatting.ipynb'],\n",
       " '../../code/data_preprocessing/covariate_preprocessing.ipynb': ['../../code/data_preprocessing/covariate/covariate_formatting.ipynb',\n",
       "  '../../code/data_preprocessing/covariate/covariate_hidden_factor.ipynb'],\n",
       " '../../code/association_scan/qtl_association_testing.ipynb': ['../../code/association_scan/TensorQTL/TensorQTL.ipynb',\n",
       "  '../../code/association_scan/quantile_models/qr_and_twas.ipynb'],\n",
       " '../../code/multivariate_genome/multivariate_mixture_vignette.ipynb': ['../../code/multivariate_genome/MASH/mixture_prior.ipynb',\n",
       "  '../../code/multivariate_genome/MASH/mash_fit.ipynb'],\n",
       " '../../code/mnm_analysis/mnm_miniprotocol.ipynb': ['../../code/mnm_analysis/univariate_fine_mapping_twas_vignette.ipynb',\n",
       "  '../../code/mnm_analysis/multivariate_multigene_fine_mapping_vignette.ipynb',\n",
       "  '../../code/mnm_analysis/univariate_fine_mapping_fsusie_vignette.ipynb',\n",
       "  '../../code/mnm_analysis/multivariate_fine_mapping_vignette.ipynb',\n",
       "  '../../code/mnm_analysis/summary_stats_finemapping_vignette.ipynb',\n",
       "  '../../code/mnm_analysis/mnm_methods/mnm_regression.ipynb',\n",
       "  '../../code/mnm_analysis/mnm_methods/rss_analysis.ipynb'],\n",
       " '../../code/pecotmr_integration/SuSiE_enloc.ipynb': [],\n",
       " '../../code/pecotmr_integration/twas_ctwas.ipynb': [],\n",
       " '../../code/mnm_analysis/mnm_methods/colocboost.ipynb': [],\n",
       " '../../code/enrichment/eoo_enrichment.ipynb': []}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictionary with keys being the mininprotocol notebooks and \n",
    "#values being lists of the module notebooks for the miniprotocol\n",
    "miniprotocol_dict = {}\n",
    "for part in yaml_data['parts']:\n",
    "    for chapter in part['chapters']:\n",
    "        miniprotocol = f\"../../{chapter['file']}\"\n",
    "        #filter\n",
    "        if miniprotocol in miniprotocol_keep:\n",
    "            if 'sections' in chapter:\n",
    "                miniprotocol_dict[miniprotocol] = [f\"../../{module['file']}\" for module in chapter['sections']]\n",
    "            else:\n",
    "                miniprotocol_dict[miniprotocol] = []\n",
    "\n",
    "            miniprotocol_dict[miniprotocol] = [x for x in miniprotocol_dict[miniprotocol] if x not in module_skip]\n",
    "\n",
    "            miniprotocol_dict[miniprotocol]\n",
    "            \n",
    "miniprotocol_dict\n",
    "###used to be in this format:\n",
    "#miniprotocol_dict = {\n",
    "#    f\"{WRKDIR}/bulk_expression/bulk_expression.ipynb\":[\n",
    "#        f\"{WRKDIR}/bulk_expression/RNA_calling.ipynb\",\n",
    "#        f\"{WRKDIR}/bulk_expression/bulk_expression_QC.ipynb\",\n",
    "#        f\"{WRKDIR}/bulk_expression/bulk_expression_normalization.ipynb\"\n",
    "#    ],\n",
    "#    f\"{WRKDIR}/splicing/splicing.ipynb\":[\n",
    "#        f\"{WRKDIR}/splicing/splicing_calling.ipynb\",\n",
    "#        f\"{WRKDIR}/splicing/splicing_normalization.ipynb\"\n",
    "#    ],\n",
    "#    f\"{WRKDIR}/data_preprocessing/covariate/covariate_preprocessing.ipynb\":[\n",
    "#        f\"{WRKDIR}/data_preprocessing/covariate/covariate_formatting.ipynb\",\n",
    "#        f\"{WRKDIR}/data_preprocessing/covariate/covariate_hidden_factor.ipynb\"\n",
    "#    ]\n",
    "#}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ba8557c3-0e3f-45bc-8f59-fd05cdbb3171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read the miniprotocol notebook and get the title (should be first cell)\n",
    "def get_miniprot_notebook_title(notebook_str):\n",
    "    with open(notebook_str, 'r') as miniprot_content:\n",
    "        miniprot_notebook = json.load(miniprot_content)\n",
    "        for i, cell in enumerate(miniprot_notebook[\"cells\"]):\n",
    "            \n",
    "            if cell[\"cell_type\"] == \"markdown\":\n",
    "                \n",
    "                if len(cell[\"source\"]) >0:\n",
    "                    if cell[\"source\"][0].startswith(\"# \"):\n",
    "                        miniprot_title = cell[\"source\"][0]\n",
    "                        miniprot_title = miniprot_title.replace(\"#\",\"\")\n",
    "                        print(miniprot_title)\n",
    "                        return miniprot_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69e8a5-bf94-46db-86f9-a3f00523dafa",
   "metadata": {},
   "source": [
    "## Experimental Design Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "367f05d3-34cd-4ae2-ae02-0faed6a51fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the content for the experimental design of the manuscript by going through each miniprotocol for the \n",
    "#title and through each module for the descriptions\n",
    "def content_for_exp_design():\n",
    "    return_content = []\n",
    "    major_step = 1\n",
    "    #iterate over major sections dict\n",
    "    for major_section in major_section_dict.keys():\n",
    "        return_content.append(f\"#### {major_section} (Step {major_step})\")\n",
    "        major_step += 1\n",
    "        \n",
    "\n",
    "\n",
    "        miniprot_step = 1\n",
    "        #iterate over miniprotocols in each major section\n",
    "        for miniprot in major_section_dict[major_section]:\n",
    "            \n",
    "            #get the title of the miniprotocol\n",
    "            miniprot_title = f\"##### {chr(ord('@')+miniprot_step)}. {get_miniprot_notebook_title(miniprot)}\"\n",
    "            return_content.append(miniprot_title)\n",
    "            \n",
    "            #don't do much if this is one of the miniprotocols sections we want to simplify\n",
    "            if miniprot in miniprotocol_simplify:\n",
    "                return_content.append(f\"Please refer to the protocol website for more information on this miniprotocol.\")\n",
    "            else:\n",
    "                #if in this section, then just read from description from the miniprotocol\n",
    "                if major_section == \"Advanced cis-QTL Analysis\":\n",
    "                    with open(miniprot, 'r') as miniprot_content:\n",
    "                            #flag to get the next cell (or cell after description header in this case)\n",
    "                            get_next_cell = False\n",
    "                            miniprot_notebook = json.load(miniprot_content)\n",
    "                            for i, cell in enumerate(miniprot_notebook[\"cells\"]):\n",
    "                                if cell[\"cell_type\"] == \"markdown\":\n",
    "                                    if len(cell[\"source\"]) >0:\n",
    "                                        content = cell[\"source\"][0]\n",
    "    \n",
    "    \n",
    "                                        if content.startswith(\"##\") and get_next_cell:\n",
    "                                            #reset the flag\n",
    "                                            get_next_cell = False\n",
    "                                        #add the description text\n",
    "                                        if get_next_cell:\n",
    "    \n",
    "                                            return_content.append(\"\\n\" + \"\\n\".join(cell[\"source\"]))\n",
    "                                        if cell[\"source\"][0].startswith(\"## Description\"):\n",
    "                                            #tells us to get the next cell after the this one for the output\n",
    "                                            get_next_cell = True\n",
    "                else:\n",
    "                    #iterate over modules in each miniprotocol                                    \n",
    "                    for module in miniprotocol_dict[miniprot]:\n",
    "                        with open(module, 'r') as module_content:\n",
    "                            #flag to get the next cell (or cell after description header in this case)\n",
    "                            get_next_cell = False\n",
    "                            module_notebook = json.load(module_content)\n",
    "                            for i, cell in enumerate(module_notebook[\"cells\"]):\n",
    "                                if cell[\"cell_type\"] == \"markdown\":\n",
    "                                    if len(cell[\"source\"]) >0:\n",
    "                                        content = cell[\"source\"][0]\n",
    "    \n",
    "    \n",
    "                                        if content.startswith(\"##\") and get_next_cell:\n",
    "                                            #reset the flag\n",
    "                                            get_next_cell = False\n",
    "                                        #add the description text\n",
    "                                        if get_next_cell:\n",
    "    \n",
    "                                            return_content.append(\"\\n\" + \"\\n\".join(cell[\"source\"]))\n",
    "                                        if cell[\"source\"][0].startswith(\"## Description\"):\n",
    "                                            #tells us to get the next cell after the this one for the output\n",
    "                                            get_next_cell = True\n",
    "\n",
    "            miniprot_step += 1\n",
    "    return \"\\n\".join(return_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f69c6e-9a35-44cf-8a3e-ee0bbb3a322e",
   "metadata": {},
   "source": [
    "## Procedure Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8ad43157-4e5b-4dec-be56-59a52b736f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_for_procedure():\n",
    "    return_content = []\n",
    "    major_step = 0\n",
    "    for major_section in major_section_dict.keys():\n",
    "        major_step += 1\n",
    "        #print(major_section)\n",
    "        return_content.append(f\"### {major_step}. {major_section}\\n\")\n",
    "        for miniprot in major_section_dict[major_section]:\n",
    "            #print(miniprot)\n",
    "            with open(miniprot, \"r\", encoding=\"utf-8\") as nb_file:\n",
    "                nb_content = nbformat.read(nb_file, as_version=4)\n",
    "                for i, cell in enumerate(nb_content.cells):\n",
    "                    if cell.cell_type == \"markdown\":\n",
    "                        for line in cell.source.splitlines():\n",
    "                            if line.strip().startswith(\"Timing\"):\n",
    "                                return_content.append(line)\n",
    "                            if line.strip().startswith(\"# \"):\n",
    "                                return_content.append(\"###\"+line)\n",
    "    \n",
    "    \n",
    "    \n",
    "                    if cell.cell_type == \"code\":\n",
    "                        if cell.source.startswith(\"sos run\") or cell.source.startswith(\"!sos run\") or cell.source.startswith(\"echo\"):\n",
    "                            if not cell.source.endswith(\" -h\"):\n",
    "                                title_text = \"\"\n",
    "                                if i > 0 and nb_content.cells[i-1].cell_type == \"markdown\":\n",
    "                                    title_text = nb_content.cells[i-1].source.strip()\n",
    "                                    title_text_no_link = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', title_text)\n",
    "                                    return_content.append(\"##\" + title_text_no_link + \"\\n\\n\")\n",
    "                                    return_content.append(\"```python\\n\")\n",
    "                                    return_content.append(cell.source.strip() + \"\\n\")\n",
    "                                    return_content.append(\"```\\n\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "                            \n",
    "    return \"\\n\".join(return_content)\n",
    "        #return_content.append(f\"#### {major_section} (Step {major_step})\")\n",
    "        #major_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d998f3f-94cc-4132-9444-9ebe30a7119c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17dee33f-0b4d-4b48-b63f-3fc223fadc9c",
   "metadata": {},
   "source": [
    "## Timing Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0b560829-a17f-4c19-be4a-1414ab8fb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the content for the timing seection of the manuscript by going through each miniprotocol\n",
    "def content_for_timing():\n",
    "    return_content = []\n",
    "    return_content.append(f\"| Step | Time|\")\n",
    "    return_content.append(f\"|------|-----|\")\n",
    "    \n",
    "    #iterate over major sections dict\n",
    "    for major_section in major_section_dict.keys():\n",
    "        table_row = \"\"\n",
    "        table_row = table_row + f\"|{major_section}|X minutes|\"\n",
    "        return_content.append(table_row.replace('\\n',''))\n",
    "\n",
    "    return \"\\n\".join(return_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bb5d8350-f3c5-4215-9546-5269a36af31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the content for the timing seection of the manuscript by going through each miniprotocol\n",
    "def content_for_timing_old():\n",
    "    return_content = []\n",
    "    return_content.append(f\"| Step(Major Section) | Substep(Miniprotocol) | Time|\")\n",
    "    return_content.append(f\"|------|-----|----|\")\n",
    "    \n",
    "    #iterate over major sections dict\n",
    "    for major_section in major_section_dict.keys():\n",
    "        table_row = \"\"\n",
    "        table_row = table_row + f\"|{major_section}\"\n",
    "        #iterate over miniprotocols in each major section\n",
    "        for miniprot in major_section_dict[major_section]:\n",
    "            with open(miniprot, 'r') as miniprot_content:\n",
    "                #get the title of the miniprotocol\n",
    "                miniprot_title = f\"{get_miniprot_notebook_title(miniprot)}\".replace(\"\\n\",\"\")\n",
    "                table_row = table_row + f\"|{miniprot_title}\"\n",
    "\n",
    "                miniprot_notebook = json.load(miniprot_content)\n",
    "                for i, cell in enumerate(miniprot_notebook[\"cells\"]):\n",
    "                    if cell[\"cell_type\"] == \"markdown\":\n",
    "                        if len(cell[\"source\"]) >0:\n",
    "                            content = cell[\"source\"][0]\n",
    "                            if content.startswith(\"#### Miniprotocol Timing\"):\n",
    "                                for c in cell[\"source\"]:\n",
    "                                    if c.startswith(\"Timing\"):\n",
    "                                        table_row = table_row +f\"|{c.replace('Timing','')}|\"\n",
    "                                        return_content.append(table_row.replace('\\n',''))\n",
    "                                        break\n",
    "                table_row = \"| \"\n",
    "        table_row = \"\"\n",
    "    return \"\\n\".join(return_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb44d5c-6222-42a5-8b2f-b8e73506d831",
   "metadata": {},
   "source": [
    "## Anticipated Results Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b0ac630f-5c03-4369-98d4-6b525623b6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the content for the anticipated results seection of the manuscript by going through each miniprotocol\n",
    "def content_for_anticipated_results():\n",
    "    \n",
    "    return_content = []\n",
    "    \n",
    "    \n",
    "    #iterate over major sections dict\n",
    "    for major_section in major_section_dict.keys():\n",
    "        miniprot_step = 1\n",
    "        on_results = False\n",
    "        #iterate over miniprotocols in each major section\n",
    "        for miniprot in major_section_dict[major_section]:\n",
    "            #get the title of the miniprotocol\n",
    "            #miniprot_title = f\"#### {chr(ord('@')+miniprot_step)}. {get_miniprot_notebook_title(miniprot)}\"\n",
    "            miniprot_title = f\"#### {get_miniprot_notebook_title(miniprot)}\"\n",
    "            return_content.append(miniprot_title)\n",
    "            miniprot_step += 1\n",
    "\n",
    "            #don't do much if this is one of the miniprotocols sections we want to simplify\n",
    "            if miniprot in miniprotocol_simplify:\n",
    "                return_content.append(f\"Please refer to the protocol website for more information on these results.\")\n",
    "            else:\n",
    "\n",
    "                    \n",
    "                with open(miniprot, 'r') as miniprot_content:\n",
    "    \n",
    "                    miniprot_notebook = json.load(miniprot_content)\n",
    "                    for i, cell in enumerate(miniprot_notebook[\"cells\"]):\n",
    "                        if cell[\"cell_type\"] == \"markdown\":\n",
    "                            if len(cell[\"source\"]) >0:\n",
    "                                content = cell[\"source\"][0]\n",
    "                                if on_results:\n",
    "                                    return_content.append(content)\n",
    "                                    on_results = False\n",
    "                                if content.startswith(\"## Anticipated Results\"):\n",
    "                                    on_results = True\n",
    "    return \"\\n\".join(return_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b024dd-94ea-4655-9c41-e04c27eb56cb",
   "metadata": {},
   "source": [
    "## Figures Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "af3d8750-d330-4182-8c27-039101baf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the content for the figures by going through the anticipated results part of each miniprotocol\n",
    "def content_for_figures():\n",
    "    \n",
    "    return_content = []\n",
    "    \n",
    "    \n",
    "    #iterate over major sections dict\n",
    "    for major_section in major_section_dict.keys():\n",
    "        #iterate over miniprotocols in each major section\n",
    "        for miniprot in major_section_dict[major_section]:\n",
    "\n",
    "            #don't do much if this is one of the miniprotocols sections we want to simplify\n",
    "            #if miniprot in miniprotocol_simplify:\n",
    "            #    return_content.append(f\"Please refer to the protocol website for more information on these results.\")\n",
    "            if miniprot not in miniprotocol_simplify:\n",
    "\n",
    "                    \n",
    "                with open(miniprot, 'r') as miniprot_content:\n",
    "\n",
    "                    get_next_cell = False\n",
    "                    miniprot_notebook = json.load(miniprot_content)\n",
    "                    for i, cell in enumerate(miniprot_notebook[\"cells\"]):\n",
    "                        if cell[\"cell_type\"] == \"markdown\":\n",
    "                            if len(cell[\"source\"]) >0:\n",
    "                                content = cell[\"source\"][0]\n",
    "\n",
    "\n",
    "\n",
    "                                if content.startswith(\"##\") and get_next_cell:\n",
    "                                    #reset the flag\n",
    "                                    get_next_cell = False\n",
    "                                #add the description text\n",
    "                                if (get_next_cell and \"png\" in str(cell[\"source\"])):\n",
    "                                    return_content.append(\"\\n\" + \"\\n\".join(cell[\"source\"]))\n",
    "                                if cell[\"source\"][0].startswith(\"## Anticipated Results\"):\n",
    "                                    #tells us to get the next cell after the this one for the output\n",
    "                                    get_next_cell = True\n",
    "\n",
    "    return \"\\n\".join(return_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092cfbe8-19ed-4e8d-b201-8decdcfbeee1",
   "metadata": {},
   "source": [
    "## References Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8693ff42-3942-44a7-8ea8-8c2aa04446a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def content_for_references():\n",
    "    \n",
    "    return_content = []\n",
    "    \n",
    "    #hold the references in a list fromatted as \"author year doi\"\n",
    "    #use this to check for duplicates and to order correctly\n",
    "    ref_list = []\n",
    "    \n",
    "    \n",
    "    #for now just check through the experimental design text \n",
    "    exp_design = content_for_exp_design()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # look for the pattern for references. Should look like:\n",
    "    #[cf. Signal et al (2022)](https://doi.org/10.1186/s12859-022-04572-7)\n",
    "    pattern = r'\\[cf(.*?)\\]\\((.*?)\\)'\n",
    "\n",
    "    # Use re.findall to find all occurrences of the pattern in the input string\n",
    "    matches = re.findall(pattern, exp_design)\n",
    "\n",
    "    # look through all the matches and add to the ref_list\n",
    "    for match in matches:\n",
    "        ref_text = match[0]\n",
    "        doi = match[1]\n",
    "        year = re.findall(r'\\b\\d{4}\\b', match[0])[0]\n",
    "        #author = re.findall(r'\\s([a-zA-Z]+)\\s', match[0])[0]\n",
    "        author = re.findall(r'\\s([a-zA-Z-]+)\\s', match[0])[0]\n",
    "\n",
    "        for_ref_list = f\"{author} et al. {year}. {doi}\"\n",
    "        #make sure it isn't already added before adding\n",
    "        if for_ref_list not in ref_list:\n",
    "            ref_list.append(for_ref_list)\n",
    "    \n",
    "    \n",
    "    ref_num = 1\n",
    "    #now iterate through the ref_list and add to the return content\n",
    "    for ref in ref_list:\n",
    "        return_content.append(f\"{ref_num}. {ref} \")\n",
    "        ref_num+=1\n",
    "    return \"\\n\".join(return_content)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c052e-fee8-4bbb-bd3f-6dcec2d8008a",
   "metadata": {},
   "source": [
    "# Do the conversion to markdown\n",
    "This reads through the example_manuscript.ipynb and the miniprotocol and module notebooks to create a markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "edfd870c-3b73-483e-aecd-909f5019421a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reference Data\n",
      "\n",
      " RNA-seq expression\n",
      "\n",
      " Alternative splicing from RNA-seq data\n",
      "\n",
      " Genotype data preprocessing\n",
      "\n",
      " Phenotype data preprocessing\n",
      "\n",
      " Covariate Data Preprocessing\n",
      "\n",
      " QTL Association Analysis\n",
      "\n",
      "  Mixture Multivariate Distribution Estimate\n",
      "\n",
      " Integrative Analysis with High-Dimensional Regression\n",
      "\n",
      " xQTL-GWAS pairwise enrichment and colocalization\n",
      " TWAS, cTWAS and MR\n",
      "\n",
      " Multi-trait colocalization using ColocBoost\n",
      " Chromosome-Specific Enrichment Analysis of Annotations Using Block Jackknife\n",
      "### 1. Reference data\n",
      "\n",
      "#### Reference Data\n",
      "Timing ~4 hours\n",
      "##### i. Download Reference Data\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/reference_data_preparation.ipynb download_hg_reference --cwd reference_data\n",
      "sos run pipeline/reference_data_preparation.ipynb download_gene_annotation --cwd reference_data\n",
      "sos run pipeline/reference_data_preparation.ipynb download_ercc_reference --cwd reference_data\n",
      "sos run pipeline/reference_data_preparation.ipynb download_dbsnp --cwd reference_data\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. Format Reference Data\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/reference_data_preparation.ipynb hg_reference \\\n",
      "    --cwd reference_data \\\n",
      "    --ercc-reference reference_data/ERCC92.fa \\\n",
      "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.fa\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iii. Format Gene Feature Data\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/reference_data_preparation.ipynb gene_annotation \\\n",
      "    --cwd reference_data \\\n",
      "    --ercc-gtf reference_data/ERCC92.gtf \\\n",
      "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.gtf \\\n",
      "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy.fasta \\\n",
      "    --stranded\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iv. Generate STAR Index\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/reference_data_preparation.ipynb STAR_index \\\n",
      "    --cwd reference_data \\\n",
      "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### v. Generate RSEM Index\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/reference_data_preparation.ipynb RSEM_index \\\n",
      "    --cwd reference_data \\\n",
      "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
      "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### vi. Generate RefFlat Annotation for Picard\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/reference_data_preparation.ipynb RefFlat_generation \\\n",
      "    --cwd reference_data \\\n",
      "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### vii. Generate SUPPA Annotation for Psichomics\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/reference_data_preparation.ipynb SUPPA_annotation \\\n",
      "    --cwd reference_data \\\n",
      "    --hg_gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### viii. Extract rsIDs for known variants\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/VCF_QC.ipynb dbsnp_annotate \\\n",
      "    --genoFile reference_data/00-All.vcf.gz\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 2. Molecular Phenotypes\n",
      "\n",
      "#### RNA-seq expression\n",
      "Timing <3.5 hours\n",
      "##### i. Perform data quality summary via `fastqc`\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/RNA_calling.ipynb fastqc \\\n",
      "    --cwd output/rnaseq/fastqc \\\n",
      "    --sample-list data/fastq.list.txt \\\n",
      "    --data-dir data/fastq\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. Cut adaptor (Optional)\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/RNA_calling.ipynb fastp_trim_adaptor \\\n",
      "    --cwd output/rnaseq --sample-list data/fastq.list.txt \\\n",
      "    --data-dir data/fastq --STAR-index reference_data/STAR_Index/ \\\n",
      "    --gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
      "    --reference-fasta reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
      "    --ref-flat reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iii. Read alignment via STAR and QC via Picard\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/RNA_calling.ipynb STAR_align \\\n",
      "    --cwd output/rnaseq/bam --sample-list data/fastq.list.txt \\\n",
      "    --data-dir data/fastq --STAR-index reference_data/STAR_Index/ \\\n",
      "    --gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
      "    --reference-fasta reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
      "    --ref-flat reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
      "    --chimSegmentMin 0 \\\n",
      "    -J 50 --mem 200G --numThreads 8\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iv. Call gene-level RNA expression via rnaseqc\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/RNA_calling.ipynb rnaseqc_call \\\n",
      "    --cwd data/bam \\\n",
      "    --sample-list data/fastq.list.txt \\\n",
      "    --data-dir data/fastq \\\n",
      "    --gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.gtf \\\n",
      "    --reference-fasta reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
      "    --varVCFfile reference_data/ZOD14598_AD_GRM_WGS_2021-04-29_all.recalibrated_variants.leftnorm.filtered.AF.WASP.vcf \\\n",
      "    --bam_list data/bam/sample_bam_list.txt\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### v. Call transcript level RNA expression via RSEM\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/RNA_calling.ipynb rsem_call \\\n",
      "    --cwd data/bam \\\n",
      "    --sample-list data/fastq.list.txt \\\n",
      "    --data-dir data/fastq \\\n",
      "    --STAR-index reference_data/STAR_Index/ \\\n",
      "    --gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
      "    --reference-fasta reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
      "    --ref-flat reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
      "    --varVCFfile reference_data/ZOD14598_AD_GRM_WGS_2021-04-29_all.recalibrated_variants.leftnorm.filtered.AF.WASP.vcf \\\n",
      "    --bam_list data/bam/sample_bam_list.txt \\\n",
      "    --RSEM-index reference_data/RSEM_Index\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### vi. Multi-sample RNA-seq QC\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/bulk_expression_QC.ipynb qc \\\n",
      "    --cwd output/rnaseq \\\n",
      "    --tpm-gct data/rnaseq/bulk_rnaseq_tmp_matrix.bed \\\n",
      "    --counts-gct data/rnaseq/bulk_rnaseq_count_matrix.bed\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### vii. Multi-sample read count normalization\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/bulk_expression_normalization.ipynb normalize \\\n",
      "    --cwd output/rnaseq \\\n",
      "    --tpm-gct output/rnaseq/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tpm.gct.gz \\\n",
      "    --counts-gct output/rnaseq/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.geneCount.gct.gz \\\n",
      "    --annotation-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf  \\\n",
      "    --count-threshold 1 --sample_participant_lookup data/rnaseq/sample_participant_lookup.txt\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "#### Alternative splicing from RNA-seq data\n",
      "Timing <2 hours\n",
      "##### i. Splicing Quantification with Leafcutter (intron usage ratio) or Psichomics (percent spliced in events)\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/splicing_calling.ipynb leafcutter \\\n",
      "    --cwd output/leaf_cutter/ \\\n",
      "    --samples output/rnaseq/xqtl_protocol_data_bam_list \n",
      "\n",
      "sos run pipeline/splicing_calling.ipynb psichomics \\\n",
      "    --cwd output/psichomics/ \\\n",
      "    --samples output/rnaseq/xqtl_protocol_data_bam_list \\\n",
      "    --splicing_annotation hg38_suppa.rds\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. Splicing QC and Normalization\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/splicing_normalization.ipynb leafcutter_norm \\\n",
      "    --cwd output/leaf_cutter/ \\\n",
      "    --ratios output/leaf_cutter/xqtl_protocol_data_bam_list_intron_usage_perind.counts.gz\n",
      "\n",
      "sos run pipeline/splicing_normalization.ipynb psichomics_norm \\\n",
      "    --cwd psichomics_output \\\n",
      "    --ratios psichomics_output/psi_raw_data.tsv\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iii. Post Processing for TensorQTL\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/gene_annotation.ipynb annotate_leafcutter_isoforms \\\n",
      "    --cwd output/leaf_cutter/ \\\n",
      "    --intron_count output/leaf_cutter/xqtl_protocol_data_bam_list_intron_usage_perind_numers.counts.gz \\\n",
      "    --phenoFile output/leaf_cutter/xqtl_protocol_data_bam_list_intron_usage_perind.counts.gz_raw_data.qqnorm.txt \\\n",
      "    --annotation-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.gtf \\\n",
      "    --sample_participant_lookup reference_data/sample_participant_lookup.rnaseq\n",
      "\n",
      "sos run pipeline/code/data_preprocessing/phenotype/gene_annotation.ipynb annotate_psichomics_isoforms \\\n",
      "    --cwd psichomics_output \\\n",
      "    --phenoFile psichomics_output/psichomics_raw_data_bedded.qqnorm.txt \\\n",
      "    --annotation-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformated.ERCC.gene.gtf\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 3. Data Pre-processing\n",
      "\n",
      "#### Genotype data preprocessing\n",
      "Timing < X minutes\n",
      "##### i. QC for VCF files\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "echo ./ZOD14598_AD_GRM_WGS_2021-04-29_chr1.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr2.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr3.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr4.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr5.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr6.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr7.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr8.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr9.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr10.recalibrated_variants.vcf.gz  ./ZOD14598_AD_GRM_WGS_2021-04-29_chr11.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr12.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr13.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr14.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr15.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr16.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr17.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr18.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr19.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr20.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr21.recalibrated_variants.vcf.gz ./ZOD14598_AD_GRM_WGS_2021-04-29_chr22.recalibrated_variants.vcf.gz \\\n",
      "    | tr ' ' '\\n' > /path/to/ZOD14598_AD_GRM_WGS_2021-04-29_vcf_files.txt\n",
      "# We only do this for autosomal variants\n",
      "sos run pipeline/VCF_QC.ipynb qc \\\n",
      "    --genoFile /path/to/ZOD14598_AD_GRM_WGS_2021-04-29_vcf_files.txt \\\n",
      "    --dbsnp-variants /path/to/reference_data/00-All.add_chr.variants.gz \\\n",
      "    --reference-genome /path/to/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
      "    --cwd vcf_qc/ \\\n",
      "    -J 22 --mem 120G\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. Merge separated bed files into one\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/genotype_formatting.ipynb vcf_to_plink \\\n",
      "    --genoFile `ls data/WGS/vcf/wgs.chr*.random.vcf.gz` \\\n",
      "    --cwd output/plink/ \n",
      "\n",
      "sos run pipeline/genotype_formatting.ipynb merge_plink \\\n",
      "    --genoFile `ls output/plink/wgs.chr*.random.bed` \\\n",
      "    --name wgs.merged \\\n",
      "    --cwd output/plink/\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iii. QC for PLINK files\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/GWAS_QC.ipynb qc_no_prune \\\n",
      "   --cwd output/plink \\\n",
      "   --genoFile output/plink/wgs.merged.bed \\\n",
      "   --geno-filter 0.1 \\\n",
      "   --mind-filter 0.1 \\\n",
      "   --hwe-filter 1e-08 \\\n",
      "   --mac-filter 0\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iv. Genotype data partition by chromosome\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/genotype_formatting.ipynb genotype_by_chrom \\\n",
      "    --genoFile output/plink/wgs.merged.plink_qc.bed \\\n",
      "    --cwd output/genotype_by_chrom \\\n",
      "    --chrom `cut -f 1 output/plink/wgs.merged.plink_qc.bim | uniq | sed \"s/chr//g\"`\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### v. Sample match with genotype\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/GWAS_QC.ipynb genotype_phenotype_sample_overlap \\\n",
      "        --cwd output/genotype/ \\\n",
      "        --genoFile output/plink/wgs.merged.plink_qc.fam  \\\n",
      "        --phenoFile output/rnaseq/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.bed.bed.gz\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### vi. Kinship\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/GWAS_QC.ipynb king \\\n",
      "    --cwd output/genotype/kinship \\\n",
      "    --genoFile output/plink/wgs.merged.plink_qc.bed \\\n",
      "    --name wgs.merged.king \\\n",
      "    --keep-samples output/genotype/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.bed.bed.sample_genotypes.txt\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### vii. Prepare unrelated individuals data for PCA\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/GWAS_QC.ipynb qc \\\n",
      "   --cwd output/genotype/ \\\n",
      "   --genoFile output/genotype/kinship/wgs.merged.plink_qc.wgs.merged.king.unrelated.bed \\\n",
      "   --mac-filter 5 \n",
      "\n",
      "#If `No related individuals detected from *.kin0` occurs, there is no separate genotype data generated for unrelated individuals. In this case, we need to work from the original genotype data and must use `--keep-samples` to run `qc` to extract samples for PCA.** For example:\n",
      "\n",
      "sos run pipeline/GWAS_QC.ipynb qc \\\n",
      "   --cwd output/genotype/ \\\n",
      "   --genoFile output/plink/wgs.merged.plink_qc.bed \\\n",
      "   --mac-filter 5\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### viii. PCA on genotype\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/PCA.ipynb flashpca \\\n",
      "   --cwd output/genotype/genotype_pca \\\n",
      "   --genoFile output/genotype/wgs.merged.plink_qc.plink_qc.prune.bed\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "#### Phenotype data preprocessing\n",
      "Timing < 12 minutes\n",
      "##### i. Phenotype Annotation\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/gene_annotation.ipynb annotate_coord \\\n",
      "    --cwd output/rnaseq \\\n",
      "    --phenoFile output/rnaseq/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.bed.gz \\\n",
      "    --coordinate-annotation reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf \\\n",
      "    --phenotype-id-column gene_id\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. Missing Value Imputation\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/phenotype_imputation.ipynb gEBMF \\\n",
      "    --phenoFile data/protocol_example.protein.bed.gz \\\n",
      "    --cwd output/phenotype/impute_gebmf \\\n",
      "    --no-qc-prior-to-impute\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iii. Partition by Chromosome\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/phenotype_formatting.ipynb phenotype_by_chrom \\\n",
      "    --cwd output/phenotype/phenotype_by_chrom \\\n",
      "    --phenoFile output/rnaseq/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.bed.bed.gz \\\n",
      "    --name bulk_rnaseq \\\n",
      "    --chrom `for i in {1..22}; do echo chr$i; done`\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "#### Covariate Data Preprocessing\n",
      "Timing < 3 minutes\n",
      "##### i. Merge Covariates and Genotype PCs\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/covariate_formatting.ipynb merge_genotype_pc \\\n",
      "    --cwd output/covariate/ \\\n",
      "    --pcaFile output/genotype/genotype_pca/wgs.merged.plink_qc.plink_qc.prune.pca.rds \\\n",
      "    --covFile data/covariate/covariates.tsv \\\n",
      "    --tol-cov 0.4 \\\n",
      "    --k `awk '$3 < 0.8' output/genotype/genotype_pca/wgs.merged.plink_qc.plink_qc.prune.pca.scree.txt | tail -1 | cut -f 1 `\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. Compute Residual on Merged Covariates and Perform Hidden Factor Analysis\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/covariate_hidden_factor.ipynb Marchenko_PC \\\n",
      "   --cwd output/covariate \\\n",
      "   --phenoFile output/rnaseq/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.bed.bed.gz  \\\n",
      "   --covFile output/covariate/covariates.wgs.merged.plink_qc.plink_qc.prune.pca.gz \\\n",
      "   --mean-impute-missing\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 4. QTL Association Testing\n",
      "\n",
      "#### QTL Association Analysis\n",
      "Timing < X minutes\n",
      "##### i. Cis TensorQTL Command\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/TensorQTL.ipynb cis \\\n",
      "    --genotype-file output/genotype_by_chrom/wgs.merged.plink_qc.genotype_by_chrom_files.txt \\\n",
      "    --phenotype-file output/phenotype/phenotype_by_chrom/bulk_rnaseq.phenotype_by_chrom_files.txt \\\n",
      "    --covariate-file output/covariate/covariates.wgs.merged.plink_qc.plink_qc.prune.pca.gz \\\n",
      "    --customized-cis-windows reference_data/TAD/TADB_enhanced_cis.bed \\\n",
      "    --cwd output/tensorqtl_cis/ \\\n",
      "    --MAC 5\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. Trans TensorQTL Command\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/TensorQTL.ipynb trans \\\n",
      "    --genotype-file data/wgs.merged.plink_qc.genotype_trans_files.txt \\\n",
      "    --phenotype-file output/phenotype/phenotype_by_chrom_for_trans/bulk_rnaseq.phenotype_by_chrom_files.txt \\\n",
      "    --region-list data/combined_AD_genes.csv \\\n",
      "    --region-list-phenotype-column 4 \\\n",
      "    --covariate-file output/covariate/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.covariates.wgs.merged.plink_qc.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
      "    --cwd output/tensorqtl_trans/ \\\n",
      "    --MAC 5\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iii. Interaction TensorQTL Command\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/TensorQTL.ipynb cis \\\n",
      "    --genotype-file output/genotype_by_chrom/wgs.merged.plink_qc.genotype_by_chrom_files.txt \\\n",
      "    --phenotype-file output/phenotype/phenotype_by_chrom/bulk_rnaseq.phenotype_by_chrom_files.txt \\\n",
      "    --covariate-file output/covariate/covariates.wgs.merged.plink_qc.plink_qc.prune.pca.gz \\\n",
      "    --customized-cis-windows reference_data/TAD/TADB_enhanced_cis.bed \\\n",
      "    --cwd output/tensorqtl_int/ \\\n",
      "    --no-permutation \\\n",
      "    --maf-threshold 0.05 \\\n",
      "    --interaction sex \\\n",
      "    -j 22\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 5. Multivariate Mixture Model\n",
      "\n",
      "####  Mixture Multivariate Distribution Estimate\n",
      "Timing ~X minutes\n",
      "##### i. Compute MASH prior\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/mixture_prior.ipynb ed_bovy \\\n",
      "    --output_prefix MWE_ed_bovy \\\n",
      "    --data data/multivariate_mixture/MWE.rds \\\n",
      "    --cwd output/multivariate_mixture --vhat mle\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. MASH fit\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/mash_fit.ipynb mash \\\n",
      "    --output-prefix MWE_ed_bovy_posterior \\\n",
      "    --data data/multivariate_mixture/MWE.rds \\\n",
      "    --vhat-data output/multivariate_mixture/MWE_ed_bovy.EE.V_simple.rds \\\n",
      "    --prior-data output/multivariate_mixture/MWE_ed_bovy.EE.prior.rds \\\n",
      "    --compute-posterior \\\n",
      "    --cwd output/multivariate_mixture\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iii. Generate Plots\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/mixture_prior.ipynb plot_U \\\n",
      "    --output-prefix protocol_example.mixture_plots  \\\n",
      "    --data output/multivariate_mixture/MWE_ed_bovy.EE.prior.rds \\\n",
      "    --cwd output/multivariate_mixture\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 6. Multiomics Regression Models\n",
      "\n",
      "#### Integrative Analysis with High-Dimensional Regression\n",
      "Timing < X minutes\n",
      "##### i. Univariate Fine-Mapping and TWAS with SuSiE\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/mnm_regression.ipynb susie_twas \\\n",
      "    --name test_susie_twas \\\n",
      "    --genoFile output/genotype_by_chrom/wgs.merged.plink_qc.1.bed \\\n",
      "    --phenoFile output/phenotype/phenotype_by_chrom_for_cis/bulk_rnaseq.phenotype_by_chrom_files.region_list.txt \\\n",
      "    --covFile output/covariate/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.covariates.wgs.merged.plink_qc.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
      "    --customized-association-windows reference_data/TAD/TADB_enhanced_cis.bed \\\n",
      "    --phenotype-names test_pheno \\\n",
      "    --max-cv-variants 5000 --ld_reference_meta_file data/ld_meta_file_with_bim.tsv \\\n",
      "    --region-name ENSG00000049246 ENSG00000054116 ENSG00000116678 \\\n",
      "    --save-data \\\n",
      "    --cwd output/mnm_regression/susie_twas\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### ii. Multivariate Fine-Mapping for multiple genes\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/mnm_regression.ipynb mnm_genes \\\n",
      "    --name ROSMAP_Ast_mega_eQTL \\\n",
      "    --genoFile data/mnm_genes/ROSMAP_NIA_WGS.leftnorm.bcftools_qc.plink_qc.11.bed \\\n",
      "    --phenoFile data/mnm_genes/snuc_pseudo_bulk.Ast.mega.normalized.log2cpm.region_list.txt \\\n",
      "    --covFile data/mnm_genes/snuc_pseudo_bulk.Ast.mega.normalized.log2cpm.rosmap_cov.ROSMAP_NIA_WGS.leftnorm.bcftools_qc.plink_qc.snuc_pseudo_bulk_mega.related.plink_qc.extracted.pca.projected.Marchenko_PC.gz \\\n",
      "    --customized-association-windows data/mnm_genes/extended_TADB.bed \\\n",
      "    --phenotype-names Ast_mega_eQTL \\\n",
      "    --max-cv-variants 5000 \\\n",
      "    --ld_reference_meta_file data/ld_meta_file_with_bim.tsv \\\n",
      "    --independent_variant_list data/mnm_genes/ld_pruned_variants.txt.gz \\\n",
      "    --fine_mapping_meta data/mnm_genes/combined_data_updated.tsv \\\n",
      "    --phenoIDFile data/mnm_genes/phenoIDFile_extended_TADB.bed \\\n",
      "    --region-name chr11_77324757_82556425 \\\n",
      "    --skip-analysis-pip-cutoff 0 \\\n",
      "    --maf 0.01 \\\n",
      "    --coverage 0.95 \\\n",
      "    --pheno_id_map_file data/mnm_genes/pheno_id_map_file.txt \\\n",
      "    --prior-canonical-matrices \\\n",
      "    --twas-cv-folds 0 \\\n",
      "    --trans-analysis \\\n",
      "    --cwd output/mnm_regression/mnm_genes -s build\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iii. Univariate Fine-Mapping of Functional (Epigenomic) Data with fSuSiE\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/mnm_regression.ipynb fsusie \\\n",
      "    --cwd output/fsusie/ \\\n",
      "    --name test_fsusie \\\n",
      "    --genoFile output/genotype_by_chrom/wgs.merged.plink_qc.genotype_by_chrom_files.txt \\\n",
      "    --phenoFile output/phenotype/phenotype_by_chrom_for_cis/bulk_rnaseq.phenotype_by_chrom_files.region_list.txt \\\n",
      "    --covFile output/covariate/bulk_rnaseq_tpm_matrix.low_expression_filtered.outlier_removed.tmm.expression.covariates.wgs.merged.plink_qc.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
      "    --numThreads 8 \\\n",
      "    --customized-association-windows reference_data/TAD/TADB_enhanced_cis.bed \\\n",
      "    --save-data \\\n",
      "    --region-name ENSG00000049246 ENSG00000054116 ENSG00000116678 ENSG00000073921 ENSG00000186891\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iv. Multivariate Fine-Mapping with mvSuSiE and mr.mash\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/mnm_regression.ipynb mnm \\\n",
      "    --name test_mnm --cwd output/mnm \\\n",
      "    --genoFile output/genotype_by_chrom/wgs.merged.plink_qc.genotype_by_chrom_files.txt \\\n",
      "    --phenoFile output/phenotype/phenotype_by_chrom_for_cis/bulk_rnaseq.phenotype_by_chrom_files.region_list.txt \\\n",
      "    --covFile output/covariate/bulk_rnaseq_tpm_matrix.low_expression_filtered.outlier_removed.tmm.expression.covariates.wgs.merged.plink_qc.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
      "    --customized-association-windows reference_data/TAD/TADB_enhanced_cis.bed \\\n",
      "    --region-name ENSG00000073921 --save-data --no-skip-twas-weights \\\n",
      "    --phenotype-names test_pheno \\\n",
      "    --mixture_prior output/multivariate_mixture/MWE_ed_bovy.EE.prior.rds \\\n",
      "    --max_cv_variants 5000 \\\n",
      "\t--ld_reference_meta_file data/ld_meta_file.tsv\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### v. Regression with Summary Statistics (RSS) Fine-Mapping and TWAS with SuSiE\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/rss_analysis.ipynb univariate_rss \\\n",
      "    --ld-meta-data data/ld_meta_file_with_bim.tsv \\\n",
      "    --gwas-meta-data data/mnm_regression/gwas_meta_data.txt \\\n",
      "    --qc_method \"rss_qc\" --impute \\\n",
      "    --finemapping_method \"susie_rss\" \\\n",
      "    --cwd output/rss_analysis \\\n",
      "    --skip_analysis_pip_cutoff 0 \\\n",
      "    --skip_regions 6:25000000-35000000 \\\n",
      "    --region_name 22:49355984-50799822\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 7. GWAS Integration\n",
      "\n",
      "#### xQTL-GWAS pairwise enrichment and colocalization\n",
      "Timing: ~X min\n",
      "#### i. Enrichment\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/SuSiE_enloc.ipynb xqtl_gwas_enrichment    \\\n",
      "    --gwas-meta-data data/susie_enloc_data/demo_gwas.block_results_db.tsv \\\n",
      "    --xqtl-meta-data data/susie_enloc_data/demo_overlap.overlapped.gwas.tsv \\\n",
      "    --xqtl_finemapping_obj preset_variants_result susie_result_trimmed  \\\n",
      "    --xqtl_varname_obj preset_variants_result variant_names  \\\n",
      "    --gwas_finemapping_obj AD_Bellenguez_2022 RSS_QC_RAISS_imputed susie_result_trimmed \\\n",
      "    --gwas_varname_obj  AD_Bellenguez_2022 RSS_QC_RAISS_imputed variant_names \\\n",
      "    --xqtl_region_obj  region_info   grange \\\n",
      "    --qtl-path data/susie_enloc_data \\\n",
      "    --gwas_path data/susie_enloc_data \\\n",
      "    --context_meta data/susie_enloc_data/context_meta.tsv \\\n",
      "    --cwd output/xqtl_gwas_enrichment\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "#### ii. Coloc \n",
      "This would trigger enrichment automatically. Include the `--skip-enrich` to skip the enrichment part. Otherwise, the `*enrichment.rds` file from the previous step must be in the location of the `--cwd` directory.\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/SuSiE_enloc.ipynb susie_coloc \\\n",
      "    --gwas-meta-data data/susie_enloc_data/demo_gwas.block_results_db.tsv \\\n",
      "    --xqtl-meta-data data/susie_enloc_data/demo_overlap.overlapped.gwas.tsv \\\n",
      "    --xqtl_finemapping_obj preset_variants_result susie_result_trimmed \\\n",
      "    --xqtl_varname_obj preset_variants_result variant_names \\\n",
      "    --gwas_finemapping_obj AD_Bellenguez_2022 RSS_QC_RAISS_imputed susie_result_trimmed \\\n",
      "    --gwas_varname_obj  AD_Bellenguez_2022 RSS_QC_RAISS_imputed variant_names \\\n",
      "    --xqtl_region_obj  region_info grange \\\n",
      "    --qtl-path data/susie_enloc_data \\\n",
      "    --gwas_path data/susie_enloc_data \\\n",
      "    --context_meta data/susie_enloc_data/context_meta.tsv \\\n",
      "    --ld_meta_file_path /restricted/projectnb/xqtl/xqtl_protocol/scripts/pixi_scripts/ld_meta_file.tsv \\\n",
      "    --cwd output/susie_coloc\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "#### TWAS, cTWAS and MR\n",
      "Timing: ~X min\n",
      "##### iii. Run TWAS\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/twas_ctwas.ipynb twas \\\n",
      "   --cwd output/twas --name test \\\n",
      "   --gwas_meta_data data/twas/gwas_meta_test.tsv \\\n",
      "   --ld_meta_data reference_data/ADSP_R4_EUR/ld_meta_file.tsv \\\n",
      "   --regions data/twas/EUR_LD_blocks.bed \\\n",
      "   --xqtl_meta_data data/twas/mwe_twas_pipeline_test_small.tsv \\\n",
      "   --xqtl_type_table data/twas/data_type_table.txt \\\n",
      "   --rsq_pval_cutoff 0.05 --rsq_cutoff 0.01\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### iv. Run cTWAS\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/twas_ctwas.ipynb ctwas \\\n",
      "   --cwd output/twas --name test \\\n",
      "   --gwas_meta_data data/twas/gwas_meta_test.tsv \\\n",
      "   --ld_meta_data data/ld_meta_file_with_bim.tsv \\\n",
      "   --xqtl_meta_data data/twas/mwe_twas_pipeline_test_small.tsv \\\n",
      "   --twas_weight_cutoff 0 \\\n",
      "   --chrom 11 \\\n",
      "   --regions data/twas/EUR_LD_blocks.bed \\\n",
      "   --region-name chr10_80126158_82231647 chr11_84267999_86714492\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "#### Multi-trait colocalization using ColocBoost\n",
      "Timing: ~X min\n",
      "##### v. Colocboost\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/colocboost.ipynb colocboost \\\n",
      "    --name test_coloc_boost_xqtl_only  \\\n",
      "    --cwd output/colocboost \\\n",
      "    --genoFile output/plink/wgs.merged.bed \\\n",
      "    --phenoFile output/phenotype/phenotype_by_chrom_for_cis/bulk_rnaseq.phenotype_by_chrom_files.region_list.txt \\\n",
      "    --covFile output/covariate/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.covariates.wgs.merged.plink_qc.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
      "    --customized-association-windows reference_data/TAD/TADB_enhanced_cis.bed \\\n",
      "    --no-separate-gwas --xqtl-coloc \\\n",
      "    --region-name ENSG00000239945 \\\n",
      "    --phenotype-names trait_A\n",
      "\n",
      "#It is also possible to analyze a selected list of regions using option `--region-list`. The last column of this file will be used for the list to analyze. Here for example use the same list of regions as we used for customized association-window:\n",
      "sos run pipeline/colocboost.ipynb colcoboost  \\\n",
      "    --name protocol_example_protein  \\\n",
      "    --genoFile input/xqtl_association/protocol_example.genotype.chr21_22.bed   \\\n",
      "    --phenoFile output/phenotype/protocol_example.protein.region_list.txt \\\n",
      "                output/phenotype/protocol_example.protein.region_list.txt \\\n",
      "    --covFile output/covariate/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
      "              output/covariate/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz  \\\n",
      "    --customized-association-windows input/xqtl_association/protocol_example.protein.enhanced_cis_chr21_chr22.bed \\\n",
      "    --no-separate-gwas --xqtl-coloc \\\n",
      "    --region-list xqtl_association/protocol_example.protein.enhanced_cis_chr21_chr22.bed \\\n",
      "    --phenotype-names trait_A trait_B\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### vi. Colocboost with GWAS\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/colocboost.ipynb colocboost \\\n",
      "    --name colocboost_gwas  \\\n",
      "    --cwd output/colocboost_gwas \\\n",
      "    --genoFile output/plink/wgs.merged.bed \\\n",
      "    --phenoFile output/phenotype/phenotype_by_chrom_for_cis/bulk_rnaseq.phenotype_by_chrom_files.region_list.txt \\\n",
      "    --covFile output/covariate/bulk_rnaseq_tmp_matrix.low_expression_filtered.outlier_removed.tmm.expression.covariates.wgs.merged.plink_qc.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
      "    --customized-association-windows reference_data/TAD/TADB_enhanced_cis.bed \\\n",
      "    --ld-meta-data data/ld_meta_file.tsv \\\n",
      "    --gwas-meta-data data/colocboost/gwas_meta.txt \\\n",
      "    --separate-gwas --xqtl-coloc \\\n",
      "    --region-name ENSG00000239945 \\\n",
      "    --phenotype-names trait_A\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 8. Enrichment and Validation\n",
      "\n",
      "#### Chromosome-Specific Enrichment Analysis of Annotations Using Block Jackknife\n",
      "Timing: ~X min\n",
      "##### i. Enrichment\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "sos run pipeline/eoo_enrichment.ipynb enrichment \\\n",
      "    --significant_variants_path data/eoo_enrichment/colocboost_binary_vcp0.1_hg38_annotation.tsv.gz \\\n",
      "    --baseline_anno_path data/eoo_enrichment/colocboost_binary_vcp0.1_hg38_annotation_data.tsv \\\n",
      "    --name enrichment_results \\\n",
      "    --cwd output/eoo_enrichment\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      " Reference Data\n",
      "\n",
      " RNA-seq expression\n",
      "\n",
      " Alternative splicing from RNA-seq data\n",
      "\n",
      " Genotype data preprocessing\n",
      "\n",
      " Phenotype data preprocessing\n",
      "\n",
      " Covariate Data Preprocessing\n",
      "\n",
      " QTL Association Analysis\n",
      "\n",
      "  Mixture Multivariate Distribution Estimate\n",
      "\n",
      " Integrative Analysis with High-Dimensional Regression\n",
      "\n",
      " xQTL-GWAS pairwise enrichment and colocalization\n",
      " TWAS, cTWAS and MR\n",
      "\n",
      " Multi-trait colocalization using ColocBoost\n",
      " Chromosome-Specific Enrichment Analysis of Annotations Using Block Jackknife\n",
      " Reference Data\n",
      "\n",
      " RNA-seq expression\n",
      "\n",
      " Alternative splicing from RNA-seq data\n",
      "\n",
      " Genotype data preprocessing\n",
      "\n",
      " Phenotype data preprocessing\n",
      "\n",
      " Covariate Data Preprocessing\n",
      "\n",
      " QTL Association Analysis\n",
      "\n",
      "  Mixture Multivariate Distribution Estimate\n",
      "\n",
      " Integrative Analysis with High-Dimensional Regression\n",
      "\n",
      " xQTL-GWAS pairwise enrichment and colocalization\n",
      " TWAS, cTWAS and MR\n",
      "\n",
      " Multi-trait colocalization using ColocBoost\n",
      " Chromosome-Specific Enrichment Analysis of Annotations Using Block Jackknife\n"
     ]
    }
   ],
   "source": [
    "with open(manuscript_format_notebook, 'r') as manuscript_format:\n",
    "    notebook = json.load(manuscript_format)\n",
    "# Clearing the content of the markdown file before appending new content\n",
    "open(output_markdown_file, 'w').close()\n",
    "#flag to tell us if we are in the procedure part. Used to skip some of the content in \n",
    "#the example_manuscript.ipynb that will be added programmatically here\n",
    "in_procedure = False\n",
    "\n",
    "for i, cell in enumerate(notebook[\"cells\"]):\n",
    "    if cell[\"cell_type\"] == \"markdown\":\n",
    "\n",
    "        if len(cell[\"source\"]) >0:\n",
    "\n",
    "            content = cell[\"source\"][0]\n",
    "            # one of the main sections (Title, Abstract, Procedure, etc...)\n",
    "            if content.startswith(\"## \"):\n",
    "                section_title = ''.join(cell[\"source\"]) + '\\n\\n'\n",
    "                append_to_markdown(output_markdown_file, section_title)\n",
    "                #if in procedure and we hit a new section, then we are no longer in the procedure section\n",
    "                if in_procedure:\n",
    "                    in_procedure = False\n",
    "                #get content for procedure section\n",
    "                if content.startswith(\"## Procedure\"):\n",
    "                    in_procedure = True\n",
    "\n",
    "                    \n",
    "                    proc = content_for_procedure()\n",
    "                    print(proc)\n",
    "                    append_to_markdown(output_markdown_file, proc)\n",
    "                #get content for timing section\n",
    "                if content.startswith(\"## Timing\"):\n",
    "                    in_procedure = False\n",
    "                    timing = content_for_timing()\n",
    "                    append_to_markdown(output_markdown_file, timing)\n",
    "                #get content for anticipated results section\n",
    "                if content.startswith(\"## Anticipated Results\"):\n",
    "                    in_procedure = False\n",
    "                    antires = content_for_anticipated_results()\n",
    "                    append_to_markdown(output_markdown_file, antires)\n",
    "                #get content for figures section\n",
    "                if content.startswith(\"## Figures\"):\n",
    "                    in_procedure = False\n",
    "                    figures = content_for_figures()\n",
    "                    #append_to_markdown(output_markdown_file, figures)\n",
    "                #get content for references section\n",
    "                if content.startswith(\"## References\"):\n",
    "                    in_procedure = False\n",
    "                    ref = content_for_references()\n",
    "                    append_to_markdown(output_markdown_file, ref)\n",
    "            #other sub sections\n",
    "            if content.startswith(\"### \") and not in_procedure:\n",
    "                section_title = ''.join(cell[\"source\"]) + '\\n\\n'\n",
    "\n",
    "                append_to_markdown(output_markdown_file, section_title)\n",
    "                # experimental design subsection of Introduction\n",
    "                if content.startswith(\"### Experimental Design\"):\n",
    "                    exp = content_for_exp_design()\n",
    "\n",
    "                    append_to_markdown(output_markdown_file, exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "30703ea8-bc21-42c4-b958-e41bb6837fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = r\"\"\"\\documentclass[12pt]{article}\n",
    "\\usepackage[utf8]{inputenc}\n",
    "\\usepackage{hyperref}\n",
    "\\usepackage{listings}\n",
    "\\usepackage{color}\n",
    "\n",
    "\\definecolor{codegray}{gray}{0.95}\n",
    "\\lstset{\n",
    "  backgroundcolor=\\color{codegray},\n",
    "  basicstyle=\\ttfamily\\footnotesize,\n",
    "  breaklines=true,\n",
    "  frame=single,\n",
    "  columns=fullflexible\n",
    "}\n",
    "\n",
    "\\begin{document}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fa4f5921-6eba-4371-a072-f47e1017e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def markdown_links_to_latex(text):\n",
    "    \"\"\"\n",
    "    Converts Markdown links [text](url) to LaTeX \\href{url}{text}.\n",
    "    Escapes underscores in URLs.\n",
    "    \"\"\"\n",
    "    def replacer(match):\n",
    "        label = match.group(1)\n",
    "        url = match.group(2).replace('_', r'\\_')  # escape underscores\n",
    "        return f'\\\\href{{{url}}}{{{label}}}'\n",
    "\n",
    "    # Regex to match [label](url)\n",
    "    pattern = re.compile(r'\\[([^\\]]+)\\]\\(([^)]+)\\)')\n",
    "    return pattern.sub(replacer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8db16399-9840-45a2-9719-42d3c1bd9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_table_to_latex(table_lines):\n",
    "    \"\"\"Convert buffered Markdown table lines to LaTeX tabular format.\"\"\"\n",
    "    if len(table_lines) < 2:\n",
    "        return \"\"  # not a table\n",
    "    \n",
    "    # header row\n",
    "    header = [h.strip() for h in table_lines[0].strip().split('|') if h.strip()]\n",
    "    # alignment row ignored, default left\n",
    "    alignment = ['l' for _ in header]\n",
    "    \n",
    "    # data rows\n",
    "    rows = []\n",
    "    for line in table_lines[2:]:  # skip header + separator\n",
    "        row = [cell.strip() for cell in line.strip().split('|') if cell.strip()]\n",
    "        rows.append(row)\n",
    "    \n",
    "    # build LaTeX\n",
    "    latex = \"\\\\begin{tabular}{\" + \"|\".join([\"\"] + alignment + [\"\"]) + \"}\\n\"\n",
    "    latex += \"\\\\hline\\n\"\n",
    "    latex += \" & \".join(header) + \" \\\\\\\\\\n\"\n",
    "    latex += \"\\\\hline\\n\"\n",
    "    for row in rows:\n",
    "        latex += \" & \".join(row) + \" \\\\\\\\\\n\"\n",
    "    latex += \"\\\\hline\\n\\\\end{tabular}\\n\"\n",
    "    return latex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "868f078d-be85-46ba-b080-eca5ae9e17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_latex_file, \"w\", encoding=\"utf-8\") as latex_output:\n",
    "    latex_output.write(header)\n",
    "    with open(output_markdown_file, 'r', encoding=\"utf-8\") as markdown_content:\n",
    "        in_code_block = False\n",
    "        in_table = False\n",
    "        in_references = False\n",
    "        found_reference = False\n",
    "        table_buffer = []\n",
    "\n",
    "        for line in markdown_content:\n",
    "            stripped = line.strip()\n",
    "\n",
    "            # handle code blocks\n",
    "            if stripped.startswith(\"```python\"):\n",
    "                in_code_block = True\n",
    "                latex_output.write(r\"\"\"\\noindent\"\"\")\n",
    "                latex_output.write(\"\\n\\\\begin{lstlisting}[language=Python]\\n\")\n",
    "                continue\n",
    "            elif stripped.startswith(\"```\") and in_code_block:\n",
    "                in_code_block = False\n",
    "                latex_output.write(\"\\\\end{lstlisting}\\n\")\n",
    "                continue\n",
    "            \n",
    "            if in_code_block:\n",
    "                latex_output.write(line)\n",
    "                continue\n",
    "\n",
    "            # detect start of references\n",
    "            if stripped.startswith(\"## References\"):\n",
    "                in_references = True\n",
    "                latex_output.write(\"\\n\\\\section*{References}\\n\\\\begin{enumerate}\\n\")\n",
    "                continue\n",
    "\n",
    "            if in_references:\n",
    "                # convert numbered references\n",
    "                if stripped and stripped[0].isdigit() :#and stripped[1] == \".\":\n",
    "                    \n",
    "                    # split at first period\n",
    "                    num, rest = stripped.split(\".\", 1)\n",
    "                    # find URL\n",
    "                    if \"http\" in rest:\n",
    "                        parts = rest.split(\"http\", 1)\n",
    "                        text = parts[0].strip().replace(\"_\", r'\\_')\n",
    "                        url = \"http\" + parts[1].strip()\n",
    "                        latex_output.write(f\"  \\\\item {text} \\\\url{{{url}}}\\n\")\n",
    "                        found_reference = True\n",
    "                    else:\n",
    "                        rest_edit = rest.strip().replace('_', r'\\\\_')\n",
    "                        latex_output.write(f\"  \\\\item {rest_edit}\\n\")\n",
    "                        found_reference = True\n",
    "                if found_reference and not stripped:\n",
    "                    # end references if a non-numbered line appears\n",
    "                    in_references = False\n",
    "                    found_reference = False\n",
    "                    latex_output.write(\"\\\\end{enumerate}\\n\")\n",
    "                    # fall through to normal processing for this line\n",
    "                continue\n",
    "\n",
    "            # handle tables\n",
    "            if '|' in stripped and not stripped.startswith('#'):\n",
    "                table_buffer.append(stripped)\n",
    "                in_table = True\n",
    "                continue\n",
    "            elif in_table:\n",
    "                latex_output.write(convert_table_to_latex(table_buffer))\n",
    "                table_buffer = []\n",
    "                in_table = False\n",
    "\n",
    "            # handle headers\n",
    "            if stripped.startswith(\"## \"):\n",
    "                latex_output.write(\"\\n\\\\section*{\" + stripped.replace(\"#\", \"\").strip() + \"}\\n\")\n",
    "            elif stripped.startswith(\"### \"):\n",
    "                latex_output.write(\"\\n\\\\subsection*{\" + stripped.replace(\"#\", \"\").strip() + \"}\\n\")\n",
    "            elif stripped.startswith(\"#### \"):\n",
    "                latex_output.write(\"\\n\\\\subsubsection*{\" + stripped.replace(\"#\", \"\").strip() + \"}\\n\")\n",
    "            elif stripped.startswith(\"##### \"):\n",
    "                latex_output.write(\"\\n\\\\paragraph*{\" + stripped.replace(\"#\", \"\").strip() + \"}\\n\")\n",
    "            else:\n",
    "                # normal text\n",
    "                line_text = stripped.replace(\"_\", r'\\_')\n",
    "                line_text = markdown_links_to_latex(line_text)\n",
    "                latex_output.write(line_text + \"\\n\")\n",
    "\n",
    "    latex_output.write(\"\\n\\\\end{document}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89214c2a-fd11-4897-a2df-b2a95f896c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
