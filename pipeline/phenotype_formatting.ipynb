{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129da00d",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Phenotype data formatting\n",
    "\n",
    "\n",
    "This module implements a collection of workflows used to format molecular phenotype data.\n",
    "\n",
    "\n",
    "\n",
    "## Input\n",
    "The input for this workflow is the collection of data for 1 conditions as described in the readme of this git repo\n",
    "1. 1 complete residual molecular_phenotype data\n",
    "2. 1 region_list\n",
    "Both of these input can be generated by the annotation module of this pipeline\n",
    "\n",
    "## Output\n",
    "For each collection, the output is \n",
    "1. 1 lists of phenotype file (bed+index) for each chrom, suitable to be fed into both apex and tensorQTL, annotated with chrom and pos\n",
    "2. 1 lists of phenotype file (bed+index) for each gene, annotated with chrom and tss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d77a3",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "An MWE is uploaded to [google drive](https://drive.google.com/drive/folders/1yjTwoO0DYGi-J9ouMsh9fHKfDmsXJ_4I?usp=sharing).\n",
    "The singularity image (sif) for running this MWE is uploaded to [google drive](https://drive.google.com/drive/folders/1mLOS3AVQM8yTaWtCbO8Q3xla98Nr5bZQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce675e0c",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/phenotype_formatting.ipynb partition_by_chrom \\\n",
    "    --cwd output  \\\n",
    "    --phenoFile MWE.log2cpm.mol_phe.bed.gz \\\n",
    "    --region-list MWE.region_list \\\n",
    "    --container containers/rna_quantification.sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca34822",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import os\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path(\"output\")\n",
    "# The filename namefor output data\n",
    "parameter: container = ''\n",
    "\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "# Path to the input molecular phenotype data.\n",
    "parameter: phenoFile = paths\n",
    "# name for the analysis output\n",
    "parameter: name= f'{phenoFile:bn}'\n",
    "# Whether the input data is named by gene_id or gene_name. By default it is gene_id, if not, please change it to gene_name\n",
    "parameter: phenotype_id_type = 'gene_id'\n",
    "gene_name_as_phenotype_id = \"gene_name\" == phenotype_id_type\n",
    "parameter: region_list = path\n",
    "import pandas as pd\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "# Get the unique chormosome that have regions to be analyzed.\n",
    "def extract(lst):\n",
    "    return [item[0] for item in lst]\n",
    "chrom = list(set(extract(regions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843b302",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Region List generation\n",
    "\n",
    "To partitioning the data by genes require a region list file which:\n",
    "\n",
    "    1. have 5 columns: chr,start,end,gene_id,gene_name\n",
    "    2. have the same gene as or less gene than that of the bed file\n",
    "    \n",
    "Input:\n",
    "\n",
    "    1. A gtf file used to generated the bed\n",
    "    2. A phenotype bed file, must have a gene_id column indicating the name of genes.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348ab46",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[generate_region_list]\n",
    "#  gene gtf annotation table\n",
    "parameter: annotation_gtf = path\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd}/{_input[0]:bnn}.region_list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    # get the five column data\n",
    "    bed_template_df_id = qtl.io.gtf_to_tss_bed(${_input[1]:r}, feature='transcript',phenotype_id = \"gene_id\" )\n",
    "    bed_template_df_name = qtl.io.gtf_to_tss_bed(${_input[1]:r}, feature='transcript',phenotype_id = \"gene_name\" )\n",
    "    bed_template_df = bed_template_df_id.merge(bed_template_df_name, on = [\"chr\",\"start\",\"end\"])\n",
    "    bed_template_df.columns = [\"#chr\",\"start\",\"end\",\"gene_id\",\"gene_name\"]\n",
    "    pheno = pd.read_csv(${_input[0]:r}, sep = \"\\t\")\n",
    "    # Retaining only the genes in the data\n",
    "    region_list = bed_template_df[bed_template_df.${phenotype_id_type}.isin(pheno.gene_id)]\n",
    "    region_list.to_csv(\"${_output}\", sep = \"\\t\",index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57abb9f6",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Process of molecular phenotype file\n",
    "This workflow produce a bed+tabix file for all the molecular pheno data that are included in the region list to feed into downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f1200",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[partition_by_chrom_1]\n",
    "# An index text file with 5 columns specifying the chr, start, end and names of regions to analyze\n",
    "parameter: region_list = path\n",
    "# Path to the input molecular phenotype data.\n",
    "input: phenoFile ,for_each = \"chrom\"\n",
    "output: f'{cwd}/{name}.{_chrom}.mol_phe.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "bash: expand = \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "    zcat $[_input] | head -1 > $[_output:n]\n",
    "    tabix $[_input] $[_chrom] >> $[_output:n] \n",
    "    bgzip -f $[_output:n]\n",
    "    tabix -p bed $[_output] -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325f8f5",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[partition_by_chrom_2]\n",
    "# Path to the input molecular phenotype data.\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{name}.processed_phenotype.per_chrom.recipe'\n",
    "import pandas as pd\n",
    "chrom_df = pd.DataFrame({\"#id\" : chrom ,\"#dir\" : _input})\n",
    "chrom_df.to_csv(_output,index = 0,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b75d0e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[partition_by_gene_1]\n",
    "# An index text file with 5 columns specifying the chr, start, end and names of regions to analyze\n",
    "parameter: region_list = path\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "# Get the unique chormosome that have regions to be analyzed.\n",
    "def extract(lst):\n",
    "    return [item[0] for item in lst]\n",
    "chrom = list(set(extract(regions)))\n",
    "# Path to the input molecular phenotype data.\n",
    "input: phenoFile ,for_each = \"regions\"\n",
    "output: f'{cwd}/{name}.{_regions[3]}.{_regions[4]}.mol_phe.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "bash: expand = \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "    zcat $[_input] | head -1 > $[_output:n]\n",
    "    zcat $[_input] | grep  $[_regions[3] if gene_name_as_phenotype_id else _regions[4]] >> $[_output:n]\n",
    "    bgzip -f $[_output:n]\n",
    "    tabix -p bed $[_output] -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c8b5d",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[partition_by_gene_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{name}.processed_phenotype.per_gene.recipe'\n",
    "import pandas as pd\n",
    "region_df = pd.DataFrame({\"#id\" : [x[3] for x in regions] ,\"dir\" : _input})\n",
    "region_df.to_csv(_output,index = 0,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24273102",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[bed_filter_na_mean_impute]\n",
    "# Tolerance of missingness rows with missing rate larger than tol_missing will be removed,\n",
    "# with missing rate smaller than tol_missing will be mean_imputed. Say if we want to keep rows with less than 5% missing, then we use 0.05 as tol_missing.\n",
    "parameter: tol_missing = 0.05\n",
    "input: phenoFile\n",
    "output: f'{_input:nn}.filter_na.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "   library(\"dplyr\")\n",
    "   library(\"tibble\")\n",
    "   library(\"readr\")\n",
    "   compute_missing <- function(mtx){\n",
    "          miss <- sum(is.na(mtx))/length(mtx)\n",
    "          return(miss)\n",
    "        }\n",
    "\n",
    "        mean_impute <- function(mtx){\n",
    "          f <- apply(mtx, 2, function(x) mean(x,na.rm = TRUE))\n",
    "          for (i in 1:length(f)) mtx[,i][which(is.na(mtx[,i]))] <- f[i]\n",
    "          return(mtx)\n",
    "        }\n",
    "    \n",
    "        filter_mtx <- function(X, missing_rate_thresh) {\n",
    "            rm_col <- which(apply(X, 2, compute_missing) > missing_rate_thresh)\n",
    "            if (length(rm_col)) X <- X[, -rm_col]\n",
    "            return(mean_impute(X))\n",
    "        }  \n",
    "  \n",
    "    bed = read_delim(\"${_input}\")\n",
    "    mtx = bed[,5:ncol(bed)]%>%as.matrix\n",
    "    rownames(mtx) = bed[,4]%>%unlist()\n",
    "    tbl_filtered = filter_mtx(mtx%>%t(),${tol_missing})%>%t()%>%as_tibble(rownames = colnames(bed)[4] )\n",
    "    bed_filtered = inner_join(bed[,1:4],tbl_filtered)\n",
    "    bed_filtered%>%write_delim(${_output}, \"\\t\" )\n",
    "  \n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    bgzip -f ${_output:n}\n",
    "    tabix ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc85aee",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[bed_filter_na]\n",
    "# Tolerance of missingness rows with missing rate larger than tol_missing will be removed,\n",
    "# with missing rate smaller than tol_missing will be mean_imputed. Say if we want to keep rows with less than 5% missing, then we use 0.05 as tol_missing.\n",
    "parameter: tol_missing = 0.05\n",
    "input: phenoFile\n",
    "output: f'{_input:nn}.filter_na.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "   library(\"dplyr\")\n",
    "   library(\"tibble\")\n",
    "   library(\"readr\")\n",
    "   compute_missing <- function(mtx){\n",
    "          miss <- sum(is.na(mtx))/length(mtx)\n",
    "          return(miss)\n",
    "        }\n",
    "\n",
    "    \n",
    "        filter_mtx <- function(X, missing_rate_thresh) {\n",
    "            rm_col <- which(apply(X, 2, compute_missing) > missing_rate_thresh)\n",
    "            if (length(rm_col)) X <- X[, -rm_col]\n",
    "            return(X)\n",
    "        }  \n",
    "  \n",
    "    bed = read_delim(\"${_input}\", \"\\t\")\n",
    "    mtx = bed[,5:ncol(bed)]%>%as.matrix\n",
    "    rownames(mtx) = bed[,4]%>%unlist()\n",
    "    tbl_filtered = filter_mtx(mtx%>%t(),${tol_missing})%>%t()%>%as_tibble(rownames = colnames(bed)[4] )\n",
    "    bed_filtered = inner_join(bed[,1:4],tbl_filtered)\n",
    "    bed_filtered%>%write_delim(\"${_output:n}\", \"\\t\" )\n",
    "  \n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    bgzip -f ${_output:n}\n",
    "    tabix ${_output} -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18985052",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[per_chr_softImpute_1]\n",
    "parameter: rank_max = 50 # max rank estimated in the per-chr methyl matrix\n",
    "parameter: lambda_hyp = 30 # hyper par, indicating the importance of the nuclear norm\n",
    "parameter: chr_recipe = path\n",
    "chrom_path_ls = pd.read_csv(chr_recipe, sep = \"\\t\")\n",
    "per_chr_file = chrom_path_ls[\"#dir\"].tolist()\n",
    "input: for_each = \"per_chr_file\"\n",
    "output: f'{_per_chr_file}_chr.softImputed.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library(softImpute)\n",
    "    library(dplyr)\n",
    "    library(readr)\n",
    "    library(tibble)\n",
    "    pheno = read_delim(\"${_per_chr_file}\", delim = \"\\t\")\n",
    "    pheno_C=as(as.matrix(pheno[, 5:ncol(pheno)]),\"Incomplete\")\n",
    "    fit=softImpute(pheno_C,rank=${rank_max},lambda=${lambda_hyp},type=\"svd\")\n",
    "    X_imp_all= complete(as.matrix(pheno),fit)\n",
    "    as_tibble(X_imp_all) %>% write_delim(\"${_output:n}\", \"\\t\")\n",
    "    \n",
    "  \n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    bgzip -f ${_output:n}\n",
    "    tabix '${_output}' -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf003d",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[per_chr_softImpute_2]\n",
    "# Path to the input molecular phenotype data.\n",
    "input: group_by = \"all\"\n",
    "output: recipe = f'{cwd}/{name}.processed_phenotype.per_chrom.softImputed.recipe',\n",
    "        imp_all = f'{cwd:a}/{phenoFile:bn}.softImputed.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    library(dplyr)\n",
    "    library(readr)\n",
    "    library(stringr)\n",
    "    input = str_split(\"${_input}\", \" \")[[1]]\n",
    "    recipe_tib = tibble('#dir' = input)\n",
    "    recipe_tib = recipe_tib %>% mutate(`#chr` = str_extract(`#dir`, \"(chr\\\\d+)|chrX\")) %>% dplyr::select(c(\"#chr\", \"#dir\"))\n",
    "    write_delim(recipe_tib, delim = \"\\t\", file = \"${_output[0]}\" )\n",
    "    imp_all = NULL\n",
    "    for(i in 1:length(input)){\n",
    "      tmp = read_delim(input[i], \"\\t\")\n",
    "      imp_all = rbind(imp_all, tmp)\n",
    "      rm(tmp)\n",
    "      }\n",
    "  \n",
    "    imp_all %>% write_delim(\"${_output[1]:n}\", \"\\t\")\n",
    "  \n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    bgzip -f f'${_output[1]:n}'\n",
    "    tabix ${_output[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbf3a21",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home/a.bed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "string = path(\"home/a.bed.gz\")\n",
    "print(f'{string:n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0616ab",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[bam_subsetting]\n",
    "parameter: region = \"chr21 chr22\"\n",
    "input: phenoFile , group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.subsetted.bam'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    samtools view -b ${_input} ${region} > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631022e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[bam_to_fastq]\n",
    "input: phenoFile, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.1.fastq',f'{cwd}/{_input:bn}.2.fastq'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    samtools fastq -1 ${_output[0]} -2 ${_output[1]} ${_input}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.21.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
