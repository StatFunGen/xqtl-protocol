{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forbidden-ocean",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# GWAS integration: enrichment and colocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-salon",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This workflow processes fine-mapping results for xQTL, generated by `susie_twas` in the `cis_analysis.ipynb` notebook for cis xQTL, and GWAS fine-mapping results produced by `susie_rss` in the `rss_analysis.ipynb` notebook. It is designed to perform enrichment and colocalization analysis, particularly when fine-mapping results originate from different regions in the case of cis-xQTL and GWAS. The pipeline is capable to integrate and analyze data across these distinct regions. Originally tailored for cis-xQTL and GWAS integration, this pipeline can be applied to other pairwise integrations. An example of such application is in trans analysis, where the fine-mapped regions might be identical between trans-xQTL and GWAS, representing a special case of this broader implementation.\n",
    "\n",
    "## Input\n",
    "\n",
    "Lists of SuSiE fine-mapping output objects, in RDS format, of `class(susie)` in R. \n",
    "\n",
    "- For GWAS the list is meta-data of format: `chr`, `start`, `end`, `study_id`, `file_path` where `file_path` is an RDS file.\n",
    "- For xQTL the list is meta-data of format: `chr`, `start`, `end`, `region_id`, `condition_id`, `file_path` where `file_path` is an RDS file. `condition_id` should be optional -- if that is the case, all conditions inside of the xQTL dataset will be analyzed.\n",
    "\n",
    "## Output\n",
    "\n",
    "1. Enrichment analysis results --- this is a global enrichment estimate that combines all input data\n",
    "2. Colocalization results for regions of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-implement",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Workdir\n",
    "parameter: cwd = path(\"output\")\n",
    "# A list of file paths for fine-mapped GWAS results. \n",
    "parameter: gwas_finemapped_meta_data = path\n",
    "# A list of file paths for fine-mapped xQTL results. \n",
    "parameter: xqtl_meta_data = path\n",
    "# Optional: if a region list is provide the enrichment analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = f\"{xqtl_meta_data:bn}.{gwas_finemapped_meta_data:bn}\"\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 200\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5m\"\n",
    "# Memory expected: quite large for enrichment analysis but small for xQTL colocalization\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1885fc43-2777-4e2c-9345-e3fa1f1911d3",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in list(df.columns)]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def extract_regional_data(gwas_meta_data, xqtl_meta_data):\n",
    "    \"\"\"\n",
    "    Extracts fine-mapped results data from GWAS and xQTL metadata files and additional GWAS data provided. \n",
    "\n",
    "    Args:\n",
    "    - gwas_meta_data (str): File path to the GWAS metadata file.\n",
    "    - xqtl_meta_data (str): File path to the xQTL weight metadata file.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of two dictionaries:\n",
    "        - GWAS Dictionary: Nested dictionary with region IDs as keys\n",
    "        - xQTL Dictionary: Nested dictionary with region IDs as keys.\n",
    "    \"\"\"\n",
    "    required_gwas_columns = ['study_id', 'chrom', 'start', 'end', 'file_path']\n",
    "    required_xqtl_columns = ['region_id', 'chrom', 'start', 'end', 'condition', 'file_path']\n",
    "\n",
    "    # Process GWAS metadata\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(gwas_df, required_gwas_columns)\n",
    "    gwas_df['file_path'] = adapt_file_path_all(gwas_df, 'file_path', gwas_meta_data)\n",
    "    gwas_df['region_id'] = gwas_df.apply(lambda row: f\"{row['chrom']}:{row['start']}-{row['end']}\", axis=1)\n",
    "\n",
    "    gwas_dict = OrderedDict()\n",
    "    for _, row in gwas_df.iterrows():\n",
    "        file_paths = [fp.strip() for fp in row['file_path'].split(',')]\n",
    "        gwas_dict[row['region_id']] = {\"meta_info\": [row['chrom'], row['start'], row['end'], row['study_id']],\n",
    "                                       \"files\": file_paths}\n",
    "\n",
    "    # Process xQTL metadata\n",
    "    xqtl_df = pd.read_csv(xqtl_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(xqtl_df, required_xqtl_columns)\n",
    "    xqtl_df['file_path'] = adapt_file_path_all(xqtl_df, 'file_path', xqtl_meta_data)\n",
    "\n",
    "    xqtl_dict = OrderedDict()\n",
    "    for _, row in xqtl_df.iterrows():\n",
    "        file_paths = [fp.strip() for fp in row['file_path'].split(',')]\n",
    "        xqtl_dict[row['region_id']] = {\"meta_info\": [row['chrom'], row['start'], row['end'], row['region_id'], row['condition']],\n",
    "                                       \"files\": file_paths}\n",
    "    return gwas_dict, xqtl_dict\n",
    "\n",
    "gwas_dict, xqtl_dict = extract_regional_data(gwas_finemapped_meta_data, xqtl_meta_data)\n",
    "regional_data = dict([(\"GWAS\", gwas_dict), (\"xQTL\", xqtl_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-carnival",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "[xqtl_gwas_enrichment]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "output: f'{cwd:a}/{name}.enrichment.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "  # RDS files for GWAS data\n",
    "  gwas_finemapped_data = c(${paths([x[\"files\"] for x in regional_data[\"GWAS\"].values()]):r,})\n",
    "  # RDS files for xQTL data\n",
    "  xqtl_finemapped_data = c(${paths([x[\"files\"] for x in regional_data[\"xQTL\"].values()]):r,})\n",
    "  result = pecotmr::xqtl_enrichment_wrapper(gwas_finemapped_data, xqtl_finemapped_data)\n",
    "  writeLines(paste(names(result), unlist(result), sep = \":\"), ${_output:ar})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-benchmark",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_coloc]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "parameter: enrichment_data = path\n",
    "meta_info = [x[\"meta_info\"] for x in regional_data['xQTL'].values()]\n",
    "xqtl_files = [x[\"files\"] for x in regional_data['xQTL'].values()]\n",
    "input: xqtl_files, group_by = 1, group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[3]}.coloc.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    chrom = ${_meta_info[0]}\n",
    "    start = ${_meta_info[1]} \n",
    "    end = ${_meta_info[2]}\n",
    "    region = \"${_meta_info[3]}\"\n",
    "    xqtl_condition = \"${_meta_info[4]}\"\n",
    "    gwas_regions = c(${paths(regional_data[\"GWAS\"].keys()):r,})\n",
    "    library(pecotmr)\n",
    "    # Step 1: find relevant GWAS regions that overlap with the xQTL region of interest\n",
    "    # gwas_overlapping_regions = ...\n",
    "    # gwas_finemapping_files = ...\n",
    "    # Step 2: load enrichment analysis results\n",
    "    # coloc_priors = get_coloc_prior(${enrichment_data:r})\n",
    "    # Step 3: Apply colocalization analysis\n",
    "    # res = coloc_wrapper(${_input:r}, gwas_finemapping_files, coloc_priors)\n",
    "    # saveRDS(res, ${_output:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
