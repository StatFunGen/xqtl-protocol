{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# GWAS integration: TWAS and MR\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This module provides software implementations for transcriptome-wide association analysis (TWAS), and Mendelian Randomization using fine-mapping instrumental variables (IV). The procedures implements the MR procedure described in Zhang et al 2020 for \"causal\" effects estimation and model validation, with the unit of analysis being a single gene-trait pair.\n",
    "\n",
    "This procedure is based on the SuSiE-TWAS workflow --- it assumes that xQTL fine-mapping has been performed (to be used for both TWAS and MR) and moleuclar traits prediction weights pre-computed (to be used for TWAS). Cross validation for TWAS weights is optional but highly recommended.\n",
    "\n",
    "GWAS data required are GWAS summary statistics and LD matrix for the region of interest.\n",
    "\n",
    "### Step 1: TWAS \n",
    "\n",
    "1. Extract GWAS z-score for region of interest and corresponding LD matrix.\n",
    "2. (Optional) perform allele matching QC for the LD matrix with summary stats.\n",
    "3. Process weights: for LASSO, Elastic Net and mr.ash we have to take the weights as is for QTL variants overlapping with GWAS variants. For SuSiE weights it can be adjusted to exactly match GWAS variants.\n",
    "4. Perofrm TWAS test for multiple sets of weights. \n",
    "5. For each gene, filter TWAS results by keeping the best model selected by CV. Drop the genes that don't show good evidence of TWAS prediction weights.\n",
    "\n",
    "### Step 2: MR for candidate genes\n",
    "\n",
    "1. Limit MR only to those showing some evidence of TWAS significance AND have strong instrumental variable (fine-mapping PIP or CS). \n",
    "2. Use fine-mapped xQTL with GWAS data to perform MR. \n",
    "3. For multiple IV, aggregate individual IV estimates using a fixed-effect meta-analysis procedure.\n",
    "4. Identify and exclude results with severe violations of the exclusion restriction (ER) assumption.\n",
    "\n",
    "## Input\n",
    "\n",
    "### GWAS Data Input Interface (Similar to `susie_rss`)\n",
    "\n",
    "\n",
    "I. **GWAS Summary Statistics Files**\n",
    "- **Input**: Vector of files for one or more GWAS studies.\n",
    "- **Format**: \n",
    "  - Tab-delimited files.\n",
    "  - First 4 columns: `chr`, `pos`, `a0`, `a1`\n",
    "  - Additional columns can be loaded using column mapping file see below  \n",
    "- **Column Mapping files (optional)**:\n",
    "  - Optional YAML file for custom column mapping.\n",
    "  - Required columns: `chr`, `pos`, `a0`, `a1`, either `z` or (`betahat` and `sebetahat`).\n",
    "  - Optional columns: `n`, `var_y` (relevant to fine-mapping).\n",
    "\n",
    "II. **GWAS Summary Statistics Meta-File**: this is optional and helpful when there are lots of GWAS data to process via the same command\n",
    "- **Columns**: `study_id`, chromosome number, path to summary statistics file, optional path to column mapping file.\n",
    "- **Note**: Chromosome number `0` indicates a genome-wide file.\n",
    "\n",
    "eg: `gwas_meta.tsv`\n",
    "\n",
    "```\n",
    "study_id    chrom    file_path                 column_mapping_file\n",
    "study1      1        gwas1.tsv.gz         column_mapping.yml\n",
    "study1      2        gwas2.tsv.gz         column_mapping.yml\n",
    "study2      0        gwas3.tsv.gz         column_mapping.yml\n",
    "```\n",
    "\n",
    "If both summary stats file (I) and meta data file (II) are specified we will take the union of the two.\n",
    "\n",
    "\n",
    "III. **LD Reference Metadata File**\n",
    "- **Format**: Single TSV file.\n",
    "- **Contents**:\n",
    "  - Columns: `chr`, `start`, `end`, path to the LD matrix, genomic build.\n",
    "  - LD matrix path format: comma-separated, first entry is the LD matrix, second is the bim file.\n",
    "- **Documentation**: Refer to our LD reference preparation document for detailed information (Tosin pending update).\n",
    "\n",
    "### Output of Fine-Mapping & TWAS Pipeline\n",
    "\n",
    "I. **xQTL Weight Database files**\n",
    "- path to various weight DB files, comma delimited.\n",
    "\n",
    "II. **xQTL Weight Database Metadata File**: this is optional and helpful when TWAS is done genome-wide for many regions via the same command\n",
    "- **Types**: Gene-based or TAD-based.\n",
    "- **Structure**: \n",
    "  - RDS format.\n",
    "  - Organized hierarchically: region → condition → weight matrix.\n",
    "  - Each column represents a different method.\n",
    "- **Format**: `chrom`, `start`, `end`, `region_id`, `condition` (e.g., tissue type, QTL), path to various weight DB files, comma delimited.\n",
    "\n",
    "eg: `xqtl_meta.tsv`\n",
    "\n",
    "```\n",
    "chrom    start    end    region_id    condition    file_path\n",
    "1        1000     5000   region1      cohor1:tissue1:eQTL      weight1.rds, weight2.rds\n",
    "2        2000     6000   region2      cohor1:tissue1:eQTL      weight3.rds\n",
    "3        3000     7000   region3      cohor1:tissue1:eQTL      weight4.rds, weight5.rds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "TWAS FIXME this is incorrect for now.\n",
    "\n",
    "- Each row corresponds to a single SNP inferred as a member of a signal cluster, with columns including:\n",
    "   - `snp`: SNP name.\n",
    "   - `beta_eQTL`: eQTL effect.\n",
    "   - `se_eQTL`: Standard error of estimated eQTL effect.\n",
    "   - `beta_GWAS`: GWAS effect.\n",
    "   - `se_GWAS`: Standard error of GWAS effect.\n",
    "   - `cluster`: Signal cluster ID (credible sets index).\n",
    "   - `pip`: SNP posterior inclusion probability (PIP).\n",
    "   - `gene_id`: Gene name.\n",
    "\n",
    "\n",
    "MR\n",
    "\n",
    "-  The output includes the following columns for each gene:\n",
    "    - `gene_id`: Gene name.\n",
    "    - `num_cluster`: Number of credible sets of the gene.\n",
    "    - `num_instruments`: Number of instruments included in the gene.\n",
    "    - `spip`: Sum of PIP for credible sets of each gene.\n",
    "    - `grp_beta`: Signal-level estimates, combining SNP-level estimates from member SNPs weighted by their PIPs.\n",
    "    - `grp_se`: Standard error of signal-level estimates.\n",
    "    - `meta`: Gene-level estimate of the causal effect, aggregating signal-level estimates using a fixed-effect meta-analysis model.\n",
    "    - `se_meta`: Standard error of the gene-level estimate of the causal effect.\n",
    "    - `Q`: Cochran’s Q statistic.\n",
    "    - `I2`: $I^2$ statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output/\")\n",
    "parameter: gwas_meta_data = path()\n",
    "parameter: xqtl_meta_data = path()\n",
    "parameter: ld_meta_data = path()\n",
    "parameter: gwas_name = []\n",
    "parameter: gwas_data = []\n",
    "parameter: column_mapping = []\n",
    "parameter: name = f\"{xqtl_meta_data:bn}.{gwas_meta_data:bn}\"\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: job_size = 100\n",
    "parameter: walltime = \"5m\"\n",
    "parameter: mem = \"8G\"\n",
    "parameter: numThreads = 1\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "from collections import OrderedDict\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in list(df.columns)]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def extract_regional_data(gwas_meta_data, xqtl_meta_data, gwas_name, gwas_data, column_mapping):\n",
    "    \"\"\"\n",
    "    Extracts data from GWAS and xQTL metadata files and additional GWAS data provided. \n",
    "\n",
    "    Args:\n",
    "    - gwas_meta_data (str): File path to the GWAS metadata file.\n",
    "    - xqtl_meta_data (str): File path to the xQTL weight metadata file.\n",
    "    - gwas_name (list): vector of GWAS study names.\n",
    "    - gwas_data (list): vector of GWAS data.\n",
    "    - column_mapping (list, optional): vector of column mapping files.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of two dictionaries:\n",
    "        - GWAS Dictionary: Maps study IDs to a list containing chromosome number, \n",
    "          GWAS file path, and optional column mapping file path.\n",
    "        - xQTL Dictionary: Nested dictionary with region IDs as keys.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If any specified file path does not exist.\n",
    "    - ValueError: If required columns are missing in the input files or vector lengths mismatch.\n",
    "    \"\"\"\n",
    "    # Check vector lengths\n",
    "    if len(gwas_name) != len(gwas_data):\n",
    "        raise ValueError(\"gwas_name and gwas_data must be of equal length\")\n",
    "    \n",
    "    if len(column_mapping)>0 and len(column_mapping) != len(gwas_name):\n",
    "        raise ValueError(\"If column_mapping is provided, it must be of the same length as gwas_name and gwas_data\")\n",
    "\n",
    "    # Required columns for each file type\n",
    "    required_gwas_columns = ['study_id', 'chrom', 'file_path']\n",
    "    required_xqtl_columns = ['region_id', 'chrom', 'start', 'end', 'condition', 'file_path']\n",
    "    \n",
    "    # Reading the GWAS metadata file\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(gwas_df, required_gwas_columns)\n",
    "    gwas_dict = OrderedDict()\n",
    "\n",
    "    # Process additional GWAS data from R vectors\n",
    "    for name, data, mapping in zip(gwas_name, gwas_data, column_mapping or [None]*len(gwas_name)):\n",
    "        gwas_dict[name] = {0: [data, mapping]}\n",
    "\n",
    "    for _, row in gwas_df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        mapping_file = row.get('column_mapping_file')\n",
    "        \n",
    "        # Adjust paths if necessary\n",
    "        file_path = adapt_file_path(file_path, gwas_meta_data)\n",
    "        if mapping_file:\n",
    "            mapping_file = adapt_file_path(mapping_file,  gwas_meta_data)\n",
    "\n",
    "       # Create or update the entry for the study_id\n",
    "        if row['study_id'] not in gwas_dict:\n",
    "            gwas_dict[row['study_id']] = {}\n",
    "\n",
    "        # Expand chrom 0 to chrom 1-22 or use the specified chrom\n",
    "        chrom_range = range(1, 23) if row['chrom'] == 0 else [row['chrom']]\n",
    "        for chrom in chrom_range:\n",
    "            if chrom in gwas_dict[row['study_id']]:\n",
    "         e       existing_entry = gwas_dict[row['study_id']][chrom]\n",
    "                raise ValueError(f\"Duplicate chromosome specification for study_id {row['study_id']}, chrom {chrom}. \"\n",
    "                                 f\"Conflicting entries: {existing_entry} and {[file_path, mapping_file]}\")\n",
    "            gwas_dict[row['study_id']][chrom] = [file_path, mapping_file]\n",
    "\n",
    "    # Reading the xQTL weight metadata file\n",
    "    xqtl_df = pd.read_csv(xqtl_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(xqtl_df, required_xqtl_columns)\n",
    "    xqtl_dict = OrderedDict()\n",
    "    for _, row in xqtl_df.iterrows():\n",
    "        file_paths = [adapt_file_path(fp.strip(), xqtl_meta_data) for fp in row['file_path'].split(',')]  # Splitting and stripping file paths\n",
    "        xqtl_dict[row['region_id']] = {\"meta_info\": [row['chrom'], row['start'], row['end'], row['region_id'], row['condition']],\n",
    "                                       \"files\": file_paths}\n",
    "    return gwas_dict, xqtl_dict\n",
    "\n",
    "gwas_dict, xqtl_dict = extract_regional_data(gwas_meta_data, xqtl_meta_data, gwas_name, gwas_data, column_mapping)\n",
    "regional_data = dict([(\"GWAS\", gwas_dict), (\"xQTL\", xqtl_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[twas_mr]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "meta_info = [x[\"meta_info\"] for x in regional_data['xQTL'].values()]\n",
    "xqtl_files = [x[\"files\"] for x in regional_data['xQTL'].values()]\n",
    "input: xqtl_files, group_by = lambda x: group_by_region(x, xqtl_files), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[3]}.twas_mr.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    # we have potentially multiple weight db RDS files for each region of interest\n",
    "    weight_db = c(${_input:r,})\n",
    "    chrom = ${_meta_info[0]}\n",
    "    start = ${_meta_info[1]} \n",
    "    end = ${_meta_info[2]}\n",
    "    region = \"${_meta_info[3]}\"\n",
    "    xqtl_conditions = c(${paths(_meta_info[4:]):r,})\n",
    "    gwas_studies = c(${paths(regional_data[\"GWAS\"].keys()):r,})\n",
    "    # load gwas data file for this particular chrom\n",
    "    gwas_files = c(${paths([v[_meta_info[0]] for k, v in regional_data[\"GWAS\"].items()]):r,})\n",
    "    library(pecotmr)\n",
    "    # Step 0: Load GWAS data for the region of interest, for each study\n",
    "    # Generate the region of interest\n",
    "    region_of_interest = data.frame(chrom = chrom, start = start, end = end)\n",
    "    # Generate the LD_meta_file data frame as the input of load_LD_matrix function\n",
    "    LD_metadata = genomic_file_paths(paste0(ld_meta_data,\"/\",chrom,sep=\"\"))\n",
    "    gwas_data = list()\n",
    "    for (s in length(gwas_studies)) {\n",
    "      gwas_data[[gwas_studies[s]]] = list()\n",
    "      gwas_sumstats = fread(gwas_files[s])%>% \n",
    "                 rename(\"pos\" = \"position\", \"chrom\" = \"chromosome\", \"A1\" = \"ref\",\"A2\" = \"alt\")%>%\n",
    "                 mutate(z=beta/se)\n",
    "      # Load LD list containing LD matrix and corresponding variants\n",
    "      gwas_LD_list = load_LD_matrix(LD_metadata, region_of_interest, gwas_sumstats)\n",
    "      # Allele flip\n",
    "      gwas_allele_flip= allele_qc(gwas_sumstats, gwas_LD_list$combined_LD_variants, match.min.prop=0.2, remove_dups=FALSE, flip=TRUE, remove=TRUE)\n",
    "      # Load LD matrix and sumstats\n",
    "      gwas_data[[gwas_studies[s]]][\"LD\"] = gwas_LD_list$combined_LD_matrix\n",
    "      gwas_data[[gwas_studies[s]]][\"sumstats\"] = gwas_allele_flip %>% mutate(variant_allele_flip = paste(chrom,pos,A1.sumstats,A2.sumstats,sep=\":\"))      \n",
    "    }\n",
    "    for (condition in xqtl_conditions) {\n",
    "        # For each region of interest for a particular xQTL (condtion), we can perform TWAS for all the input studies, with all of the weights available\n",
    "        # Step 1: load the weight matrix: a matrix of weights for the specified condition and region; each column is weight for a method. The row names should be variant names.\n",
    "        weights[[xqtl_conditions[condition]]] = list()\n",
    "        # For each condition, we extract the weights of multiple weight df RDS file\n",
    "        for (weight_db_file in length(weight_db)){\n",
    "        weights_matrix = load_twas_weights(weight_db[weight_db_file], condition, variable_name_obj=\"variant_names\", twas_weights_table = \"twas_weights\")\n",
    "        adjusted_susie_weights = adjust_susie_weights(weight_db[weight_db_file], weights_matrix, keep_variants = gwas_data[[gwas_studies[s]]][\"sumstats\"]$variant_allele_flip)\n",
    "        # Step 2: for each study we perform TWAS: \n",
    "        # take overlap between weights and the gwas_data summstats for each study. If there are some variants in sumstats but not in weights then simply make weight zero\n",
    "        # Step 2-1: NOT YET IMPLEMEMENTED (FIXME) adjust susie_weights if possible --- if susie_fit exist in weight_db. This should replace the origional SuSiE_weight computed above\n",
    "        # Step 3: perform TWAS on the weights and sumstats\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
