{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monthly-offer",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Quantifying expression from RNA-seq data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ea40d-cc28-4fe8-95e6-a52f78081cc8",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c71d20-0645-41b5-b187-c6215ca22c5a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Our pipeline follows the GTEx pipeline from the [GTEx](https://gtexportal.org/home/aboutGTEx#staticTextAnalysisMethods)/[TOPMed](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md) project for the quantification of expression from RNA-seq data. Either paired end or single end fastq.gz files may be used as the initial input. Different read [strandedness options](https://rnabio.org/module-09-appendix/0009/12/01/StrandSettings/) are possible, including rf, fr, unstranded or strand_missing, based on library types from Signal et al [[cf. Signal et al (2022)](https://doi.org/10.1186/s12859-022-04572-7)]. Strand detection steps are included in this pipeline to automaticaly detect the strand of the input fastq file by levradging the gene count table ouptut from [STAR](https://physiology.med.cornell.edu/faculty/skrabanek/lab/angsd/lecture_notes/STARmanual.pdf). Read length data is required and is set to a default value of 100 for reads of zero length. \n",
    "\n",
    "We recommend using fastqc to quality control reads, followed by the removal of adaptors with fastp if necessary. After quality control has been conducted, STAR may be used to align reads to the reference genome. We combine this mapping step with an additionaly quality control step using Picard to mark duplicate reads and collect other metrics. \n",
    "\n",
    "Gene-level RNA expression is called with RNA-SeQC v2.4.2 using a reference gtf that has been collapsed to contain genes instead of gene transcripts [[cf. DeLuca et al., Bioinformatics, 2012](https://doi.org/10.1093/bioinformatics/bts196)]. Reads are only included if they are uniquely mapped (mapping quality of 255 for STAR BAMs), if they are aligned in proper pairs, if the read alignment distance was less than or equal to six (i.e. alignments must not contain more than six non-reference bases), and if they are fully contained within exon boundaries. Reads overlapping introns are not included. Exon level read counts are also called by RNA-SeQC. If a read overlaps multiple exons, then a fractional value equal to the portion of the read contained within the exon is allotted. We call transcript-level RNA expression using RSEM v1.3.0 with a transcript reference gtf. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cca1b-3537-44e3-a2ca-d7e95c4a12c9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "A tab delimited file containing sample read information with one line per sample. This file should include a header for the following columns:\n",
    "\n",
    "1. `ID` - sample ID (required)\n",
    "2. `fq1` - name of the fastq file (required)\n",
    "3. `fq2` - name of the second fastq file of the sample if using paired end data (required only if using paired end data)\n",
    "4. `strand` - the strandedness of the sample (optional)  \n",
    "[Options include](https://rnabio.org/module-09-appendix/0009/12/01/StrandSettings/) rf, fr, unstranded or strand_missing, based on library types from Signal et al [[cf. Signal et al. 2022](https://doi.org/10.1186/s12859-022-04572-7)]. Strand detection steps are included in this pipeline to automaticaly detect the strand of the input fastq file by leveraging the gene count table ouptut from STAR.  \n",
    "5. `read_length` - the read length of the sample's reads (optional)  \n",
    "If this is unknown, enter 0 and the pipeline will use a default read length of 100. This default can be changed using the `--sjdbOverhang` parameter. If none of the samples have read length information then please leave out the read_length column so that the pipeline uses the default length for each sample. \n",
    "\n",
    "## Output\n",
    "\n",
    "Gene and transcript expression matrices in addition to other intermediate files (see `Command interface` steps below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8beb9f9-f655-4c43-b127-e7b1102de571",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965179e6-2253-498a-9573-107cc6953d37",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### i. Perform data quality summary via `fastqc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f216e-f6cb-4b57-8658-eb6ba2bd4582",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing: <4 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c754af09-254b-4045-a43e-2e79822b2fe5",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Generate fastqc report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982ce9b-9976-4a00-991b-945129dcc809",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f113a66-973b-4b61-8209-b0cf182b93ad",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mfastqc\u001b[0m: \n",
      "INFO: t6c9b3702399ca271 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473763 (\"job_t6c9b3702399ca271\") has been submitted\n",
      "INFO: t4e4ffcdfae79f275 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473764 (\"job_t4e4ffcdfae79f275\") has been submitted\n",
      "INFO: t002fa5ea2fc74efe \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473765 (\"job_t002fa5ea2fc74efe\") has been submitted\n",
      "INFO: tf93ce24f1580b2b3 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473766 (\"job_tf93ce24f1580b2b3\") has been submitted\n",
      "INFO: t386bb7e2e8436820 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473767 (\"job_t386bb7e2e8436820\") has been submitted\n",
      "INFO: t54ae889b31a122b1 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473768 (\"job_t54ae889b31a122b1\") has been submitted\n",
      "INFO: t3f1ff409f6f0c26c \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473769 (\"job_t3f1ff409f6f0c26c\") has been submitted\n",
      "INFO: t7750753938a38086 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473770 (\"job_t7750753938a38086\") has been submitted\n",
      "INFO: t1a1d8d75185936a3 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473771 (\"job_t1a1d8d75185936a3\") has been submitted\n",
      "INFO: t7d74fb9d4370b2f8 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473772 (\"job_t7d74fb9d4370b2f8\") has been submitted\n",
      "INFO: tf25a5c96c0afea02 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473773 (\"job_tf25a5c96c0afea02\") has been submitted\n",
      "INFO: t50f350a2d0a9b770 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473774 (\"job_t50f350a2d0a9b770\") has been submitted\n",
      "INFO: tcc8278b405e89bca \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473775 (\"job_tcc8278b405e89bca\") has been submitted\n",
      "INFO: t62c5fa6ee2f7c7dc \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473776 (\"job_t62c5fa6ee2f7c7dc\") has been submitted\n",
      "INFO: t8ad9981a328c22ce \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473777 (\"job_t8ad9981a328c22ce\") has been submitted\n",
      "INFO: t365ddb198dd08add \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473778 (\"job_t365ddb198dd08add\") has been submitted\n",
      "INFO: t1d88d659d66b24f7 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473779 (\"job_t1d88d659d66b24f7\") has been submitted\n",
      "INFO: t2d89152690c6fdd0 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473780 (\"job_t2d89152690c6fdd0\") has been submitted\n",
      "INFO: t03056c7c071f8774 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473781 (\"job_t03056c7c071f8774\") has been submitted\n",
      "INFO: t3be3c3d82d746be4 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473782 (\"job_t3be3c3d82d746be4\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m20\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "\u001b[91mERROR\u001b[0m: \u001b[91m[fastqc]: [fastqc]: Output target /restricted/projectnb/xqtl/xqtl_protocol/output_test/1000-PCC.bam.1.fq_fastqc.html does not exist after the completion of step fastqc\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!sos run RNA_calling.ipynb fastqc \\\n",
    "    --cwd ../../output_test \\\n",
    "    --sample-list ../../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --container oras://ghcr.io/cumc/rna_quantification_apptainer:latest \\\n",
    "    -c ../csg.yml  -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69fb99-24fb-4714-8b50-4e4ec5dbd435",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### ii. Cut adaptor (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da496d3-70e3-4547-b41c-664155fc55af",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing: ~10 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933150be-b05b-4b18-97f1-42c291867b7f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Trim adaptors with fastp. A new sample list will be generated witht the trimmed fastq files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4486eb-2869-4b99-bd6a-86c371e9368e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mfastp_trim_adaptor_1\u001b[0m: \n",
      "INFO: tf7f9ca7a0415220a \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473815 (\"job_tf7f9ca7a0415220a\") has been submitted\n",
      "INFO: t3a0f6bb6054347f9 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473816 (\"job_t3a0f6bb6054347f9\") has been submitted\n",
      "INFO: t11993d4365affa80 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473817 (\"job_t11993d4365affa80\") has been submitted\n",
      "INFO: t81717ecba3dd47ef \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473818 (\"job_t81717ecba3dd47ef\") has been submitted\n",
      "INFO: t67c489f5d6c4c96d \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473820 (\"job_t67c489f5d6c4c96d\") has been submitted\n",
      "INFO: tbd40a790cf6fff58 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473821 (\"job_tbd40a790cf6fff58\") has been submitted\n",
      "INFO: t2f7a8204d07a3580 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473822 (\"job_t2f7a8204d07a3580\") has been submitted\n",
      "INFO: t5053d7957ff62683 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473823 (\"job_t5053d7957ff62683\") has been submitted\n",
      "INFO: t3ca261894a797116 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473824 (\"job_t3ca261894a797116\") has been submitted\n",
      "INFO: tedb41e7f30829651 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2473825 (\"job_tedb41e7f30829651\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mfastp_trim_adaptor_1\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/1000-PCC.bam.1.fq.trimmed.fq.gz /restricted/projectnb/xqtl/xqtl_protocol/output_test/1000-PCC.bam.2.fq.trimmed.fq.gz... (20 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mfastp_trim_adaptor_2\u001b[0m: \n",
      "INFO: \u001b[32mfastp_trim_adaptor_2\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mfastp_trim_adaptor_2\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/../../PCC_sample_list_subset.trimmed.txt\u001b[0m\n",
      "INFO: Workflow fastp_trim_adaptor (ID=w22250ab85e96dedb) is executed successfully with 2 completed steps, 11 completed substeps and 10 completed tasks.\n"
     ]
    }
   ],
   "source": [
    "!sos run RNA_calling.ipynb fastp_trim_adaptor \\\n",
    "    --cwd ../../output_test \\\n",
    "    --sample-list ../../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --reference-fasta ../../reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
    "    --container oras://ghcr.io/cumc/rna_quantification_apptainer:latest \\\n",
    "    -c ../csg.yml  -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df42f2-035d-430e-b8e4-f38770ca1826",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### iii. Read alignment via STAR and QC via Picard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37e5aa-4e08-417f-8768-8bddef238a96",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing <2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d64589-0ee4-4789-8f1c-b94b05d1752f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Align the reads with STAR and generate the bam_list recipe for downstream molecular phenotype count matrixes. This step also checks the standedness of the data and removes duplicates with Picard. If --varVCFfile is provided, VW tags will be added to the alignments, and filtering based on these tags will be performed. The --wasp option applies WASP correction, while --unique enables unique filtering. Using both options applies both filters, while omitting them means no filtering is applied. Note that filtering is not required when quantifying expression for eQTLs.\n",
    "**NB:** If you just provide `varVCFfile` but not provide `--unique` or `--wasp`, filtering won't be performed although the name of the output bam files will contain `_wasp_qc`. This is required if aligning reads to generate splicing data later in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286a074a-fa8e-4ceb-a7d4-643cde0bc756",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mSTAR_align_1\u001b[0m: \n",
      "Insufficent memory for STAR, changing to 40G\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "Using read length specified in the sample list\n",
      "INFO: tff65c4e72cd9bf6b \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: t4b927e18be9c4b88 \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: tff65c4e72cd9bf6b \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399515 (\"job_tff65c4e72cd9bf6b\") has been submitted\n",
      "INFO: tcaa28c0bb65f9149 \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: tfda1f7e13e034e17 \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: t4b927e18be9c4b88 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399516 (\"job_t4b927e18be9c4b88\") has been submitted\n",
      "INFO: te8abdb425a2403aa \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: tec167ba0767b70e7 \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: tcaa28c0bb65f9149 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399517 (\"job_tcaa28c0bb65f9149\") has been submitted\n",
      "INFO: te2655672abbc9b17 \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: t87915c95c4041d60 \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: tfda1f7e13e034e17 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399518 (\"job_tfda1f7e13e034e17\") has been submitted\n",
      "INFO: te07ad3a636b7b1dd \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: te8abdb425a2403aa \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399519 (\"job_te8abdb425a2403aa\") has been submitted\n",
      "INFO: tdf4f824b17a8a5dc \u001b[32mrestart\u001b[0m from status \u001b[32msubmitted\u001b[0m\n",
      "INFO: tec167ba0767b70e7 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399520 (\"job_tec167ba0767b70e7\") has been submitted\n",
      "INFO: te2655672abbc9b17 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399521 (\"job_te2655672abbc9b17\") has been submitted\n",
      "INFO: t87915c95c4041d60 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399522 (\"job_t87915c95c4041d60\") has been submitted\n",
      "INFO: te07ad3a636b7b1dd \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399523 (\"job_te07ad3a636b7b1dd\") has been submitted\n",
      "INFO: tdf4f824b17a8a5dc \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399524 (\"job_tdf4f824b17a8a5dc\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m8\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m4\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m4\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m4\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: \u001b[32mSTAR_align_1\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/1000-PCC.bam.Aligned.sortedByCoord.out.bam /restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/1000-PCC.bam.Aligned.toTranscriptome.out.bam... (20 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mstrand_detected_1\u001b[0m: \n",
      "for sample 1000-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9945125919850982, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=0) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1001-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9946500588593552, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=1) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1002-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9943981411103944, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=2) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1003-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.99371513806055, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=3) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1004-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9947175340513231, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=4) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1005-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9943227514543671, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=5) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1006-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9951195009722332, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=6) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1009-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9941384572915384, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=7) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1010-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.9905496531374057, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=8) is \u001b[32mcompleted\u001b[0m.\n",
      "for sample 1011-PCC.bam\n",
      "Counts for the 2nd read strand aligned with RNA is 0.994062350839579, > 90% of aligned count\n",
      "Data is likely RF/fr-firststrand\n",
      "INFO: \u001b[32mstrand_detected_1\u001b[0m (index=9) is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: Running \u001b[32mstrand_detected_2\u001b[0m: \n",
      "Using strand specified in the input samples list ['rf', 'rf', 'rf', 'rf', 'rf', 'rf', 'rf', 'rf', 'rf', 'rf'], replacing strand_missing with detected strand\n",
      "INFO: \u001b[32mstrand_detected_2\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: Running \u001b[32mSTAR_align_2\u001b[0m: \n",
      "INFO: t6dcd2e18a3a45ff6 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t64cea24f76f181ec \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t6dcd2e18a3a45ff6 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399713 (\"job_t6dcd2e18a3a45ff6\") has been submitted\n",
      "INFO: td34a8dded96d0613 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t2b1cc10183d526af \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t64cea24f76f181ec \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399714 (\"job_t64cea24f76f181ec\") has been submitted\n",
      "INFO: t448a4eafbcb0db0e \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: td34a8dded96d0613 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399715 (\"job_td34a8dded96d0613\") has been submitted\n",
      "INFO: tfeedd0c0bb469db9 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: td2521716031d133c \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t2b1cc10183d526af \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399716 (\"job_t2b1cc10183d526af\") has been submitted\n",
      "INFO: t052b51dab89cb9f6 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: taa6cb5680f559bc8 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t448a4eafbcb0db0e \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399717 (\"job_t448a4eafbcb0db0e\") has been submitted\n",
      "INFO: tc238906499d07145 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: tfeedd0c0bb469db9 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399718 (\"job_tfeedd0c0bb469db9\") has been submitted\n",
      "INFO: td2521716031d133c \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399719 (\"job_td2521716031d133c\") has been submitted\n",
      "INFO: t052b51dab89cb9f6 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399720 (\"job_t052b51dab89cb9f6\") has been submitted\n",
      "INFO: taa6cb5680f559bc8 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399721 (\"job_taa6cb5680f559bc8\") has been submitted\n",
      "INFO: tc238906499d07145 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2399722 (\"job_tc238906499d07145\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m8\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m8\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m4\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mSTAR_align_2\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/1000-PCC.bam.alignment_summary_metrics /restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/1000-PCC.bam.rna_metrics... (40 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mSTAR_align_3\u001b[0m: \n",
      "INFO: t4431c5c078751a48 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2400115 (\"job_t4431c5c078751a48\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mSTAR_align_3\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/PCC_sample_list_subset_bam_list\u001b[0m\n",
      "INFO: Workflow STAR_align (ID=w9e2ac856c879aa3f) is executed successfully with 5 completed steps, 32 completed substeps and 21 completed tasks.\n"
     ]
    }
   ],
   "source": [
    "# STAR alignment without adding tags for filters(of course without filter): the execution speed will also be shorter than STARalignment without filter \n",
    "!sos run RNA_calling.ipynb STAR_align \\\n",
    "    --cwd ../../output_test/star_output \\\n",
    "    --sample_list ../../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container oras://ghcr.io/cumc/rna_quantification_apptainer:latest \\\n",
    "    --reference-fasta ../../reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
    "    --sample-name 1000-PCC.bam \\\n",
    "    -s build -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96171a-3220-4c46-be35-625ac34b3bc1",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# STAR alignment with only WASP correction to diminish allel-specific bias:\n",
    "!sos run RNA_calling.ipynb STAR_align \\\n",
    "    --cwd ../../output_test/star_output/wasp \\\n",
    "    --sample_list ../../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container oras://ghcr.io/cumc/rna_quantification_apptainer:latest \\\n",
    "    --reference-fasta ../../reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
    "    --varVCFfile ../../reference_data/ZOD14598_AD_GRM_WGS_2021-04-29_all.recalibrated_variants.leftnorm.filtered.AF.WASP.vcf\n",
    "    --sample-name 1000-PCC.bam \\\n",
    "    --wasp yes \\\n",
    "    -s build -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70df22b-3b35-45a7-9950-f55c911c3461",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# STAR alignment with only Unique filtering to get unique reads:\n",
    "!sos run RNA_calling.ipynb STAR_align \\\n",
    "    --cwd ../../output_test/star_output/unique \\\n",
    "    --sample_list ../../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container oras://ghcr.io/cumc/rna_quantification_apptainer:latest \\\n",
    "    --reference-fasta ../../reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
    "    --varVCFfile ../../reference_data/ZOD14598_AD_GRM_WGS_2021-04-29_all.recalibrated_variants.leftnorm.filtered.AF.WASP.vcf\n",
    "    --sample-name 1000-PCC.bam \\\n",
    "    --unique yes \\\n",
    "    -s build -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd8ed6-244c-4868-a401-12dcf5a61f77",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# STAR alignment with both WASP correction and unique filtering:\n",
    "!sos run RNA_calling.ipynb STAR_align \\\n",
    "    --cwd ../../output_test/star_output/unique_wasp \\\n",
    "    --sample_list ../../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container oras://ghcr.io/cumc/rna_quantification_apptainer:latest \\\n",
    "    --reference-fasta ../../reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
    "    --varVCFfile ../../reference_data/ZOD14598_AD_GRM_WGS_2021-04-29_all.recalibrated_variants.leftnorm.filtered.AF.WASP.vcf\n",
    "    --sample-name 1000-PCC.bam \\\n",
    "    --unique yes \\\n",
    "    --wasp yes \\\n",
    "    -s build -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1780b55-92c5-4052-87b8-7c62991e4573",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### iv. Call gene-level RNA expression via rnaseqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35130f-8b15-40ce-8766-35cba81348a8",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing <30 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dcb6a8-6820-498a-844a-3cd751c39f9d",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Call gene-level RNA expression using rnaseqc and run multiqc. The gtf file must include gene-level data instead of transcript-level data. Add `--varVCFfile` if adding tags during `STAR_align`. Change your `--cwd` option for different filter choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe078beb-f8d1-4a9b-a358-c49a9cd87b47",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mrnaseqc_call_1\u001b[0m: \n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=1) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=2) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=3) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=4) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=5) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=6) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=7) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=8) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_1\u001b[0m (index=9) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: \u001b[32mrnaseqc_call_1\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/1000-PCC.bam.rnaseqc.gene_tpm.gct.gz /restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/1000-PCC.bam.rnaseqc.gene_reads.gct.gz... (40 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mrnaseqc_call_2\u001b[0m: \n",
      "INFO: Step \u001b[32mrnaseqc_call_2\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_2\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: \u001b[32mrnaseqc_call_2\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/PCC_sample_list_subset.rnaseqc.gene_tpm.gct.gz /restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/PCC_sample_list_subset.rnaseqc.gene_readsCount.gct.gz... (4 items)\u001b[0m\n",
      "INFO: Running \u001b[32mrnaseqc_call_3\u001b[0m: \n",
      "INFO: Step \u001b[32mrnaseqc_call_3\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: Step \u001b[32mrnaseqc_call_3\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: \u001b[32mrnaseqc_call_3\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/PCC_sample_list_subset.multiqc_report.html\u001b[0m\n",
      "INFO: Running \u001b[32mrnaseqc_call_4\u001b[0m: \n",
      "INFO: t1555865b4d80b5cd \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474362 (\"job_t1555865b4d80b5cd\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mrnaseqc_call_4\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/PCC_sample_list_subset.picard.aggregated_quality.metrics.tsv\u001b[0m\n",
      "INFO: Workflow rnaseqc_call (ID=w79a27a4da28c0267) is executed successfully with 1 completed step, 3 ignored steps, 12 ignored substeps and 11 completed tasks.\n"
     ]
    }
   ],
   "source": [
    "!sos run RNA_calling.ipynb rnaseqc_call \\\n",
    "    --cwd ../../output_test/star_output \\\n",
    "    --sample-list ../../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf \\\n",
    "    --container oras://ghcr.io/cumc/rna_quantification_apptainer:latest \\\n",
    "    --reference-fasta ../../reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
    "    --bam-list ../../output_test/star_output/PCC_sample_list_subset_bam_list \\\n",
    "    -s build  -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0ef67-f05e-4c32-be1d-516d0d7c17d0",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### v. Call transcript level RNA expression via RSEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34363ee-6890-4b7d-8dfb-4080a720502b",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing <X hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7ae70-de76-4441-9ee5-fce2bc5a6d1b",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Call transcript level RNA expression using RSEM and run multiqc. The gtf file used as input should match the one used to generate the RSEM index and therefore contain transcript-level, not gene-level, information. Add `--varVCFfile` if adding tags during `STAR_align`. Change your `--cwd` option for different filter choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9d7c9-1c8f-4c25-81d4-04ca543df67d",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e3befc-5b19-4073-96e0-e9925b01eb34",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input samples are paired-end sequences.\n",
      "INFO: Running \u001b[32mrsem_call_1\u001b[0m: \n",
      "INFO: t17683f24f8513b9f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474418 (\"job_t17683f24f8513b9f\") has been submitted\n",
      "INFO: tfd9bbafcf2595672 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474419 (\"job_tfd9bbafcf2595672\") has been submitted\n",
      "INFO: t660ceef6ffd66beb \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474420 (\"job_t660ceef6ffd66beb\") has been submitted\n",
      "INFO: t86853a6bef66a16f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474421 (\"job_t86853a6bef66a16f\") has been submitted\n",
      "INFO: t964856c7b9681030 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474422 (\"job_t964856c7b9681030\") has been submitted\n",
      "INFO: t2445a0809b0320b4 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474423 (\"job_t2445a0809b0320b4\") has been submitted\n",
      "INFO: td531b7f2ccc37b56 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474424 (\"job_td531b7f2ccc37b56\") has been submitted\n",
      "INFO: t5826bd542c40b4aa \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474425 (\"job_t5826bd542c40b4aa\") has been submitted\n",
      "INFO: t5a87f52c0ac43d60 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474426 (\"job_t5a87f52c0ac43d60\") has been submitted\n",
      "INFO: t1c886f27aef992cb \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474427 (\"job_t1c886f27aef992cb\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m10\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m9\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m8\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m8\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m8\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m7\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m6\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m5\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m4\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m4\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m4\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m3\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m2\u001b[0m tasks.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mrsem_call_1\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/1000-PCC.bam.rsem.isoforms.results /restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/1000-PCC.bam.rsem.genes.results... (30 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mrsem_call_2\u001b[0m: \n",
      "INFO: t8726dc727d5bd7b2 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2474550 (\"job_t8726dc727d5bd7b2\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "\u001b[91mERROR\u001b[0m: \u001b[91m[rsem_call_2]: [t8726dc727d5bd7b2]: Executing script in Singularity returns an error (exitcode=1, stderr=/restricted/projectnb/xqtl/xqtl_protocol/output_test/star_output/PCC_sample_list_subset.rsem.aggregated_quality.metrics.stderr).\n",
      "The script has been saved to /usr3/graduate/fgrennjr/.sos/bcd8530c53fd4b87//usr3/graduate/fgrennjr/.sos/bcd8530c53fd4b87.To reproduce the error please run:\n",
      "\u001b[0m\u001b[32msingularity exec  /usr3/graduate/fgrennjr/.sos/singularity/library/ghcr.io-cumc-rna_quantification_apptainer-latest.sif micromamba run -a \"\" -n rna_quantification Rscript /usr3/graduate/fgrennjr/.sos/bcd8530c53fd4b87/singularity_run_768303.R\u001b[0m\u001b[91m\n",
      "[rsem_call]: Exits with 2 pending steps (rsem_call_3, rsem_call_4)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sos run RNA_calling.ipynb rsem_call \\\n",
    "    --cwd ../../output_test/star_output \\\n",
    "    --sample-list ../../PCC_sample_list_subset \\\n",
    "    --data-dir /restricted/projectnb/amp-ad/ROSMAP_PCC_AC/PCC/ \\\n",
    "    --STAR-index ../../reference_data/STAR_Index/ \\\n",
    "    --gtf ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container oras://ghcr.io/cumc/rna_quantification_apptainer:latest \\\n",
    "    --reference-fasta ../../reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --ref-flat ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.ref.flat \\\n",
    "    --bam-list ../../output_test/star_output/PCC_sample_list_subset_bam_list \\\n",
    "    --RSEM-index ../../reference_data/RSEM_Index \\\n",
    "    -s build  -c ../csg.yml  -q neurology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad498164-72b9-4071-b52b-70f61c13648b",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3c1de-915c-44a4-84dc-e121c2b278fe",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "| Step | Substep | Problem | Possible Reason | Solution |\n",
    "|------|---------|---------|------------------|---------|\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-worse",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supported-mozambique",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run RNA_calling.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  bam_to_fastq\n",
      "  fastqc\n",
      "  fastp_trim_adaptor\n",
      "  trimmomatic_trim_adaptor\n",
      "  STAR_align\n",
      "  strand_detected\n",
      "  picard_qc\n",
      "  rnaseqc_call\n",
      "  rsem_call\n",
      "  filter_reads\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        The output directory for generated files.\n",
      "  --sample-list VAL (as path, required)\n",
      "                        Sample meta data list\n",
      "  --sample-name  (as list)\n",
      "                        Sample names to analyze\n",
      "  --data-dir  path(f\"{sample_list:d}\")\n",
      "\n",
      "                        Raw data directory, default to the same directory as\n",
      "                        sample list\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --java-mem 6G\n",
      "                        Memory for Java virtual mechine (`picard`)\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "  --varVCFfile ''\n",
      "                        VarVCFfile for data preparation for wasp_filtering\n",
      "  --entrypoint  ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
      "\n",
      "  --[no-]uncompressed (default to False)\n",
      "                        Whether the fasta/fastq file is compressed or not.\n",
      "\n",
      "Sections\n",
      "  bam_to_fastq:\n",
      "  fastqc:\n",
      "  fastp_trim_adaptor_1:\n",
      "    Workflow Options:\n",
      "      --window-size 4 (as int)\n",
      "                        sliding window setting\n",
      "      --required-quality 20 (as int)\n",
      "      --leading 20 (as int)\n",
      "                        the mean quality requirement option for cut_front\n",
      "      --trailing 20 (as int)\n",
      "                        the mean quality requirement option for cut_tail\n",
      "      --min-len 15 (as int)\n",
      "                        reads shorter than length_required will be discarded\n",
      "      --fasta-with-adapters-etc . (as path)\n",
      "                        Path to the reference adaptors\n",
      "  fastp_trim_adaptor_2:\n",
      "  trimmomatic_trim_adaptor:\n",
      "    Workflow Options:\n",
      "      --fasta-with-adapters-etc . (as path)\n",
      "                        Illumina clip setting Path to the reference adaptors\n",
      "      --seed-mismatches 2 (as int)\n",
      "      --palindrome-clip-threshold 30 (as int)\n",
      "      --simple-clip-threshold 10 (as int)\n",
      "      --window-size 4 (as int)\n",
      "                        sliding window setting\n",
      "      --required-quality 20 (as int)\n",
      "      --leading 3 (as int)\n",
      "                        Other settings\n",
      "      --trailing 3 (as int)\n",
      "      --min-len 50 (as int)\n",
      "  STAR_align_1:\n",
      "    Workflow Options:\n",
      "      --gtf VAL (as path, required)\n",
      "                        Reference gene model\n",
      "      --STAR-index VAL (as path, required)\n",
      "                        STAR indexing file\n",
      "      --outFilterMultimapNmax 20 (as int)\n",
      "      --alignSJoverhangMin 8 (as int)\n",
      "      --alignSJDBoverhangMin 1 (as int)\n",
      "      --outFilterMismatchNmax 999 (as int)\n",
      "      --outFilterMismatchNoverLmax 0.1 (as float)\n",
      "      --alignIntronMin 20 (as int)\n",
      "      --alignIntronMax 1000000 (as int)\n",
      "      --alignMatesGapMax 1000000 (as int)\n",
      "      --outFilterType BySJout\n",
      "      --outFilterScoreMinOverLread 0.33 (as float)\n",
      "      --outFilterMatchNminOverLread 0.33 (as float)\n",
      "      --limitSjdbInsertNsj 1200000 (as int)\n",
      "      --outSAMstrandField intronMotif\n",
      "      --outFilterIntronMotifs None\n",
      "      --alignSoftClipAtReferenceEnds Yes\n",
      "      --quantMode TranscriptomeSAM GeneCounts (as list)\n",
      "      --outSAMattrRGline ID:rg1 SM:sm1 (as list)\n",
      "      --outSAMattributes NH HI AS nM NM ch (as list)\n",
      "      --chimSegmentMin 15 (as int)\n",
      "      --chimJunctionOverhangMin 15 (as int)\n",
      "      --chimOutType Junctions WithinBAM SoftClip (as list)\n",
      "      --chimMainSegmentMultNmax 1 (as int)\n",
      "      --sjdbOverhang 100 (as int)\n",
      "      --mapping-quality 255 (as int)\n",
      "  strand_detected_1:\n",
      "  strand_detected_2:\n",
      "    Workflow Options:\n",
      "      --strand ''\n",
      "  picard_qc, STAR_align_2:\n",
      "    Workflow Options:\n",
      "      --gtf VAL (as path, required)\n",
      "                        Reference gene model\n",
      "      --ref-flat VAL (as path, required)\n",
      "                        Path to flat reference file, for computing QC metric\n",
      "      --reference-fasta VAL (as path, required)\n",
      "                        The fasta reference file used to generate star index\n",
      "      --optical-distance 100 (as int)\n",
      "                        For the patterned flowcell models (HiSeq X), change to\n",
      "                        2500\n",
      "      --[no-]zap-raw-bam (default to False)\n",
      "  STAR_align_3:\n",
      "  rnaseqc_call_1:\n",
      "    Workflow Options:\n",
      "      --bam-list VAL (as path, required)\n",
      "      --gtf VAL (as path, required)\n",
      "                        Reference gene model\n",
      "      --detection-threshold 5 (as int)\n",
      "      --mapping-quality 255 (as int)\n",
      "  rnaseqc_call_2:\n",
      "    Workflow Options:\n",
      "      --bam-list VAL (as path, required)\n",
      "  rsem_call_1:\n",
      "    Workflow Options:\n",
      "      --bam-list VAL (as path, required)\n",
      "      --RSEM-index VAL (as path, required)\n",
      "      --max-frag-len 1000 (as int)\n",
      "      --[no-]estimate-rspd (default to True)\n",
      "  rsem_call_2:\n",
      "    Workflow Options:\n",
      "      --bam-list VAL (as path, required)\n",
      "  rsem_call_3, rnaseqc_call_3:\n",
      "  rsem_call_4, rnaseqc_call_4:\n",
      "  filter_reads:\n",
      "    Workflow Options:\n",
      "      --unique ''\n",
      "      --wasp ''\n",
      "      --mapping-quality 255 (as int)\n"
     ]
    }
   ],
   "source": [
    "!sos run RNA_calling.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-frame",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-measurement",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files.\n",
    "parameter: cwd = path(\"output\")\n",
    "# Sample meta data list\n",
    "parameter: sample_list = path\n",
    "# Sample names to analyze\n",
    "parameter: sample_name = list() #input should be --sample_name sample1 sample2, if multiple samples\n",
    "# Raw data directory, default to the same directory as sample list\n",
    "parameter: data_dir = path(f\"{sample_list:d}\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Memory for Java virtual mechine (`picard`)\n",
    "parameter: java_mem = \"6G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "# VarVCFfile for data preparation for wasp_filtering\n",
    "parameter: varVCFfile = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')\n",
    "## Whether the fasta/fastq file is compressed or not.\n",
    "parameter: uncompressed = False\n",
    "import os\n",
    "import pandas as pd\n",
    "## FIX: The way to get sample needs to be revamped to 1. Accomodate rf/fr as column 2. accomodate single end read (Only 2 fq/samples)\n",
    "sample_inv = pd.read_csv(sample_list,sep = \"\\t\")\n",
    "\n",
    "# Align specific sample, to ensure correct grouping in subsequent steps, perform this step prior to extracting strand, read_length, and sample_id information\n",
    "if len(sample_name)>0:\n",
    "    print(\"Align sample\",sample_specific, 'only...')\n",
    "    sample_inv = sample_inv[sample_inv['ID'].isin(sample_name)] \n",
    "    \n",
    "## Extract strand information if user have specified the strand\n",
    "strand_inv = []\n",
    "\n",
    "if \"strand\" in sample_inv.columns:\n",
    "    strand_inv = sample_inv.strand.values.tolist()\n",
    "    sample_inv = sample_inv.drop(\"strand\" , axis = 1)\n",
    "    stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\",\"strand_missing\"] for x in strand_inv ]), msg = \"strand columns should only include ``fr``, ``rf``, ``strand_missing`` or ``unstranded``\")\n",
    "            \n",
    "## Extract read_length if user have specified read_length\n",
    "read_length = [0] * len(sample_inv.index)\n",
    "if \"read_length\" in sample_inv.columns:\n",
    "    read_length = sample_inv.read_length.values.tolist()\n",
    "    sample_inv = sample_inv.drop(\"read_length\" , axis = 1)\n",
    "    \n",
    "\n",
    "## Extract sample_id\n",
    "sample_inv_list = sample_inv.values.tolist()\n",
    "sample_id = [x[0] for x in sample_inv_list]\n",
    "\n",
    "    \n",
    "## Get the file name for single/paired end data\n",
    "file_inv = [x[1:] for x in sample_inv_list]\n",
    "file_inv = [item for sublist in file_inv for item in sublist]\n",
    "\n",
    "raw_reads = [f'{data_dir}/{x}' for x in file_inv]\n",
    "\n",
    "\n",
    "for y in raw_reads:\n",
    "        if not os.path.isfile(y):\n",
    "            raise ValueError(f\"File {y} does not exist\")\n",
    "\n",
    "if len(raw_reads) != len(set(raw_reads)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in sample file list\")\n",
    "\n",
    "# Is the RNA-seq data pair-end\n",
    "is_paired_end = 0 if len(raw_reads) == len(sample_id) else 1 \n",
    "from sos.utils import env\n",
    "env.logger.info(f'Input samples are {\"paired-end\" if is_paired_end else \"single-end\"} sequences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3d271-6505-47b1-929c-a141069f4ab1",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 0: Convert BAM back to fastq if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6105d",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[bam_to_fastq]\n",
    "input: raw_reads, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.1.fastq',f'{cwd}/{_input:bn}.2.fastq'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint=entrypoint\n",
    "    samtools sort -n ${_input} -o ${_output[0]:nn}.sorted.bam\n",
    "    bedtools bamtofastq -i ${_output[0]:nn}.sorted.bam -fq ${_output[0]} -fq2 ${_output[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-commission",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 1: QC before alignment\n",
    "\n",
    "This step utilize `fastqc` and will generate two QC report in `html` format. It should be noted that the [paired_end reads will also be qc separately](https://www.biostars.org/p/190584/)\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `fastq1` and `fastq2`: paths to original `fastq.gz` file.\n",
    "\n",
    "### Step Outputs\n",
    "* Two `html` file for QC report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "refined-bidder",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastqc]\n",
    "input: raw_reads, group_by =  1\n",
    "output: f'{cwd}/{_input:bn}_fastqc.html',f'{cwd}/{_input:bn}_fastqc.zip' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint=entrypoint\n",
    "    fastqc ${_input} -o ${_output[0]:d}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-chosen",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Step 2: Remove adaptor through `fastp`\n",
    "\n",
    "Documentation: [fastp](https://github.com/OpenGene/fastp)\n",
    "\n",
    "We use `fastp` in place of the `Trimmomatic` for fastp's ability to detect adapter from the reads. It was a c++ command line tool published in [Sept 2018](https://academic.oup.com/bioinformatics/article/34/17/i884/5093234). It  will use the following algorithm to detect the adaptors:\n",
    ">The adapter-sequence detection algorithm is based on two assumptions: the first is that only one adapter exists in the data; the second is that adapter sequences exist only in the read tails. These two assumptions are valid for major next-generation sequencers like Illumina HiSeq series, NextSeq series and NovaSeq series. We compute the k-mer (k = 10) of first N reads (N = 1 M). From this k-mer, the sequences with high occurrence frequencies (>0.0001) are considered as adapter seeds. Low-complexity sequences are removed because they are usually caused by sequencing artifacts. The adapter seeds are sorted by its occurrence frequencies. A tree-based algorithm is applied to extend the adapter seeds to find the real complete adapter\n",
    "\n",
    "It was demostrated that fastp can remove all the adaptor automatically and completely faster than Trimmomatic and cutadapt\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `fastq`: 1 set of fq.gz file for each sample. (2 files if paired end, 1 file if single end )\n",
    "\n",
    "### Step Outputs\n",
    "* 1 set of  `fastq.gz` file for alignment. (2 files if paired end, 1 file if single end )\n",
    "* The unpaired reads (i.e.where a read survived, but the partner read did not.) will be discarded by default as those were not used in the following steps. This feature can be added were it was needed. \n",
    "* 1 set of html documenting the quality of input reads(1 html file and 1 json file)\n",
    "\n",
    "### Step options\n",
    "\n",
    "A few options were selected to be customizable so that fastp step can have the same level of flexiblity as the Trimmomatic step had. They are:\n",
    "- min_len: length_required, reads shorter than this will be discarded, default is 15. (int [=15])\n",
    "- window_size: cut_window_size, the window size option shared by cut_front, cut_tail or cut_sliding. Range: 1~1000, default: 4 (int [=4])\n",
    "- leading/trailing : cut_front_mean_quality/cut_tail_mean_quality, the mean quality requirement option for cut_front/cut_tail, which move a sliding window from front/tail, drop the bases in the window if its mean quality < threshold. **Notice the choice of quality score in `fastp` (N=20) is a lot higher than that of `trimmomatic` (N=3)**. Default `fastp` setting is in line with that of [`cutadapt`](https://cutadapt.readthedocs.io/en/stable/guide.html).\n",
    "\n",
    "The default value are set to be the same as the default value of the `fastp` software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-handbook",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastp_trim_adaptor_1]\n",
    "# sliding window setting\n",
    "parameter: window_size = 4\n",
    "parameter: required_quality = 20\n",
    "# the mean quality requirement option for cut_front\n",
    "parameter: leading = 20\n",
    "# the mean quality requirement option for cut_tail\n",
    "parameter: trailing = 20\n",
    "# reads shorter than length_required will be discarded\n",
    "parameter: min_len = 15\n",
    "# Path to the reference adaptors\n",
    "parameter: fasta_with_adapters_etc = path(\".\")\n",
    "warn_if(fasta_with_adapters_etc.is_file(),msg = \"Use input fasta and adaptor detection of paired-end read was disabled\" )\n",
    "\n",
    "input: raw_reads, group_by = is_paired_end + 1 , group_with = \"sample_id\"\n",
    "output: [f'{cwd}/{path(x):bn}.trimmed.fq.gz' for x in _input]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash:  container=container,expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "        fastp -i ${f'{_input[0]} -I {_input[1]}' if is_paired_end else _input} -o ${ f'{_output[0]} -O {_output[1]}' if is_paired_end else _output } \\\n",
    "            ${f'--adapter_fasta {fasta_with_adapters_etc}' if fasta_with_adapters_etc.is_file() else \"--detect_adapter_for_pe\"}  -V -h ${_output[0]:n}.html -j ${_output[0]:n}.json -w ${numThreads} \\\n",
    "            --length_required ${min_len}  -W ${window_size} -M ${required_quality} -5 -3 --cut_front_mean_quality ${leading} --cut_tail_mean_quality ${leading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e84b1-027a-473d-807d-eb189f309690",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastp_trim_adaptor_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{sample_list:n}.trimmed.txt'\n",
    "sample_inv_tmp = pd.read_csv(sample_list,sep = \"\\t\")\n",
    "import csv\n",
    "if is_paired_end:\n",
    "    sample_inv_tmp.fq1 = [f'{x:r}'.replace(\"'\",\"\") for x in _input][::2]\n",
    "    sample_inv_tmp.fq2 = [f'{x:r}'.replace(\"'\",\"\") for x in _input][1::2]\n",
    "else:\n",
    "    sample_inv_tmp.fq1 = [f'{x:r}'.replace(\"'\",\"\") for x in _input]\n",
    "sample_inv_tmp.to_csv(_output,sep = \"\\t\",index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-shooting",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 2 Alternative: Remove adaptor through `Trimmomatic`\n",
    "\n",
    "Documentation: [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)\n",
    "\n",
    "We have replaced this with `fastp`, see above, which performs better than `Trimmomatic` in terms of removing adaptors that `Trimmomatic` cannot detect. `fastp` can also automatically guess the adapter sequences from data and by default no adapter sequence is required for input to `fastp`.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `software_dir`: directory for the software\n",
    "* `fasta_with_adapters_etc`: **filename** for the adapter reference file. According to `Trimmomatic` documention,\n",
    "\n",
    "> As a rule of thumb newer libraries will use `TruSeq3`, but this really depends on your service provider. If you use FASTQC, the \"Overrepresented Sequences\" report can help indicate which adapter file is best suited for your data. \"Illumina Single End\" or \"Illumina Paired End\" sequences indicate single-end or paired-end `TruSeq2` libraries, and the appropriate adapter files are `TruSeq2-SE.fa` and `TruSeq2-PE.fa` respectively. \"TruSeq Universal Adapter\" or \"TruSeq Adapter, Index …\" indicates `TruSeq-3` libraries, and the appropriate adapter files are `TruSeq3-SE.fa` or `TruSeq3-PE.fa`, for single-end and paired-end data respectively. Adapter sequences for `TruSeq2` multiplexed libraries, indicated by \"Illumina Multiplexing \n",
    "…\", and the various RNA library preparations are not currently included.\n",
    "\n",
    "We have `fastqc` workflow previously defined and executed. Users should decide what fasta adapter reference to use based on `fastqc` results (or their own knowledge).\n",
    "\n",
    "### Step Outputs\n",
    "* Two paired `fastq.gz` file for alignment\n",
    "* Two unpaired `fastq.gz` \n",
    "\n",
    ">For single-ended data, one input and one output file are specified, plus the processing steps. For paired-end data, two input files are specified, and 4 output files, 2 for the 'paired' output where both reads survived the processing, and 2 for corresponding 'unpaired' output where a read survived, but the partner read did not.\n",
    "\n",
    "\n",
    "**You need to figure out from fastqc results what adapter reference sequence to use.**, eg `--fasta_with_adapters_etc TruSeq3-PE.fa`. These files can be downloaded from `Trimmomatic` github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "turkish-alfred",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[trimmomatic_trim_adaptor]\n",
    "# Illumina clip setting\n",
    "# Path to the reference adaptors\n",
    "parameter: fasta_with_adapters_etc = path(\".\")\n",
    "parameter: seed_mismatches = 2\n",
    "parameter: palindrome_clip_threshold = 30\n",
    "parameter: simple_clip_threshold = 10\n",
    "# sliding window setting\n",
    "parameter: window_size = 4\n",
    "parameter: required_quality = 20\n",
    "# Other settings\n",
    "parameter: leading = 3\n",
    "parameter: trailing = 3\n",
    "parameter: min_len = 50\n",
    "input: raw_reads, group_by = is_paired_end + 1 , group_with = \"sample_id\"\n",
    "output: ([ f'{cwd}/{_sample_id}_paired_{_input[0]:bn}.gz', f'{cwd}/{_sample_id}_unpaired_{_input[0]:bn}.gz',  f'{cwd}/{_sample_id}_paired_{_input[1]:bn}.gz',f'{cwd}/{_sample_id}_unpaired_{_input[1]:bn}.gz' ] if is_paired_end else f'{cwd}/{_sample_id}_trimmed_{_input:bn}.gz')\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    trimmomatic -Xmx${java_mem} ${\"PE\" if is_paired_end else \"SE\"}  -threads ${numThreads} \\\n",
    "        ${_input:r}  ${_output:r} \\\n",
    "        ILLUMINACLIP:${fasta_with_adapters_etc}:${seed_mismatches}:${palindrome_clip_threshold}:${simple_clip_threshold} \\\n",
    "        LEADING:${leading} TRAILING:${trailing} SLIDINGWINDOW:${window_size}:${required_quality} MINLEN:${min_len}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-necessity",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 3: Alignment through `STAR`\n",
    "\n",
    "Documentation : [STAR](https://github.com/alexdobin/STAR) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_STAR.py)\n",
    "\n",
    "This step is the main step for `STAR` alignment. \n",
    "\n",
    "Originally, the STAR alignment is implemented using the run_STAR.py command from gtex. However, in order to [accomodate different read length among samples](https://github.com/cumc/xqtl-protocol/issues/318), We reimplement what was included in the wrapper, including the re-alignment of STAR unsorted bam using samtools to avoid high mem consumption.  \n",
    "\n",
    "\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* paths to trimmed `fastq.gz` file from Step 1 as documented in the new sample list.\n",
    "* `STAR_index`: directory for the STAR aligment index\n",
    "\n",
    "### Step Outputs\n",
    "\n",
    "* bam file output `${cwd}/{sample_id}.Aligned.sortedByCoord.bam`, will be used in step 3 and 4\n",
    "* bam file output `${cwd}/{sample_id}.Aligned.toTranscriptome.bam`, will be used in step 5\n",
    "\n",
    "**NB:** \"${}\" and \"$[]\" are exchangeable as expension, just to keep them consistent throught the bash block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "documented-malawi",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_align_1]\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "# STAR indexing file\n",
    "parameter: STAR_index = path\n",
    "parameter: outFilterMultimapNmax = 20 \n",
    "parameter: alignSJoverhangMin = 8 \n",
    "parameter: alignSJDBoverhangMin = 1 \n",
    "parameter: outFilterMismatchNmax = 999 \n",
    "parameter: outFilterMismatchNoverLmax = 0.1 #larger than Yang's group (outFilterMismatchNoverReadLmax 0.04)\n",
    "parameter: alignIntronMin = 20 \n",
    "parameter: alignIntronMax = 1000000 \n",
    "parameter: alignMatesGapMax = 1000000 \n",
    "parameter: outFilterType =  \"BySJout\" \n",
    "parameter: outFilterScoreMinOverLread = 0.33 \n",
    "parameter: outFilterMatchNminOverLread = 0.33 \n",
    "parameter: limitSjdbInsertNsj = 1200000 \n",
    "parameter: outSAMstrandField = \"intronMotif\" \n",
    "parameter: outFilterIntronMotifs = \"None\" \n",
    "parameter: alignSoftClipAtReferenceEnds = \"Yes\" \n",
    "parameter: quantMode = [\"TranscriptomeSAM\", \"GeneCounts\"]\n",
    "parameter: outSAMattrRGline = [\"ID:rg1\", \"SM:sm1\"]\n",
    "parameter: outSAMattributes = [\"NH\", \"HI\", \"AS\", \"nM\", \"NM\", \"ch\"] #(Yang's group: outSAMattributes NH HI AS nM XS vW). vW added is varVCFfile is set\n",
    "parameter: chimSegmentMin = 15 \n",
    "parameter: chimJunctionOverhangMin = 15 \n",
    "parameter: chimOutType = [\"Junctions\", \"WithinBAM\", \"SoftClip\"]\n",
    "parameter: chimMainSegmentMultNmax = 1 \n",
    "parameter: sjdbOverhang = 100\n",
    "parameter: mapping_quality = 255\n",
    "if int(mem.replace(\"G\",\"\")) <  40:\n",
    "    print(\"Insufficent memory for STAR, changing to 40G\")\n",
    "    star_mem = '40G'\n",
    "else:\n",
    "    star_mem = mem\n",
    "if varVCFfile:\n",
    "    if not path(varVCFfile).is_file():\n",
    "        raise FileNotFoundError(f\"Cannot find varVCFfile ``{varVCFfile}``\")\n",
    "    print(\"Adding wasp filter in STAR alignment\")\n",
    "# This option is commented out because it will force the downstream analysis to use 40G, which significantlly slow down the process.\n",
    "input: raw_reads, group_by = is_paired_end + 1, group_with = {\"sample_id\",\"read_length\"}\n",
    "output: cord_bam = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.out.bam',\n",
    "        trans_bam = f'{cwd}/{_sample_id}.Aligned.toTranscriptome.out.bam'\n",
    "if _read_length == 0:\n",
    "    print(\"Using specified --sjdbOverhang as read length\")\n",
    "else:\n",
    "    print(\"Using read length specified in the sample list\")\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = star_mem, cores = numThreads\n",
    "bash: container=container, expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    \n",
    "    # Remove previous output files\n",
    "    rm -rf $[cwd]/$[_sample_id].*.out.*.gz\n",
    "    rm -rf $[cwd]/$[_sample_id]._STARpass1\n",
    "\n",
    "    set -e\n",
    "\n",
    "    # Record start timestamp for alignment\n",
    "    align_start_time=$(date +%s)\n",
    "\n",
    "    # Run STAR alignment\n",
    "    star --runMode alignReads \\\n",
    "         --runThreadN $[numThreads] \\\n",
    "         --genomeDir $[STAR_index] \\\n",
    "         --readFilesIn $[_input:r] \\\n",
    "         --readFilesCommand $[\"cat\" if uncompressed else \"zcat\"] \\\n",
    "         --outFileNamePrefix $[_output[0]:nnnn]. \\\n",
    "         --outSAMstrandField $[outSAMstrandField] \\\n",
    "         --twopassMode Basic \\\n",
    "         --outFilterMultimapNmax $[outFilterMultimapNmax] \\\n",
    "         --alignSJoverhangMin $[alignSJoverhangMin] \\\n",
    "         --alignSJDBoverhangMin $[alignSJDBoverhangMin] \\\n",
    "         --outFilterMismatchNmax $[outFilterMismatchNmax] \\\n",
    "         --outFilterMismatchNoverLmax $[outFilterMismatchNoverLmax] \\\n",
    "         --alignIntronMin $[alignIntronMin] \\\n",
    "         --alignIntronMax $[alignIntronMax] \\\n",
    "         --alignMatesGapMax $[alignMatesGapMax] \\\n",
    "         --outFilterType $[outFilterType] \\\n",
    "         --outFilterScoreMinOverLread $[outFilterScoreMinOverLread] \\\n",
    "         --outFilterMatchNminOverLread $[outFilterMatchNminOverLread] \\\n",
    "         --limitSjdbInsertNsj $[limitSjdbInsertNsj] \\\n",
    "         --outFilterIntronMotifs $[outFilterIntronMotifs] \\\n",
    "         --alignSoftClipAtReferenceEnds $[alignSoftClipAtReferenceEnds] \\\n",
    "         --quantMode $[\" \".join(quantMode)] \\\n",
    "         --outSAMtype BAM Unsorted \\\n",
    "         --outSAMunmapped Within \\\n",
    "         --genomeLoad NoSharedMemory \\\n",
    "         --chimSegmentMin $[chimSegmentMin] \\\n",
    "         --chimJunctionOverhangMin $[chimJunctionOverhangMin] \\\n",
    "         --chimOutType $[\" \".join(chimOutType)] \\\n",
    "         --chimMainSegmentMultNmax $[chimMainSegmentMultNmax] \\\n",
    "         --chimOutJunctionFormat 0 \\\n",
    "         --outSAMattributes $[\" \".join(outSAMattributes)] $[\"vW\" if varVCFfile else \"\"] \\\n",
    "         --outSAMattrRGline $[\" \".join(outSAMattrRGline)] \\\n",
    "         --sjdbOverhang $[sjdbOverhang if _read_length == 0 else _read_length ] \\\n",
    "         --sjdbGTFfile $[gtf] $[(\"--varVCFfile %s --waspOutputMode SAMtag\" % varVCFfile) if varVCFfile else \"\"]\n",
    "\n",
    "    # Record end timestamp for alignment and calculate runtime\n",
    "    align_end_time=$(date +%s)\n",
    "    align_runtime=$((align_end_time - align_start_time))\n",
    "\n",
    "    # Remove temporary files\n",
    "    rm -r $[_output[0]:nnnn]._STARgenome\n",
    "    $[ f'rm -rf [_output[0]:nnnn]._STARtmp' if is_paired_end else \"\"]\n",
    "\n",
    "    # Record start timestamp for sorting\n",
    "    sort_start_time=$(date +%s)\n",
    "\n",
    "    # Sort the aligned BAM file\n",
    "    samtools sort --threads $[numThreads] -o $[_output[0]:nnnn].Aligned.sortedByCoord.out.bam $[_output[0]:nnnn].Aligned.out.bam\n",
    "    rm $[_output[0]:nnnn].Aligned.out.bam\n",
    "\n",
    "    # Record end timestamp for sorting and calculate runtime\n",
    "    sort_end_time=$(date +%s)\n",
    "    sort_runtime=$((sort_end_time - sort_start_time))\n",
    "\n",
    "    # Index the final BAM file\n",
    "    samtools index $[_output[0]]\n",
    "\n",
    "    # Print runtime information\n",
    "    echo \"STAR alignment started at: $(date -d @$align_start_time)\"\n",
    "    echo \"STAR alignment ended at: $(date -d @$align_end_time)\"\n",
    "    echo \"STAR alignment runtime: $align_runtime seconds\"\n",
    "    echo \"\"\n",
    "    echo \"Sorting started at: $(date -d @$sort_start_time)\"\n",
    "    echo \"Sorting ended at: $(date -d @$sort_end_time)\"\n",
    "    echo \"Sorting runtime: $sort_runtime seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e2a8f8-8c3f-4b60-aa31-b6e438530d38",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[strand_detected_1: shared=\"step_strand_detected\"]\n",
    "input: output_from(\"STAR_align_1\")[\"trans_bam\"], group_with = \"sample_id\"\n",
    "import pandas as pd\n",
    "ReadsPerGene = pd.read_csv(f'{cwd}/{_sample_id}.ReadsPerGene.out.tab' , sep = \"\\t\" , header = None )\n",
    "ReadsPerGene_list = ReadsPerGene.loc[3::,].sum(axis = 0, numeric_only = True).values.tolist()\n",
    "strand_percentage = [x/ReadsPerGene_list[0] for x in ReadsPerGene_list]\n",
    "print(f'for sample {_sample_id}')\n",
    "if strand_percentage[1] > 0.9:\n",
    "    print(f'Counts for the 1st read strand aligned with RNA is {strand_percentage[1]}, > 90% of aligned count')\n",
    "    print('Data is likely FR/fr-secondstrand')\n",
    "    strand_detected = \"fr\"\n",
    "elif  strand_percentage[2] > 0.9:\n",
    "    print(f'Counts for the 2nd read strand aligned with RNA is {strand_percentage[2]}, > 90% of aligned count')\n",
    "    print('Data is likely RF/fr-firststrand')\n",
    "    strand_detected = \"rf\"\n",
    "elif max( strand_percentage[1],  strand_percentage[2]) < 0.6:\n",
    "    print(f'Both {strand_percentage[1]} and  {strand_percentage[2]} are under 60% of reads explained by one direction')\n",
    "    print('Data is likely unstranded')\n",
    "    strand_detected = \"unstranded\"\n",
    "else:\n",
    "    strand_detected = 'strand_missing' \n",
    "    print(f'Data does not fall into a likely stranded (max percent explained {max( strand_percentage[1],  strand_percentage[2])} > 0.9) or unstranded layout (max percent explained {max( strand_percentage[1],  strand_percentage[2])} < 0.6), please check your data and manually specified the ``--strand`` option as ``fr``, ``rf`` or ``unstranded``')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a896c7-a922-4f90-9c37-bffc1fdeb1bc",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[strand_detected_2: shared=\"strand\"]\n",
    "input: group_by = \"all\"\n",
    "parameter: strand = \"\"\n",
    "import pandas as pd\n",
    "if not strand:\n",
    "    if len(strand_inv) > 0:\n",
    "        strand = strand_inv\n",
    "        for i in range(0,len(strand)):\n",
    "            if strand[i] == \"strand_missing\":\n",
    "                strand[i] = step_strand_detected[i]\n",
    "        print(f'Using strand specified in the input samples list {strand}, replacing strand_missing with detected strand')\n",
    "    else:\n",
    "        warn_if(not all(x is step_strand_detected[0] for x in step_strand_detected), msg = \"strands detected are different among samples, please check your protocol, we will use the detected strand for each samples\")    \n",
    "        strand = step_strand_detected\n",
    "        print(f'Using detected strand for each samples {strand}')\n",
    "else:\n",
    "    stop_if(strand not in [\"fr\", \"rf\", \"unstranded\"], msg = \"``--strand`` option should be ``fr``, ``rf`` or ``unstranded``\")\n",
    "    print(f'Using ``--strand`` overwrite option for all the samples {strand[0]}') \n",
    "    strand = [strand] * len(step_strand_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-baking",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 4: Mark duplicates reads & QC through `Picard`\n",
    "\n",
    "This step is the first QC step after `STAR` alignment. This step will performed QC to collect multipe metrics regarding the RNASeq using Picard. Then it will also generate a new `.bam` file with duplication marked with the hexadecimal value of `0x0400`, which corresponds to a decimal value of 1024\n",
    " \n",
    "### Step Inputs:\n",
    "\n",
    "* `STAR_bam`: path to the output in Step 2.\n",
    "* `reference_fasta`:  The fasta reference file used to generate star index, it is critical that the this fasta is *exactly* the same as those that generate the STAR Index, else this error will occurs: https://github.com/cumc/xqtl-protocol/issues/357\n",
    "\n",
    "* `RefFlat file` \n",
    "\n",
    "This file is needed for picard CollectRnaSeqMetrics module, which in turn\n",
    "> produces metrics describing the distribution of the bases within the transcripts. It calculates the total numbers and the fractions of nucleotides within specific genomic regions including untranslated regions (UTRs), introns, intergenic sequences (between discrete genes), and peptide-coding sequences (exons). This tool also determines the numbers of bases that pass quality filters that are specific to Illumina data (PF_BASES).\n",
    "\n",
    "The refFlat file can be generated by the reference_data module, RefFlat_generation step.\n",
    "\n",
    "### Step Outputs:\n",
    "\n",
    "* A collection of metrics file for each of the samples\n",
    "* A new `.bam` file with duplication marked with the hexadecimal value of `0x0400`, which corresponds to a decimal value of 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19088a92-c333-400c-9f5d-39715a33e75f",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[picard_qc, STAR_align_2]\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "depends: sos_variable('strand')\n",
    "# Path to flat reference file, for computing QC metric\n",
    "parameter: ref_flat = path\n",
    "# The fasta reference file used to generate star index\n",
    "parameter: reference_fasta = path\n",
    "# For the patterned flowcell models (HiSeq X), change to 2500\n",
    "parameter: optical_distance = 100\n",
    "parameter: zap_raw_bam = False\n",
    "picard_strand_dict = {\"rf\":\"SECOND_READ_TRANSCRIPTION_STRAND\",\"fr\": \"FIRST_READ_TRANSCRIPTION_STRAND\",\"unstranded\":\"NONE\" }\n",
    "input: output_from('filter_reads'), group_by = 2, group_with = {\"sample_id\",\"strand\"}\n",
    "output: picard_metrics = f'{cwd}/{_sample_id}.alignment_summary_metrics',\n",
    "        picard_rna_metrics = f'{cwd}/{_sample_id}.rna_metrics',\n",
    "        md_bam = f'{_input[0]:n}.md.bam',\n",
    "        md_metrics = f'{_input[0]:n}.md.metrics',\n",
    "        bigwig = f'{_input[0]:n}.md.bw',\n",
    "        output_summary = f'{cwd}/{_sample_id}.bam_file_meta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    # Record start timestamp for CollectMultipleMetrics\n",
    "    start_time=$(date +%s)\n",
    "    echo \"CollectMultipleMetrics started at: $(date)\"\n",
    "    set -e\n",
    "    picard -Xmx${java_mem} CollectMultipleMetrics \\\n",
    "        -REFERENCE_SEQUENCE ${reference_fasta} \\\n",
    "        -PROGRAM CollectAlignmentSummaryMetrics \\\n",
    "        -PROGRAM CollectInsertSizeMetrics \\\n",
    "        -PROGRAM QualityScoreDistribution \\\n",
    "        -PROGRAM MeanQualityByCycle \\\n",
    "        -PROGRAM CollectBaseDistributionByCycle \\\n",
    "        -PROGRAM CollectGcBiasMetrics \\\n",
    "        -VALIDATION_STRINGENCY STRICT \\\n",
    "        -INPUT  ${_input[\"cord_bam_wasp_qc\"]} \\\n",
    "        -OUTPUT  ${_output[0]:n}\n",
    "    \n",
    "    # Record end timestamp and calculate runtime for CollectMultipleMetrics\n",
    "    end_time=$(date +%s)\n",
    "    runtime=$((end_time - start_time))\n",
    "    echo \"CollectMultipleMetrics ended at: $(date)\"\n",
    "    echo \"CollectMultipleMetrics runtime: $runtime seconds\"\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[-1]:n}.stderr', stdout = f'{_output[-1]:n}.stdout', entrypoint=entrypoint\n",
    "    start_time=$(date +%s)\n",
    "    echo \"MarkDuplicates started at: $(date)\"\n",
    "    set -e\n",
    "    picard -Xmx${java_mem}  MarkDuplicates \\\n",
    "        -I ${_input[\"cord_bam_wasp_qc\"]}  \\\n",
    "        -O ${_output[2]} \\\n",
    "        -PROGRAM_RECORD_ID null \\\n",
    "        -M ${_output[3]} \\\n",
    "        -TMP_DIR ${cwd}\\\n",
    "        -MAX_RECORDS_IN_RAM 500000 -SORTING_COLLECTION_SIZE_RATIO 0.25 \\\n",
    "        -ASSUME_SORT_ORDER coordinate \\\n",
    "        -TAGGING_POLICY DontTag \\\n",
    "        -OPTICAL_DUPLICATE_PIXEL_DISTANCE ${optical_distance} \\\n",
    "        -CREATE_INDEX true \\\n",
    "        -CREATE_MD5_FILE true \\\n",
    "        -VALIDATION_STRINGENCY STRICT \\\n",
    "        -REMOVE_SEQUENCING_DUPLICATES false \\\n",
    "        -REMOVE_DUPLICATES false\n",
    "    samtools index ${_output[2]}\n",
    "    # Record end timestamp and calculate runtime for MarkDuplicates\n",
    "    end_time=$(date +%s)\n",
    "    runtime=$((end_time - start_time))\n",
    "    echo \"MarkDuplicates ended at: $(date)\"\n",
    "    echo \"MarkDuplicates runtime: $runtime seconds\"\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stdout', entrypoint=entrypoint\n",
    "    set -e\n",
    "    # Get only lines with rRNA and transcript_id from the GTF file\n",
    "    cat ${gtf} | grep rRNA | grep transcript_id > ${cwd}/$(basename ${gtf}).tmp.${_input[\"cord_bam_wasp_qc\"]:bnnnn}  # To avoid data racing problem, modification was made here to change the output location to the output path\n",
    "    \n",
    "    # Extract header from the BAM file\n",
    "    samtools view -H ${_input[\"cord_bam_wasp_qc\"]}  > ${_input[\"cord_bam_wasp_qc\"]}.RI\n",
    "\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stdout', entrypoint=entrypoint\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        from collections import defaultdict\n",
    "        chrom = []\n",
    "        start = []\n",
    "        end = []\n",
    "        strand = []\n",
    "        tag = []\n",
    "        filename = os.path.basename(\"${gtf}.tmp.${_input[\"cord_bam_wasp_qc\"]:bnnnn}\")\n",
    "        annotation_gtf = os.path.join(\"${cwd}\",filename) # also change the path to read the file only with rRNA and transcript_id from the GTF file\n",
    "\n",
    "        with open(annotation_gtf, 'r') as gtf:\n",
    "            for row in gtf:\n",
    "                row = row.strip().split('\\t')\n",
    "                if row[0][0]=='#' or row[2]!=\"transcript\": continue # skip header\n",
    "                chrom.append(row[0])\n",
    "                start.append(row[3])\n",
    "                end.append(row[4])\n",
    "                strand.append(row[6])\n",
    "                attributes = defaultdict()\n",
    "                for a in row[8].replace('\"', '').split(';')[:-1]:\n",
    "                    kv = a.strip().split(' ')\n",
    "                    if kv[0]!='tag':\n",
    "                        attributes[kv[0]] = kv[1]\n",
    "                    else:\n",
    "                        attributes.setdefault('tags', []).append(kv[1])\n",
    "                tag.append(attributes)\n",
    "        transcript_id = [x[\"transcript_id\"] for x in tag]\n",
    "        RI = pd.DataFrame(data={'chr':chrom, 'start':start, 'end':end, 'strand':strand,'transcript_id' : transcript_id })\n",
    "        RI.to_csv(\"${_input[\"cord_bam_wasp_qc\"]}.RI\", index = 0, header = 0, mode = \"a\",sep = \"\\t\" )\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stdout', entrypoint=entrypoint\n",
    "    # Record start timestamp for CollectRnaSeqMetrics\n",
    "    start_time=$(date +%s)\n",
    "    echo \"CollectRnaSeqMetrics started at: $(date)\"\n",
    "    set -e\n",
    "    picard -Xmx${java_mem} CollectRnaSeqMetrics \\\n",
    "        -REF_FLAT ${ref_flat} \\\n",
    "        -RIBOSOMAL_INTERVALS ${_input[\"cord_bam_wasp_qc\"]}.RI \\\n",
    "        -STRAND_SPECIFICITY ${picard_strand_dict[_strand]} \\\n",
    "        -CHART_OUTPUT ${_output[0]:n}.rna_metrics.pdf \\\n",
    "        -VALIDATION_STRINGENCY STRICT \\\n",
    "        -INPUT ${_input[\"cord_bam_wasp_qc\"]}  \\\n",
    "        -OUTPUT  ${_output[0]:n}.rna_metrics\n",
    "    \n",
    "    rm ${cwd}/$(basename ${gtf}).tmp.${_input[\"cord_bam_wasp_qc\"]:bnnnn} # remove the path of the corresponding file with only rRNA and transcript_id from the GTF file\n",
    "    \n",
    "    # Record end timestamp and calculate runtime for CollectRnaSeqMetrics\n",
    "    end_time=$(date +%s)\n",
    "    runtime=$((end_time - start_time))\n",
    "    echo \"CollectRnaSeqMetrics ended at: $(date)\"\n",
    "    echo \"CollectRnaSeqMetrics runtime: $runtime seconds\"\n",
    "\n",
    "bash: container=container, expand= \"$[ ]\", stderr = f'{_output[0]:n}.bw.stderr', stdout = f'{_output[0]:n}.bw.stdout', entrypoint=entrypoint\n",
    "    generate_bigwig() {\n",
    "        # Check if enough arguments are provided\n",
    "        if [[ $# -ne 1 ]]; then\n",
    "            echo \"Usage: generate_bigwig <file.bam>\"\n",
    "            return 1\n",
    "        fi\n",
    "\n",
    "        local input_file=\"$1\"\n",
    "        \n",
    "        # Derive file.bw from file.bam\n",
    "        local bigwig_file=\"${input_file%.*}.bw\"\n",
    "\n",
    "        # Execute the commands\n",
    "        samtools index \"$input_file\"\n",
    "        bamcoverage -b \"$input_file\" -o \"$bigwig_file\"\n",
    "    }\n",
    "\n",
    "    export -f generate_bigwig  # To make it available in subshells, if needed.\n",
    "    generate_bigwig $[_output[\"md_bam\"]]\n",
    "\n",
    "import pandas as pd\n",
    "out = pd.DataFrame({\n",
    "    \"sample_id\": _sample_id,\n",
    "    \"strand\": _strand,\n",
    "    \"coord_bam_list\": f'{_output[\"md_bam\"]:b}',\n",
    "    \"BW_list\": f'{_output[\"bigwig\"]:b}',\n",
    "    \"SJ_list\": f'{_output[\"md_bam\"]:bnnnnn}.SJ.out.tab',\n",
    "    \"trans_bam_list\": f'{_output[\"md_bam\"]:bnnnn}.toTranscriptome.out{\"_wasp_qc\" if varVCFfile else \"\"}.bam'},\n",
    "    index=[0])\n",
    "out.to_csv(_output[\"output_summary\"], sep=\"\\t\", index=False)\n",
    "\n",
    "if zap_raw_bam:\n",
    "    _input[\"cord_bam_wasp_qc\"].zap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26edeb27-56e6-4e80-b116-cc913d6fe0e3",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_align_3]\n",
    "input: group_by = \"all\"\n",
    "depends: sos_variable(\"strand\")\n",
    "output: f'{cwd}/{sample_list:bn}.bam_file_list'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = walltime, mem = mem, cores = 1\n",
    "python: container=container, expand= \"${ }\", entrypoint=entrypoint\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    coord_bam_list = [${_input:br,}][2::6]\n",
    "    sorted_bigwig_list = [${_input:br,}][4::6]\n",
    "    SJ_list = [f'{x}.SJ.out.tab' for x in [${_input:bnnnnnr,}][2::6]]\n",
    "    trans_bam_list = [f'{x}.toTranscriptome.out{\"_wasp_qc\" if \"${varVCFfile}\" else \"\"}.bam' for x in [${_input:bnnnnr,}][2::6]]\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"sample_id\": ${sample_id},\n",
    "        \"strand\": ${strand},\n",
    "        \"coord_bam_list\": coord_bam_list,\n",
    "        \"BW_list\": sorted_bigwig_list,\n",
    "        \"SJ_list\": SJ_list,\n",
    "        \"trans_bam_list\": trans_bam_list\n",
    "    })\n",
    "\n",
    "    out.to_csv(\"${_output}\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266d430-2c34-4b13-a816-7729703d3e55",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 5: Post aligment QC through `RNA-SeQC`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f0051-fabd-4b32-8a50-0461a3c3be09",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Documentation : [RNA-SeQC](https://github.com/getzlab/rnaseqc) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_rnaseqc.py)\n",
    "\n",
    "This step is second QC step after `STAR` alignment. It will perform RNA-seq quantification as well. \n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `bam_list`: path to the output of STAR_output, which outlined the strands and bam file used for each samples.\n",
    "* `gtf`: reference genome `.gtf` file, this gtf file need to have the same chr name format as the index used to generate the bam file and must be on collapsed gene gtf \n",
    "\n",
    "### Step Outputs\n",
    "\n",
    "\n",
    "The following output files are generated in the output directory you provide:\n",
    "\n",
    "* {sample}.metrics.tsv : A tab-delimited list of (Statistic, Value) pairs of all statistics and metrics recorded.\n",
    "\n",
    "* {sample}.exon_reads.gct : A tab-delimited GCT file with (Exon ID, Gene Name, coverage) tuples for all exons which had at least part of one read mapped.\n",
    "\n",
    "* {sample}.gene_reads.gct : A tab-delimited GCT file with (Gene ID, Gene Name, coverage) tuples for all genes which had at least one read map to at least one of its exons\n",
    "\n",
    "* {sample}.gene_tpm.gct : A tab-delimited GCT file with (Gene ID, Gene Name, TPM) tuples for all genes reported in the gene_reads.gct file. Note: this file is renamed to .gene_rpkm.gct if the --rpkm flag is present.\n",
    "\n",
    "* {sample}.fragmentSizes.txt : A list of fragment sizes recorded, if a BED file was provided\n",
    "\n",
    "* {sample}.coverage.tsv : A tab-delimited list of (Gene ID, Transcript ID, Mean Coverage, Coverage Std, Coverage CV) tuples for all transcripts encountered in the GTF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1538e84-7294-4ca1-a6c2-3f7f60f47e1a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_1]\n",
    "import os\n",
    "import pandas as pd\n",
    "parameter: bam_list = path\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "sample_inv = pd.read_csv(bam_list,sep = \"\\t\")\n",
    "parameter: detection_threshold = 5\n",
    "parameter: mapping_quality = 255\n",
    "## Extract strand information if user have specified the strand\n",
    "strand_list = sample_inv.strand.values.tolist()\n",
    "stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\"] for x in strand_list ]), msg = \"strand columns should only include ``fr``, ``rf`` or ``unstranded``, please check the bam_list\")     \n",
    "## Extract sample_id\n",
    "sample_id = sample_inv.sample_id.values.tolist()\n",
    "    \n",
    "## Get the file name for cood_bam data\n",
    "coord_bam_list_inv = sample_inv.coord_bam_list.values.tolist()\n",
    "coord_bam_list = [f'{cwd}/{x}' for x in coord_bam_list_inv ]\n",
    "input:  coord_bam_list, group_by = 1, group_with = {\"sample_id\",\"strand_list\"}\n",
    "output: f'{cwd}/{_sample_id}.rnaseqc.gene_tpm.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.gene_reads.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.exon_reads.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.metrics.tsv'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    cd ${cwd} && \\\n",
    "    rnaseqc \\\n",
    "        ${gtf:a} \\\n",
    "        ${_input:a} \\\n",
    "        ${_output[0]:d}  \\\n",
    "        ${(\"--stranded \" + _strand_list) if _strand_list != \"unstranded\" else \"\"} \\\n",
    "        ${f'--detection-threshold {detection_threshold}'} ${f'--mapping-quality {mapping_quality}'} ${ '' if is_paired_end else '-u'}\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    mv ${_output[0]:d}/${_input:b}.gene_tpm.gct ${_output[0]:n}\n",
    "    mv ${_output[0]:d}/${_input:b}.gene_reads.gct ${_output[1]:n}\n",
    "    mv ${_output[0]:d}/${_input:b}.exon_reads.gct ${_output[2]:n}\n",
    "    mv ${_output[0]:d}/${_input:b}.metrics.tsv ${_output[3]}\n",
    "    gzip ${_output[0]:n} ${_output[1]:n} ${_output[2]:n}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f79eca-00a8-4782-a936-20890939c7b1",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The RNASEQC results were merged in the following step,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596adc2a-2f86-4ccf-a298-9d2199e8f4ca",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_2]\n",
    "parameter: bam_list = path\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{bam_list:bn}.rnaseqc.gene_tpm.gct.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rnaseqc.gene_readsCount.gct.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rnaseqc.exon_readsCount.gct.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rnaseqc.metrics.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    def make_gct(gct_path):\n",
    "        # sample_name\n",
    "        sample_name = \".\".join(os.path.basename(gct_path).split(\".\")[:-4])\n",
    "        # read_input\n",
    "        pre_gct = pd.read_csv(gct_path,sep = \"\\t\",\n",
    "                              skiprows= 2,index_col=\"Name\").drop(\"Description\",axis = 1)\n",
    "        pre_gct.index.name = \"gene_ID\"\n",
    "        pre_gct.columns = [sample_name]\n",
    "        return(pre_gct)\n",
    "\n",
    "    def merge_gct(gct_path_list):\n",
    "        gct = pd.DataFrame()\n",
    "        for gct_path in gct_path_list:\n",
    "            #check duplicated indels and remove them.\n",
    "            gct_col = make_gct(gct_path)\n",
    "            gct = gct.merge(gct_col,right_index=True,left_index = True,how = \"outer\")\n",
    "        return gct\n",
    "\n",
    "    input_list = [${_input:r,}]\n",
    "    tpm_list = input_list[0::4]\n",
    "    gc_list = input_list[1::4]\n",
    "    ec_list = input_list[2::4]\n",
    "    gct_path_list_list = [tpm_list,gc_list,ec_list]\n",
    "    output_path = [${_output:r,}][0:3]\n",
    "    for i in range(0,len(output_path)):\n",
    "        output = merge_gct(gct_path_list_list[i])\n",
    "        output.to_csv(output_path[i], sep = \"\\t\")\n",
    "    metrics_list = input_list[3::4]\n",
    "    with open(\"${cwd}/${bam_list:bn}.rnaseqc.metrics_output_list\", \"w\") as f:\n",
    "        f.write('\\n'.join(metrics_list))\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    aggregate_rnaseqc_metrics  ${_output[3]:n}_output_list ${_output[3]:nn}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7df23-0c6c-4fe3-9aa0-4a597bd99ef8",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 6: Quantify expression through `RSEM`\n",
    "\n",
    "Documentation : [RSEM](https://deweylab.github.io/RSEM/rsem-calculate-expression.html) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_RSEM.py)\n",
    "\n",
    "This step generate the expression matrix from STAR output. Estimate gene and isoform expression from RNA-Seq data are generated.\n",
    "\n",
    "### Step Input\n",
    "\n",
    "* `bam_list`: path to the output of STAR_output, which outlined the strands and bam file used for each samples.\n",
    "* transcript-level BAM file: path to the output of Step 3.\n",
    "* `RSEM_index`: path to RSEM index\n",
    "\n",
    "### Step Outputs\n",
    "Please see the output section of https://deweylab.github.io/RSEM/rsem-calculate-expression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b8bbdc-49f3-432d-98bb-809b05ee8dcf",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_1]\n",
    "parameter: bam_list = path\n",
    "parameter: RSEM_index = path\n",
    "parameter: max_frag_len = 1000\n",
    "parameter: estimate_rspd = True\n",
    "\n",
    "sample_inv = pd.read_csv(bam_list,sep = \"\\t\")\n",
    "\n",
    "## Extract strand information if user have specified the strand\n",
    "strand_list = sample_inv.strand.values.tolist()\n",
    "stop_if(not all([x in [\"fr\", \"rf\", \"unstranded\"] for x in strand_list ]), msg = \"strand columns should only include ``fr``, ``rf`` or ``unstranded``, please check the bam_list\")     \n",
    "## Extract sample_id\n",
    "sample_id = sample_inv.sample_id.values.tolist()\n",
    "    \n",
    "## Get the file name for trans_bam_list data\n",
    "trans_bam_list_inv = sample_inv.trans_bam_list.values.tolist()\n",
    "trans_bam_list = [f'{cwd}/{x}' for x in trans_bam_list_inv]\n",
    "input: trans_bam_list, group_by = 1, group_with = {\"sample_id\", \"strand_list\"} \n",
    "output: f'{cwd}/{_sample_id}.rsem.isoforms.results', f'{cwd}/{_sample_id}.rsem.genes.results',f'{cwd}/{_sample_id}.rsem.stat/{_sample_id}.rsem.cnt'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    run_rsem ${RSEM_index:a} ${_input:a} ${_sample_id} \\\n",
    "        -o ${_output[0]:d} \\\n",
    "        --max_frag_len ${max_frag_len} \\\n",
    "        --estimate_rspd ${'true' if estimate_rspd else 'false'} \\\n",
    "        --paired_end ${\"true\" if is_paired_end else \"false\"} \\\n",
    "        --is_stranded ${\"true\" if _strand_list != \"unstranded\" else \"false\"} \\\n",
    "        --threads ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b4dba-f341-4d07-9936-a243af953fb0",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The RSEM results were merged in the following steps, seven files (four for each columns in the isoform output and 3 for each of the genes output) will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dc5d6cc-8feb-452c-95cb-91fcf9a619e7",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_2]\n",
    "parameter: bam_list = path\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{bam_list:bn}.rsem_transcripts_expected_count.txt.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rsem_transcripts_tpm.txt.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rsem_transcripts_fpkm.txt.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rsem_transcripts_isopct.txt.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rsem_genes_expected_count.txt.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rsem_genes_tpm.txt.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rsem_genes_fpkm.txt.gz',\n",
    "        f'{cwd}/{bam_list:bn}.rsem.aggregated_quality.metrics.tsv'\n",
    "\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    input_list = [${_input:r,}]\n",
    "    with open('${cwd}/${bam_list:bn}.rsem.isoforms_output_list', \"w\") as f:\n",
    "        f.write('\\n'.join(input_list[0::3]))\n",
    "    with open('${cwd}/${bam_list:bn}.rsem.genes_output_list', \"w\") as f:\n",
    "        f.write('\\n'.join(input_list[1::3]))\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "         aggregate_rsem_results ${cwd}/${bam_list:bn}.rsem.isoforms_output_list {expected_count,TPM,FPKM,IsoPct} ${_output[0]:nnn} -o ${cwd}\n",
    "         aggregate_rsem_results ${cwd}/${bam_list:bn}.rsem.genes_output_list {expected_count,TPM,FPKM} ${_output[1]:nnn} -o ${cwd}\n",
    "# -o to specify the output-dir(writable);the defualt is cwd of the sos command-/home/username/data(read-only)\n",
    "\n",
    "R:  container=container, expand= \"${ }\", stderr = f'{_output[-1]:n}.stderr', stdout = f'{_output[-1]:n}.stdout', entrypoint=entrypoint\n",
    "     readRSEM.cnt <- function (source) {\n",
    "            # RSEM .cnt files gives statistics about the (transcriptome) alignment passed to RSEM:\n",
    "            # Row 1: N0 (# unalignable reads);\n",
    "            #        N1 (# alignable reads);\n",
    "            #        N2 (# filtered reads due to too many alignments);\n",
    "            #        N_tot (N0+N1+N2)\n",
    "            # Row 2: nUnique (# reads aligned uniquely to a gene);\n",
    "            #        nMulti (# reads aligned to multiple genes);\n",
    "            #        nUncertain (# reads aligned to multiple locations in the given reference sequences, which include isoform-level multi-mapping reads)\n",
    "            # Row 3: nHits (# total alignments);\n",
    "            #        read_type (0: single-end read, no quality; 1: single-end read, with quality score; 2: paired-end read, no quality score; 3: paired-end read, with quality score)\n",
    "            # Source: https://groups.google.com/forum/#!topic/rsem-users/usmPKgsC5LU\n",
    "            # Note: N1 = nUnique + nMulti\n",
    "\n",
    "            stopifnot(file.exists(source[1]))\n",
    "            isDir <- file.info(source)$isdir\n",
    "            if (isDir) {\n",
    "                files <- system(paste(\"find\", source, \"-name \\\"*.rsem.cnt\\\"\"), intern=TRUE)\n",
    "                stopifnot(length(files) > 0)\n",
    "                samples <- gsub(\"_rsem.cnt\", \"\", basename(files), fixed=TRUE)\n",
    "            } else {\n",
    "                files <- source\n",
    "                samples <- gsub(\".rsem.cnt\", \"\", basename(files), fixed=TRUE)\n",
    "            }\n",
    "            metrics <- list()\n",
    "            for (i in 1:length(files)) {\n",
    "                m <- read.table(files[i], header=FALSE, sep=\" \", comment.char=\"#\", stringsAsFactors=FALSE, nrows=3, fill=TRUE)\n",
    "                metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                File=files[i],\n",
    "                TotalReads=m[1, 4],\n",
    "                AlignedReads=m[1, 2],\n",
    "                UniquelyAlignedReads=m[2, 1],\n",
    "                stringsAsFactors=FALSE)\n",
    "            }\n",
    "            metrics <- do.call(rbind, metrics)\n",
    "            row.names(metrics) <- metrics$Sample\n",
    "\n",
    "            return(metrics)\n",
    "        }\n",
    "        sourceRSEM = c(${_input:r,})\n",
    "        sourceRSEM = sourceRSEM[seq(3,length(sourceRSEM),3)]\n",
    "        metrics.RSEM = readRSEM.cnt(sourceRSEM)\n",
    "        write.table(metrics.RSEM, file=\"${_output[-1]}\",col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fab047-76ce-4fb9-9dab-ab121dbf9550",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 7: summarize with MultiQC\n",
    "\n",
    "[MultiQC](https://multiqc.info/docs/#using-multiqc) \n",
    "\n",
    "\n",
    ">MultiQC is a reporting tool that parses summary statistics from results and log files generated by other bioinformatics tools. MultiQC doesn't run other tools for you - it's designed to be placed at the end of analysis pipelines or to be run manually when you've finished running your tools. When you launch MultiQC, it recursively searches through any provided file paths and finds files that it recognises. It parses relevant information from these and generates a single stand-alone HTML report file. It also saves a directory of data files with all parsed data for further downstream use.\n",
    "\n",
    "MultiQC will automatically generate QC report for anything embedded within the given directory. Therefore providing the directory containing all the output will surfice.\n",
    "\n",
    "The output of MultiQC is a multi-module report each corresponding to the quality report of each step of analysis previously performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4d7905-b84b-40f3-8fad-e61e339452ee",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_3,rnaseqc_call_3]\n",
    "output: f'{_input[0]:nnn}.multiqc_report.html'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "report: output = f\"{_output:n}.multiqc_config.yml\"\n",
    "  extra_fn_clean_exts:\n",
    "      - '_rsem'\n",
    "  fn_ignore_dirs:\n",
    "      - '*_STARpass1'\n",
    "bash:  container=container,expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    multiqc ${_input:d} -v -n ${_output:b} -o ${_output:d} -c ${_output:n}.multiqc_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac480f9-ffc1-4a63-9a14-4ea6147121b7",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_4, rnaseqc_call_4]\n",
    "# Path to flat reference file, for computing QC metric\n",
    "output: f'{_input[0]:nn}.picard.aggregated_quality.metrics.tsv'# it will be outputed in a very 'deep' directory with {cwd}/\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, trunk_size = job_size\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    ## Define Function\n",
    "    readPicard.alignment_summary_metrics <- function (source) {\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.alignment_summary_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".alignment_summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".alignment_summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=${is_paired_end}+1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PF_READS=sum(m$PF_READS[1:2]),\n",
    "                                   PF_READS_ALIGNED=sum(m$PF_READS_ALIGNED[1:2]),\n",
    "                                   PCT_PF_READS_ALIGNED=sum(m$PF_READS_ALIGNED[1:2])/sum(m$PF_READS[1:2]),\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    readPicard.rna_metrics <- function(source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.rna_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".rna_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".rna_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PCT_RIBOSOMAL_BASES=m$PCT_RIBOSOMAL_BASES,\n",
    "                                   PCT_CODING_BASES=m$PCT_CODING_BASES,\n",
    "                                   PCT_UTR_BASES=m$PCT_UTR_BASES,\n",
    "                                   PCT_INTRONIC_BASES=m$PCT_INTRONIC_BASES,\n",
    "                                   PCT_INTERGENIC_BASES=m$PCT_INTERGENIC_BASES,\n",
    "                                   PCT_MRNA_BASES=m$PCT_MRNA_BASES,\n",
    "                                   PCT_USABLE_BASES=m$PCT_USABLE_BASES,\n",
    "                                   MEDIAN_CV_COVERAGE=m$MEDIAN_CV_COVERAGE,\n",
    "                                   MEDIAN_5PRIME_BIAS=m$MEDIAN_5PRIME_BIAS,\n",
    "                                   MEDIAN_3PRIME_BIAS=m$MEDIAN_3PRIME_BIAS,\n",
    "                                   MEDIAN_5PRIME_TO_3PRIME_BIAS=m$MEDIAN_5PRIME_TO_3PRIME_BIAS,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "    readPicard.duplicate_metrics <- function(source) {\n",
    "\n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      pattern_suffix <- ifelse(\"${varVCFfile}\"!=\"\",\"_wasp_qc\",\"\")\n",
    "      pattern <- paste0('*.Aligned.sortedByCoord.out',pattern_suffix,'.md.metrics')\n",
    "      substitute <- paste0('.Aligned.sortedByCoord.out',pattern_suffix,'.md.metrics')\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name\",pattern), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(substitute, \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(substitute, \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "\n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PERCENT_DUPLICATION=m$PERCENT_DUPLICATION,\n",
    "                                   ESTIMATED_LIBRARY_SIZE=m$ESTIMATED_LIBRARY_SIZE,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "\n",
    "    readPicard <- function(source) {\n",
    "      metrics.aln <- readPicard.alignment_summary_metrics(source)\n",
    "      metrics.rna <- readPicard.rna_metrics(source)\n",
    "      metrics.dup <- readPicard.duplicate_metrics(source)\n",
    "\n",
    "      stopifnot(all(row.names(metrics.aln) %in% row.names(metrics.rna)) &\n",
    "                all(row.names(metrics.rna) %in% row.names(metrics.dup)) &\n",
    "                all(row.names(metrics.dup) %in% row.names(metrics.aln)))\n",
    "\n",
    "      metrics.aln$File <- NULL\n",
    "      metrics.rna$File <- NULL\n",
    "      metrics.dup$File <- NULL\n",
    "      metrics.rna$Sample <- NULL\n",
    "      metrics.dup$Sample <- NULL\n",
    "\n",
    "      metrics <- cbind(metrics.aln, metrics.rna[row.names(metrics.aln), ])\n",
    "      metrics <- cbind(metrics, metrics.dup[row.names(metrics.aln), ])\n",
    "\n",
    "      return(metrics)\n",
    "    }\n",
    "\n",
    "    ## Execution  \n",
    "    sourcePicard = ${_input[-1]:dr}\n",
    "\n",
    "    Picard_qualityMetrics <- readPicard(sourcePicard)\n",
    "    write.table(Picard_qualityMetrics, file=\"${_output}\",col.names=TRUE, row.names=FALSE, quote=FALSE)\n",
    "\n",
    "\n",
    "# Need to considering .checkpoint directory(hidden) when there's interruption caused by previous error in [STAR_lign] on interactive sessions. It might cause inconsistent length of metrics of metrics.aln."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5c2a9-8c65-464d-8d4a-ecf5136905d0",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 8: Filter Reads with WASP\n",
    "\n",
    "This optional step in the STAR alignment process offers filtration based on WASP correction and unique read identification. WASP correction mitigates reference allele bias in RNA-seq data by eliminating reads with specific WASP flags. Unique read filtering retains only uniquely mapped reads, identified by a mapping quality (MAPQ) value of 255, as per GTEx Consortium standards. These filters can be applied individually or in combination to enhance alignment quality and prepare BAM files for different analysis.\n",
    " \n",
    "### Step Inputs:\n",
    "\n",
    "* No input needed to specify manually. All files are from the output from `STAR_align_1`\n",
    "\n",
    "### Step Outputs:\n",
    "\n",
    "* A collection of filtered(or not) `.bam` files\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2cf54-a31b-47d8-b63e-6446c9da9d27",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[filter_reads]\n",
    "parameter: unique=\"\"\n",
    "parameter: wasp=\"\"\n",
    "parameter: mapping_quality = 255\n",
    "depends: sos_variable('strand')\n",
    "input: output_from(\"STAR_align_1\"), group_by = 2, group_with = {\"sample_id\",\"strand\"}\n",
    "output: cord_bam_wasp_qc = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.out_wasp_qc.bam',\n",
    "        trans_bam_wasp_qc = f'{cwd}/{_sample_id}.Aligned.toTranscriptome.out_wasp_qc.bam'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "\n",
    "    # WASP-based QC filtering function\n",
    "    wasp_filter() {\n",
    "        # Check if an argument is provided\n",
    "        if [[ -z \"$1\" ]]; then\n",
    "            echo \"Usage: wasp_filter <file.something.ext>\"\n",
    "            return 1\n",
    "        fi\n",
    "        local input_file=\"$1\"\n",
    "        local base_name=\"${input_file%.*}\"  # Removes the last extension\n",
    "        local derived_file=\"${base_name%.*}.int.bam\"  # Removes the second to last extension and adds .ext     \n",
    "\n",
    "        if [ $[unique] ]; then # pay attention to the spcae around the variable in [[]].\n",
    "            # wasp_filtering & unique_filtering\n",
    "            if [ $[wasp] ]; then\n",
    "                echo \"Applying unique_WASP filtering\"\n",
    "                samtools view -h -q $[mapping_quality] \"$input_file\" | grep -v \"vW:i:[2-7]\" | samtools view -b > \"$derived_file\"\n",
    "            # unique_filtering\n",
    "            else\n",
    "                echo \"Applying unique filtering\"\n",
    "                samtools view -h -q $[mapping_quality] \"$input_file\" | samtools view -b > \"$derived_file\" # remember to convert SAM files back to BAM files\n",
    "            fi\n",
    "        else\n",
    "            # only wasp_filtering: Potential Allele-Specific Alignment Bias\n",
    "            if [ $[wasp] ]; then\n",
    "                echo \"Applying WASP filtering\"\n",
    "                samtools view -h \"$input_file\" | grep -v \"vW:i:[2-7]\" | samtools view -b > \"$derived_file\"\n",
    "            # no filter\n",
    "            else\n",
    "                echo \"Applying no filtering\"\n",
    "                cp \"$input_file\" \"$derived_file\"\n",
    "            fi\n",
    "        fi\n",
    "        return 0\n",
    "    }\n",
    "\n",
    "    # I tried using the filtered bam file to replace the original bam file, but I got \"Exec format error\". Don't do that! It's risky for the file being overwritten and being read simultaneously.\n",
    "\n",
    "    export -f wasp_filter  # Make it available in subshells, if needed\n",
    "\n",
    "    # Perform WASP-based QC filtering\n",
    "    if [ $[varVCFfile] ]; then\n",
    "        wasp_filter $[_input[0]]\n",
    "        wasp_filter $[_input[1]] \n",
    "        mv $[_output[0]:nnnn].Aligned.sortedByCoord.int.bam  $[_output[0]]\n",
    "        mv $[_output[1]:nnnn].Aligned.toTranscriptome.int.bam  $[_output[1]]\n",
    "    fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
