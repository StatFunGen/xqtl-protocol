{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solar-artwork",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Quantifying alternative splicing from RNA-seq data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ef11f-60b0-4044-8a13-f5e8669fd67a",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437a565-908c-4160-a302-e24ab313edba",
   "metadata": {},
   "source": [
    "Our pipeline calls alternative splicing events from RNA-seq data using leafcutter and psichomics to call the RNA-seq data from `fastq.gz` data which has been mapped to a reference genome using STAR with the wasp option. It implements the GTEx pipeline for GTEx/TOPMed project. Please refer to [this page](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md) for detail. The choice of pipeline modules in this project is supported by internal (unpublished) benchmarks from GTEx group.\n",
    "\n",
    "We use two different tools to quantify the many types of splicing events which are outlined in [[cf. Wang et al (2008)](https://doi.org/10.1038/nature07509)] and [[cf. Park et al (2018)](https://doi.org/10.1016/j.ajhg.2017.11.002)]. The first, leafcutter, quantifies the usage of alternatively excised introns. This collectively captures skipped exons, 5’ and 3’ alternative splice site usage and other complex events [[cf. Li et al 2018](https://doi.org/10.1038/s41588-017-0004-9)]. This method was previously applied to ROSMAP data as part of the Brain xQTL version 2.0.  The second, psichomics, quantifies specific splicing events [[cf. Agostinho et al. 2019](https://doi.org/10.1093/nar/gky888)].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-computer",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "**Various reference data needs to be prepared before using this workflow**. [Here we provide a module](https://cumc.github.io/xqtl-pipeline/code/data_preprocessing/reference_data.html) to download and prepare the reference data. \n",
    "\n",
    "A minimal working example is uploaded in the [google drive](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R). It contains example inputs for leafcutter/psichomics, two spliing annotations for psichomics, the meta-data file list, and a example of blacklist chromosome file for leafcutter.\n",
    "\n",
    "The leafcutter and psichomics tools may be run in parallel and are not dependent on one another.\n",
    "\n",
    "The product of this workflow can be used in generating phenotype tables using /molecular_phenotyles/QC/splicing_normalization.ipynb.\n",
    "\n",
    "Both leafcutter and psichomics section, a meta-data file, white space delimited, containing 4 columns: sample ID, RNA strandness and path to the BAM files input for leafcutter section and to SJ.out.tab files for psichomics section:\n",
    "\n",
    "```\n",
    "sample_id       strand          bam_list                                SJ_list\n",
    "sample_1        rf              sample_1.Aligned.sortedByCoord.out.bam  sample_1.SJ.out.tab\n",
    "sample_2        fr              sample_2.Aligned.sortedByCoord.out.bam  sample_2.SJ.out.tab\n",
    "sample_3        strand_missing  sample_3.Aligned.sortedByCoord.out.bam  sample_3.SJ.out.tab\n",
    "```\n",
    "\n",
    "If only one type of input files is prepared, one of the bam_list column and SJ_list column can be left empty.\n",
    "\n",
    "### `leafcutter`\n",
    "\n",
    "The bam files can be generated by `the STAR_align` workflow from our RNA_calling.ipynb module. \n",
    "\n",
    "All the BAM files should be available under specified folder (default assumes the same folder as where the meta-data file is).\n",
    "\n",
    "If intend to blacklist some chromosomes and not analyze it, add one text file named black_list.txt with one chromosome name per line in the same directory of the meta-data file.\n",
    "\n",
    "\n",
    "### `psichomics`\n",
    "\n",
    "The SJ.out.tab files can be generated by `the STAR_align` workflow from our RNA_calling.ipynb module. \n",
    "\n",
    "All the SJ.out.tab files should be available under specified folder (default assumes the same folder as where the meta-data file is).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-cooling",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "### `leafcutter`\n",
    "\n",
    "`{sample_list}` below refers to the name of the meta-data file input.\n",
    "\n",
    "Main output include: \n",
    "\n",
    "- `{sample_list}_intron_usage_perind.counts.gz` file with row id in format: \"chromosome:intron_start:intron_end:cluster_id\", column labeled as input sample names and each type of intron usage ratio under each sample (i.e. #particular intron in a sample / #total introns classified in the same cluster in a sample) in each cells. \n",
    "- `{sample_list}_intron_usage_perind_numers.counts.gz` file with the same row and column label but the count of each intron in each cells.\n",
    "\n",
    "### `psichomics`\n",
    "\n",
    "- `psi_raw_data.tsv` A dataframe of PSI values (quantification of the alternative splicing events) with first column splicing event identifier (for instance, SE_1_-_2125078_2124414_2124284_2121220_C1orf86) is composed of:\n",
    "\n",
    "                   Event type (SE stands for skipped exon)\n",
    "                   Chromosome (1)\n",
    "                   Strand (-)\n",
    "                   Relevant coordinates depending on event type (in this case, the first constitutive exon’s end, the                            alternative exon’ start and end and the second constitutive exon’s start)\n",
    "                   Associated gene (C1orf86)\n",
    "\n",
    "| Splicing Event Type | Abbreviation | [Coordinates](https://bioconductor.org/packages/release/bioc/manuals/psichomics/man/psichomics.pdf) |\n",
    "| --- | --- | --- |\n",
    "| Skipped Exon | SE | constitutive exon 1 end, alternative exon (start and end) and constitutive exon 2 start |\n",
    "| Mutually exclusive exon | MXE | constitutive exon 1 end, alternative exon 1 and 2 (start and end) and constitutive exon 2 start |\n",
    "| Alternative 5' splice site | A5SS | constitutive exon 1 end, alternative exon 1 end and constitutive exon 2 start |\n",
    "| Alternative 3' splice site | A3SS | constitutive exon 1 end, alternative exon 1 start and constitutive exon 2 start |\n",
    "| Alternative first exon | AFE | constitutive exon 1 end, alternative exon 1 end and constitutive exon 2 start |\n",
    "| Alternative last exon | ALE | constitutive exon 1 end, alternative exon 1 start and constitutive exon 2 start |\n",
    "| Alternative first exon (exon-centered - less reliable) | AFE_exon | constitutive exon 1 end, alternative exon 1 end and constitutive exon 2 start |\n",
    "| Alternative last exon (exon-centered - less reliable) | ALE_exon | constitutive exon 1 end, alternative exon 1 start and constitutive exon 2 start |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-internet",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-margin",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### i. Splicing Quantification\n",
    "#### a. Leafcutter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdeac4b-e9ee-4bc9-9db7-511b01437c20",
   "metadata": {},
   "source": [
    "Timing: <30min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c88d9b-26ef-40b3-87f6-429f9e4e61fa",
   "metadata": {},
   "source": [
    "Quantify the usage of alternatively excised introns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e7cbff-a17f-42dd-b854-1eed7ae94213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mleafcutter_1\u001b[0m: \n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=0) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=1) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=2) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=3) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=4) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=5) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=6) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=7) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=8) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m (index=9) is \u001b[32mignored\u001b[0m due to saved signature\n",
      "INFO: \u001b[32mleafcutter_1\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/leafcutter/1000-PCC.bam.Aligned.sortedByCoord.out_wasp_qc.md.junc /restricted/projectnb/xqtl/xqtl_protocol/output_test/leafcutter/1001-PCC.bam.Aligned.sortedByCoord.out_wasp_qc.md.junc... (10 items in 10 groups)\u001b[0m\n",
      "INFO: Running \u001b[32mleafcutter_2\u001b[0m: \n",
      "INFO: t53ce7f911566d133 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2489088 (\"job_t53ce7f911566d133\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mleafcutter_2\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/leafcutter/PCC_sample_list_subset_leafcutter_intron_usage_perind.counts.gz\u001b[0m\n",
      "INFO: Workflow leafcutter (ID=w2d4888af18ce85b3) is executed successfully with 1 completed step, 1 ignored step, 10 ignored substeps and 1 completed task.\n"
     ]
    }
   ],
   "source": [
    "!sos run splicing_calling.ipynb leafcutter \\\n",
    "    --cwd ../../output_test/leafcutter \\\n",
    "    --samples ../../PCC_sample_list_subset_leafcutter \\\n",
    "    --data-dir ../../output_test/star_output_wasp \\\n",
    "    --container oras://ghcr.io/cumc/leafcutter_apptainer:latest \\\n",
    "    -c ../csg.yml -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-tuition",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "#### b. Psichomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b6326a-5052-47bc-bda9-ab9f9c99c2be",
   "metadata": {},
   "source": [
    "Timing: ~30min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e797b8-1b27-4e69-83d3-943e4546e58c",
   "metadata": {},
   "source": [
    "Quantify specific splicing events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fe7020-382e-48a9-b571-8c0ea8e7d8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mpsichomics_1\u001b[0m: \n",
      "INFO: t9842e29b618d1fad \u001b[32mrestart\u001b[0m from status \u001b[32mfailed\u001b[0m\n",
      "INFO: t9842e29b618d1fad \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2490344 (\"job_t9842e29b618d1fad\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mpsichomics_1\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/psichomics/psichomics_junctions.txt\u001b[0m\n",
      "INFO: Running \u001b[32mpsichomics_2\u001b[0m: \n",
      "INFO: t4908c2bee8ddf317 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2490387 (\"job_t4908c2bee8ddf317\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mpsichomics_2\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/psichomics/psi_raw_data.tsv\u001b[0m\n",
      "INFO: Workflow psichomics (ID=w2c5d88bef5022bf9) is executed successfully with 2 completed steps and 2 completed tasks.\n"
     ]
    }
   ],
   "source": [
    "!sos run splicing_calling.ipynb psichomics \\\n",
    "    --cwd ../../output_test/psichomics/ \\\n",
    "    --samples ../../PCC_sample_list_subset_leafcutter \\\n",
    "    --data-dir ../../output_test/star_output_wasp \\\n",
    "    --splicing_annotation ../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.SUPPA_annotation.rds \\\n",
    "    --container oras://ghcr.io/cumc/psichomics_apptainer:latest \\\n",
    "    -c ../csg.yml -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9567a5-64b1-494e-b10f-2db715097851",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e94de-7f4a-4f09-87a9-8a9d4a23934e",
   "metadata": {},
   "source": [
    "| Step | Substep | Problem | Possible Reason | Solution |\n",
    "|------|---------|---------|------------------|---------|\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-photography",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "forbidden-harassment",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run splicing_calling.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  leafcutter\n",
      "  psichomics\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        The output directory for generated files.\n",
      "  --samples VAL (as path, required)\n",
      "                        Sample meta data list\n",
      "  --data-dir  path(f\"{samples:d}\")\n",
      "\n",
      "                        Raw data directory, default to the same directory as\n",
      "                        sample list\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  leafcutter_1:\n",
      "    Workflow Options:\n",
      "      --anchor-len 8 (as int)\n",
      "                        anchor length (default 8)\n",
      "      --min-intron-len 50 (as int)\n",
      "                        minimum intron length to be analyzed (default 50)\n",
      "      --max-intron-len 500000 (as int)\n",
      "                        maximum intron length to be analyzed (default 500000)\n",
      "  leafcutter_2:\n",
      "    Workflow Options:\n",
      "      --min-clu-reads 50 (as int)\n",
      "                        minimum reads in a cluster (default 50 reads)\n",
      "      --max-intron-len 500000 (as int)\n",
      "                        maximum intron length to be analyzed (default 500000)\n",
      "  psichomics_1:\n",
      "  psichomics_2:\n"
     ]
    }
   ],
   "source": [
    "sos run splicing_calling.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-happiness",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loaded-connecticut",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files. \n",
    "parameter: cwd = path(\"output\")\n",
    "# Sample meta data list\n",
    "parameter: samples = path\n",
    "# Raw data directory, default to the same directory as sample list\n",
    "parameter: data_dir = path(f\"{samples:d}\")\n",
    "\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')\n",
    "\n",
    "def get_samples(fn, dr):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    samples = pd.read_csv(fn, sep='\\t')\n",
    "    names = []\n",
    "    strandness = []\n",
    "    bam_list = []\n",
    "    bam_files = []\n",
    "    SJtab_list = []\n",
    "    SJtab_files = []\n",
    "    \n",
    "    samples = samples.fillna(\"NA\")\n",
    "    names = samples['sample_id'].tolist()\n",
    "    strandness = samples['strand'].tolist()\n",
    "    bam_list = samples['coord_bam_list'].tolist()\n",
    "    SJtab_list = samples['SJ_list'].tolist()\n",
    "    \n",
    "    if ((len(bam_list) == sum(x == \"NA\" for x in bam_list)) & (len(SJtab_list) == sum(x == \"NA\" for x in SJtab_list))):\n",
    "        raise ValueError(\"At least one type of input should be ready\")\n",
    "        \n",
    "    for j in range(len(strandness)):\n",
    "        # for regtools command usage, replace 0 = unstranded/XS, 1 = first-strand/RF, 2 = second-strand/FR\n",
    "        if strandness[j] == 'rf':\n",
    "            strandness[j] = 'RF'\n",
    "        if strandness[j] == 'fr':\n",
    "            strandness[j] = 'FR'\n",
    "        if strandness[j] == 'strand_missing':\n",
    "            strandness[j] = 'XS'\n",
    "            \n",
    "    if (len(bam_list) != 0) & (len(bam_list) != sum(x == \"NA\" for x in bam_list)):\n",
    "        for y in bam_list:\n",
    "            y = os.path.join(dr, y)\n",
    "            if not os.path.isfile(y):\n",
    "                raise ValueError(f\"File {y} does not exist\")\n",
    "            bam_files.append(y)\n",
    "        \n",
    "    if len(bam_list) != len(set(bam_list)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in BAM file list\")\n",
    "    \n",
    "    if (len(SJtab_list) != 0) & (len(SJtab_list) != sum(x == \"NA\" for x in SJtab_list)):\n",
    "        for y in SJtab_list:\n",
    "            y = os.path.join(dr, y)\n",
    "            if not os.path.isfile(y):\n",
    "                raise ValueError(f\"File {y} does not exist\")\n",
    "            SJtab_files.append(y)\n",
    "        \n",
    "    if len(SJtab_list) != len(set(SJtab_list)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in SJ.tab file list\")\n",
    "        \n",
    "    return names, strandness, bam_files, SJtab_files\n",
    "\n",
    "sample_id, strandness, bam_data, SJtab_data = get_samples(samples, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-venture",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `leafcutter`\n",
    "\n",
    "Documentation: [`leafcutter`](https://davidaknowles.github.io/leafcutter/index.html). The choices of regtool parameters are [discussed here](https://github.com/davidaknowles/leafcutter/issues/127).\n",
    "\n",
    "### Other clustering options:\n",
    "\n",
    "*   \"-q\", \"--quiet\" : don't print status messages to stdout, default=True.\n",
    "\n",
    "*   \"-p\", \"--mincluratio\" : minimum fraction of reads in a cluster that support a junction, default 0.001. \n",
    "\n",
    "*   \"-c\", \"--cluster\" : refined cluster file when clusters are already made, default = None.\n",
    "\n",
    "*   \"-k\", \"--nochromcheck\" : Don't check that the chromosomes are well formated e.g. chr1, chr2, ..., or 1, 2, ..., default = False.\n",
    "\n",
    "*    \"-C\", \"--includeconst\" : also include constitutive introns, default = False.\n",
    "\n",
    "The default parameter we used are:\n",
    "\n",
    "`--min_clu_ratio 0.001 --max_intron_len 500000 --min_clu_reads 30`\n",
    "\n",
    "These parameter is based on [GTEX's sQTL discovery pipeline (Section 3.4.3) ](https://www.science.org/action/downloadSupplement?doi=10.1126%2Fscience.aaz1776&file=aaz1776_aguet_sm.pdf)\n",
    "\n",
    "### Things to keep in mind:\n",
    "\n",
    "* If .bam.bai index files of the .bam input are ready before using leafCutter, it can be placed in the same directory with input .bam files and the \"samtools index ${_input}\" line can be skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-information",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_1, leafcutter_preprocessing_1]\n",
    "# anchor length (default 8)\n",
    "parameter: anchor_len = 8\n",
    "# minimum intron length to be analyzed (default 50)\n",
    "parameter: min_intron_len = 50\n",
    "# maximum intron length to be analyzed (default 500000)\n",
    "parameter: max_intron_len = 500000\n",
    "input: bam_data, group_by = 1, group_with = \"strandness\"\n",
    "output: f'{cwd}/{_input:bn}.junc' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    samtools index ${_input}\n",
    "    regtools junctions extract -a ${anchor_len} -m ${min_intron_len} -M ${max_intron_len} -s ${_strandness} ${_input} -o ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "artificial-contrast",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_2]\n",
    "# minimum reads in a cluster (default 50 reads)\n",
    "parameter: min_clu_reads = 30 \n",
    "# maximum intron length to be analyzed (default 500000)\n",
    "parameter: max_intron_len = 500000 \n",
    "# minimum fraction of reads in a cluster that support a junction (default 0.001)\n",
    "parameter: min_clu_ratio = 0.001\n",
    "input: group_by = 'all'\n",
    "output: f'{cwd}/{samples:bn}_intron_usage_perind.counts.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    rm -f ${_output:nn}.junc\n",
    "    for i in ${_input:r}; do\n",
    "    echo $i >> ${_output:nn}.junc ; done\n",
    "    leafcutter_cluster_regtools.py -j ${_output:nn}.junc -o ${f'{_output:bnn}'.replace(\"_perind\",\"\")} -m ${min_clu_reads} -l ${max_intron_len} -r ${cwd} -p ${min_clu_ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "miniature-hostel",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_preprocessing_2]\n",
    "input: group_by = 'all'\n",
    "output: f'{cwd}/{samples:bn}_intron_usage_perind.junc'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    rm -f ${_output:r}\n",
    "    for i in ${_input:r}; do\n",
    "    echo $i >> ${_output:r} ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-cincinnati",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `psichomics`\n",
    "\n",
    "Documentation: [`psichomics`](http://bioconductor.org/packages/release/bioc/html/psichomics.html)\n",
    "\n",
    "### Other options\n",
    "\n",
    "quantifySplicing( annotation,\n",
    "                  junctionQuant,\n",
    "                  eventType = c(\"SE\", \"MXE\", \"ALE\", \"AFE\", \"A3SS\", \"A5SS\"),\n",
    "                  minReads = 10,\n",
    "                  genes = NULL\n",
    ")\n",
    "\n",
    "In function quantifySplicing, arguments eventType (Character: splicing event types to quantify), minReads (Integer: values whose number of total supporting read counts is below minReads are returned as NA) and genes (Character: gene symbols for which to quantify splicing events. If NULL, events from all genes are quantified.) can be specified. Usage and default values are shown above.\n",
    "\n",
    "### Alternative Splicing Annotation Information\n",
    "\n",
    "Two alternative splicing annotations will be provided in this pipeline which can be download [here](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R). The hg38_suppa.rds is created Via SUPPA using the gtf file of the xqtl-pipeline, and the modified_psichomics_hg38_splicing_annotation.rds is modified from the default Human hg38 (2018-04-30) annotation provided by psichomics package. Description of the database can be found in the Alternative splicing annotation section in the [MATERIALS AND METHODS](https://academic.oup.com/nar/article/47/2/e7/5114259?login=true#130023625) part. Gene names of the original annotation are replaced by Ensembl ids for format unifying. The Ensembl IDs used in modifiction are matched from the gtf file, HGNC database, SUPPA and VASTTOOL records within the original annotation.\n",
    "\n",
    "Theoretically the annotation created using the gtf file only will give results more consistent with other part of the pipeline. The annotation modified from psichomics original hg38 annotation can identify more events since it was build based on information maximizing principle, however there will be risk of containing outdated information too. \n",
    "\n",
    "For details of generation method of the gtf file and the two splicing annotations, please check the GFF3 to GTF formatting, Generation of SUPPA annotation for psichomics, and Modification of psichomics default Hg38 splicing annotation sections in [reference_data.ipynb](https://github.com/cumc/xqtl-pipeline/blob/main/code/data_preprocessing/reference_data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "norman-finger",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[psichomics_1]\n",
    "input: SJtab_data\n",
    "output: f'{cwd}/psichomics_junctions.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    library(\"psichomics\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"purrr\")\n",
    "\n",
    "    df_replace <- function (df, old, new){\n",
    "          if (is.na(old)) {\n",
    "              df[is.na(df)] <- new\n",
    "          } else {\n",
    "              df[df == old] <- new\n",
    "          }\n",
    "      return (df)\n",
    "      }\n",
    "\n",
    "    options(scipen = 15) # to avoid writing in scientific notation\n",
    "    files = list()\n",
    "  \n",
    "    # Get the structure of input data\n",
    "    for (f in c(${_input:ar,})){\n",
    "      filename = gsub(\"^.*/\", \"\", f)\n",
    "      directory = gsub(filename, \"\", f)\n",
    "      if (length(files[[directory]]) == 0){\n",
    "        files[[directory]] = filename\n",
    "        } else {\n",
    "      files[[directory]] = append(files[[directory]], filename)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    batch_junction_list = list()\n",
    "    res = list()\n",
    "  \n",
    "    # Since prepareJunctionQuant() somehow don't read file names with path, if orinigal data are from the same directory, \n",
    "    #       just run it and write to the output folder, else run on each directory seperately and merge the junction tables.\n",
    "    if (length(files) == 1) {\n",
    "        setwd(names(files)[1])\n",
    "        prepareJunctionQuant(files[[1]], output = '${cwd}/psichomics_junctions.txt')\n",
    "    } else {\n",
    "        for (i in 1: (length(files))) {\n",
    "        d = names(files[i])\n",
    "        setwd(d)\n",
    "        output_name = sprintf(\"${cwd}/psichomics_junctions_%s.txt\",\n",
    "                               tail(as.list(strsplit(d, '/')[[1]]), 1))\n",
    "        prepareJunctionQuant(files[[d]], output = output_name)\n",
    "        batch_junction_list = append(batch_junction_list, output_name)\n",
    "        }\n",
    "  \n",
    "        for (file_name in batch_junction_list) {\n",
    "        res[[file_name]] = read.table(file_name, sep = '\\t', header = TRUE)\n",
    "        }\n",
    "  \n",
    "        # save the information of originally created NA if any\n",
    "        res = lapply(res, df_replace, old = NA, new = \"original_na\")\n",
    "\n",
    "        # merge the junction tables and pad with 0\n",
    "        res = res %>% reduce(full_join, by = \"Junction.ID\")\n",
    "        res[is.na(res)] <- 0\n",
    "\n",
    "        # fill the original NAs back\n",
    "        res = df_replace(res, \"original_na\", NA)\n",
    "        write.table(res, file = '${cwd}/psichomics_junctions.txt', quote = FALSE, sep = '\\t', row.names = FALSE)\n",
    "  \n",
    "    }\n",
    "  \n",
    "    options(scipen = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "traditional-reserve",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[psichomics_2]\n",
    "# splicing annotation for psichomics\n",
    "parameter: splicing_annotation = path\n",
    "output: f'{cwd}/psi_raw_data.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    library(\"psichomics\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"purrr\")    \n",
    "    data <- read.table(\"${cwd}/psichomics_junctions.txt\", sep = '\\t', header = TRUE)\n",
    "    \n",
    "    names(data) <- sub('X', '', names(data))\n",
    "    data <- data[- grep(\"chrM\", data$Junction.ID),]\n",
    "    data <- data[- grep(\"chrUn\", data$Junction.ID),]\n",
    "    data <- data[- grep(\"random\", data$Junction.ID),]\n",
    "  \n",
    "    junctionQuant <- data[,-1]\n",
    "    rownames(junctionQuant) <- data[,1]\n",
    "\n",
    "    # Record of how original psichomics example did, we mimicked the format change loadLocalFiles() function did on our\n",
    "    #       data to avoid bugs.\n",
    "    #data <- loadLocalFiles(\"${cwd}\")\n",
    "    #junctionQuant <- data[[1]]$`Junction quantification`\n",
    "    \n",
    "    annotation = readRDS(\"${splicing_annotation:a}\")\n",
    "    psi <- quantifySplicing(annotation, junctionQuant)\n",
    "    write.table(psi, file='${cwd}/psi_raw_data.tsv', quote=FALSE, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
