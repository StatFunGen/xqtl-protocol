{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# QTL Association Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "We perform QTL association testing using TensorQTL [[cf. Taylor-Weiner et al (2019)](https://doi.org/10.1186/s13059-019-1836-7)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "- List of molecular phenotype files: a list of `bed.gz` files containing the table for the molecular phenotype. It should have a companion index file in `tbi` format. It is the output of gene_annotation or phenotype_by_chorm\n",
    "- List of genotypes in both PLINK binary format (bed/bim/fam) and PLINK 2 binary genotype table (pgen/pvar/psam) for each chromosome, previously processed through our genotype QC pipelines.\n",
    "- Covariate file, a file with #id + samples name as colnames and each row a covariate: fixed and known covariates as well as hidden covariates recovered from factor analysis.\n",
    "- Optionally, a list of traits (genes, regions of molecular features etc) to analyze.\n",
    "\n",
    "### Example phenotype list\n",
    "\n",
    "\n",
    "The header of the bed.gz is per the [TensorQTL](https://github.com/broadinstitute/tensorqtl) convention:\n",
    "\n",
    "- Phenotypes must be provided in BED format, sorted by chromosome and [start,end] position, with a single header line starting with # and the first four columns corresponding to: chr, start, end, phenotype_id, with the remaining columns corresponding to samples (the identifiers must match those in the genotype input). The BED file should specify the cis-window (usually the TSS), with start = the minimum start for each gene, end = the maximum end for each gene(extracted from phenotype referenced gtf file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#chr\tstart\tend\tID\tpath\n",
    "chr1\t631073\t632616\tENSG00000237973\t/home/rl3328/motor_data/pheno_data/Quad/phenotype_by_chrom/genes_logcpm.bed.chr1.bed.gz\n",
    "chr1\t633695\t634376\tENSG00000248527\t/home/rl3328/motor_data/pheno_data/Quad/phenotype_by_chrom/genes_logcpm.bed.chr1.bed.gz\n",
    "chr1\t825137\t859446\tENSG00000228794\t/home/rl3328/motor_data/pheno_data/Quad/phenotype_by_chrom/genes_logcpm.bed.chr1.bed.gz\n",
    "chr1\t944202\t959309\tENSG00000188976\t/home/rl3328/motor_data/pheno_data/Quad/phenotype_by_chrom/genes_logcpm.bed.chr1.bed.gz\n",
    "chr1\t975203\t982093\tENSG00000187642\t/home/rl3328/motor_data/pheno_data/Quad/phenotype_by_chrom/genes_logcpm.bed.chr1.bed.gz\n",
    "chr1\t1216908\t1232067\tENSG00000078808\t/home/rl3328/motor_data/pheno_data/Quad/phenotype_by_chrom/genes_logcpm.bed.chr1.bed.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "\n",
    "For cis-analysis:\n",
    "\n",
    "- Optionally, a list of genomic regions associate with each molecular features to analyze. The default cis-analysis will use a window around TSS. This can be customized to take given start and end genomic coordinates. we currently suggest using 1Mb window around a gene because longer customized cis-windows (such as extending by TAD) does not yield significant improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output  \n",
    "\n",
    "For each chromosome, several summary statistics files are generated, including both nominal test statistics for each test and region (gene) level association evidence.  \n",
    "\n",
    "### Nominal Association Results  \n",
    "\n",
    "The columns of the nominal association result are as follows:  \n",
    "\n",
    "- **chrom**: Variant chromosome.  \n",
    "- **pos**: Variant chromosomal position (basepairs).  \n",
    "- **molecular_trait_id**: Molecular trait identifier (gene).  \n",
    "- **variant_id**: ID of the variant (rsid or chr:position:ref:alt).  \n",
    "- **tss_distance**: Distance of the SNP to the gene transcription start site (TSS).  \n",
    "- **tes_distance**: Distance of the SNP to the gene transcription end site (TES).  \n",
    "- **cis_window_start_distance**: Distance of the SNP to the start of the cis window (if using a customized cis window).  \n",
    "- **cis_window_end_distance**: Distance of the SNP to the end of the cis window (if using a customized cis window).  \n",
    "- **af**: The allele frequency of this SNP.  \n",
    "- **ma_samples**: Number of samples carrying the minor allele.  \n",
    "- **ma_count**: Total number of minor alleles across individuals.  \n",
    "- **pvalue**: Nominal P-value from linear regression.  \n",
    "- **bhat**: Slope of the linear regression.  \n",
    "- **sebhat**: Standard error of bhat.  \n",
    "- **n**: Number of phenotypes after basic QC.  \n",
    "#### Multiple Testing Corrected Results:  \n",
    "- **qvalue**: Calculated q-value for each SNP (grouped by gene).  \n",
    "\n",
    "### Interaction Association Results  \n",
    "\n",
    "The columns of interaction association results are as follows (FIXME):  \n",
    "\n",
    "**Model:**  \n",
    "$$\n",
    "\\text{phenotype} = \\beta_0 + \\beta_1 \\cdot \\text{snp} + \\beta_2 \\cdot \\text{msex} + \\beta_3 \\cdot (\\text{snp} \\times \\text{msex}) + \\epsilon\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "(Taking msex as the interaction factor)  \n",
    "\n",
    "- **chrom**: Chromosome number.  \n",
    "- **pos**: Variant chromosomal position (basepairs).  \n",
    "- **a2**: Variant reference allele (A, C, T, or G).  \n",
    "- **a1**: Variant alternate allele.  \n",
    "- **molecular_trait_id**: Molecular trait identifier, varies from phenotypes to phenotypes.  \n",
    "- **variant_id**: ID of the top variant (rsid or chr:position:ref:alt).  \n",
    "- **af**: Alternative allele frequency in the MiGA cohort.  \n",
    "- **ma_samples**: Number of samples carrying the minor allele.  \n",
    "- **ma_count**: Total number of minor alleles across individuals.  \n",
    "- **pvalue**: P-value of the main effect from the nonlinear regression.  \n",
    "- **bhat**: Slope of the main effect from the nonlinear regression.  \n",
    "- **se**: Standard error of beta.  \n",
    "- **pvalue_msex**: P-value of the msex term from the nonlinear regression.  \n",
    "- **bhat_msex**: Slope of the msex term from the nonlinear regression.  \n",
    "- **se_msex**: Standard error of bhat_msex.  \n",
    "- **pvalue_msex_interaction**: P-value of the interaction term from the nonlinear regression.  \n",
    "- **bhat_msex_interaction**: Slope of the interaction term from the nonlinear regression.  \n",
    "- **se_msex_interaction**: Standard error of beta_msex_interaction.  \n",
    "- **molecular_trait_object_id**: An intermediate ID (can be ignored).  \n",
    "- **n**: Number of samples.\n",
    "#### Multiple Testing Corrected Results:  \n",
    "- **qvalue_main**: The q-value of the main effect.  \n",
    "- **qvalue_interaction**: The q-value of the interaction effect.  \n",
    "\n",
    "### Region (Gene) Level Association Evidence  \n",
    "\n",
    "The column specifications for region-level association evidence are as follows:  \n",
    "\n",
    "- **chrom**: Chromosome number.  \n",
    "- **pos**: Variant chromosomal position (basepairs).  \n",
    "- **n_variant**: Total number of variants tested in cis.  \n",
    "- **beta_shape1**: First parameter value of the fitted beta distribution.  \n",
    "- **beta_shape2**: Second parameter value of the fitted beta distribution.  \n",
    "- **true_df**: Effective degrees of freedom of the beta distribution approximation.  \n",
    "- **p_true_df**: Empirical P-value for the beta distribution approximation.  \n",
    "- **variant_id**: ID of the top variant (rsid or chr:position:ref:alt).  \n",
    "- **tss_distance**: Distance of the SNP to the gene transcription start site (TSS).  \n",
    "- **tes_distance**: Distance of the SNP to the gene transcription end site (TES).  \n",
    "- **ma_samples**: Number of samples carrying the minor allele.  \n",
    "- **ma_count**: Total number of minor alleles across individuals.  \n",
    "- **af**: Alternative allele frequency.  \n",
    "- **p_nominal**: Nominal P-value from linear regression.  \n",
    "- **bhat**: Slope of the linear regression.  \n",
    "- **sehat**: Standard error of the bhat.  \n",
    "- **p_perm**: First permutation P-value directly obtained from the permutations with the direct method.  \n",
    "- **p_beta**: Second permutation P-value obtained via beta approximation (this is the one to use for downstream analysis).  \n",
    "- **molecular_trait_object_id**: Molecular trait identifier (gene).  \n",
    "- **n_traits**: Group size in the permutation test.  \n",
    "- **genomic_inflation**: Genomic inflation factor (lambda), quantifying the extent of bulk inflation and the excess false positive rate.  \n",
    "#### Multiple Testing Corrected Results:  \n",
    "- **q_beta**: Q-value for p_beta using Storey's method (qvalue), more conservative than FDR.  \n",
    "- **q_perm**: Q-value for p_perm using Storey's method (qvalue), more conservative than FDR.  \n",
    "- **fdr_beta**: Adjusted P-value for p_beta using the Benjamini-Hochberg method (FDR).  \n",
    "- **fdr_perm**: Adjusted P-value for p_perm using the Benjamini-Hochberg method (FDR).  \n",
    "- **p_nominal_threshold**: Nominal p-value threshold for variants in the corresponding molecular trait, derived from empirical beta distribution as a result of permutation testing.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example Steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The data can be found on [Synapse](https://www.synapse.org/#!Synapse:syn36416559/files/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### i. Cis TensorQTL Command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run pipeline/TensorQTL.ipynb cis \\\n",
    "    --genotype-file output/genotype_by_chrom/protocol_example.genotype.chr21_22.genotype_by_chrom_files.txt \\\n",
    "    --phenotype-file  output/phenotype_by_chrom/protocol_example.protein.bed.phenotype_by_chrom_files.region_list.txt \\\n",
    "    --covariate-file output/covariate/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
    "    --customized_cis_windows prototype_example/protocol_example/protocol_example.protein.enhanced_cis_chr21_chr22.bed \\\n",
    "    --cwd output/cis_association/ \\\n",
    "    --MAC 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### ii. Trans TensorQTL Command \n",
    "\n",
    "Note: Some protein is not in the customized cis windows list. There we will need to remove them from the analysis by create a region_list. Noted that the region list need to be a actual file. So `<()` file is not acceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "zcat output/protocol_example.protein.bed.gz | cut -f 1,2,3,4 | grep -v -e ENSG00000163554 \\\n",
    "    -e ENSG00000171564 -e ENSG00000171560 -e ENSG00000171557 > output/protocol_example.protein.region_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The following will take more than 180G of memory to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run xqtl-protocol/pipeline/TensorQTL.ipynb trans \\\n",
    "    --genotype-file output/genotype_by_chrom/protocol_example.genotype.chr21_22.genotype_by_chrom_files.txt \\\n",
    "    --phenotype-file  output/phenotype_by_chrom/protocol_example.protein.bed.phenotype_by_chrom_files.region_list.txt \\\n",
    "    --region-list output/protocol_example.protein.region_list \\\n",
    "    --covariate-file output/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.unrelated.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
    "    --customized-cis-windows input/protocol_example.protein.enhanced_cis_chr21_chr22.bed \\\n",
    "    --cwd output/association/trans/ \\\n",
    "    --MAC 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "| Step | Substep | Problem | Possible Reason | Solution |\n",
    "|------|---------|---------|------------------|---------|\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command Interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run TensorQTL.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  cis\n",
      "  trans\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        Path to the work directory of the analysis.\n",
      "  --phenotype-file VAL (as path, required)\n",
      "                        Phenotype file, or a list of phenotype per region.\n",
      "  --genotype-file VAL (as path, required)\n",
      "                        A genotype file in PLINK binary format (bed/bam/fam)\n",
      "                        format, or a list of genotype per chrom\n",
      "  --covariate-file VAL (as path, required)\n",
      "                        Covariate file\n",
      "  --name  f\"{phenotype_file:bn}_{covariate_file:bn}\"\n",
      "\n",
      "                        Prefix for the analysis output\n",
      "  --region-list . (as path)\n",
      "                        An optional subset of regions of molecular features to\n",
      "                        analyze. The last column is the gene names\n",
      "  --region-list-phenotype-column 4 (as int)\n",
      "  --keep-sample . (as path)\n",
      "                        Set list of sample to be keep\n",
      "  --interaction ''\n",
      "                        FIXME: please document\n",
      "  --customized-cis-windows . (as path)\n",
      "                        An optional list documenting the custom cis window for\n",
      "                        each region to analyze, with four column, chr, start,\n",
      "                        end, region ID (eg gene ID). If this list is not\n",
      "                        provided, the default `window` parameter (see below)\n",
      "                        will be used.\n",
      "  --phenotype-group . (as path)\n",
      "                        The phenotype group file to group molecule_trait into\n",
      "                        molecule_trait_object This applies to multiple molecular\n",
      "                        events in the same region, such as sQTL analysis.\n",
      "  --chromosome  (as list)\n",
      "                        The name of phenotype corresponding to gene_id or\n",
      "                        gene_name in the region\n",
      "  --MAC 0 (as int)\n",
      "                        Minor allele count cutoff\n",
      "  --window 1000000 (as int)\n",
      "                        Specify the cis window for the up and downstream radius\n",
      "                        to analyze around the region of interest in units of bp\n",
      "                        This parameter will be set to zero if\n",
      "                        `customized_cis_windows` is provided.\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 12h\n",
      "  --mem 16G\n",
      "  --container ''\n",
      "                        Container option for software to run the analysis:\n",
      "                        docker or singularity\n",
      "  --entrypoint  ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
      "\n",
      "  --maf-threshold  MAC/(2.0*N) \n",
      "\n",
      "                        Minor allele frequency cutoff. It will overwrite minor\n",
      "                        allele cutoff. You may consider setting it to higher for\n",
      "                        interaction analysis if you have statistical power\n",
      "                        concerns\n",
      "\n",
      "Sections\n",
      "  cis_1:\n",
      "    Workflow Options:\n",
      "      --[no-]skip-nominal-if-exist (default to False)\n",
      "                        parse input file lists skip nominal association results\n",
      "                        if the files exists already This is false by default\n",
      "                        which means to recompute everything This is only\n",
      "                        relevant when the `parquet` files for nominal results\n",
      "                        exist but not the other files and you want to avoid\n",
      "                        computing the nominal results again\n",
      "      --[no-]permutation (default to True)\n",
      "  cis_2:\n",
      "  trans_1:\n",
      "    Workflow Options:\n",
      "      --batch-size 50000 (as int)\n",
      "      --pval-threshold 1.0 (as float)\n",
      "      --[no-]permutation (default to False)\n",
      "                        Permutation testing is incorrect when the analysis is\n",
      "                        done by chrom\n"
     ]
    }
   ],
   "source": [
    "sos run TensorQTL.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Old Minimal working example\n",
    "\n",
    "An MWE is uploaded to [google drive](https://drive.google.com/drive/folders/1yjTwoO0DYGi-J9ouMsh9fHKfDmsXJ_4I?usp=sharing).\n",
    "The singularity image (sif) for running this MWE is uploaded to [google drive](https://drive.google.com/drive/folders/1mLOS3AVQM8yTaWtCbO8Q3xla98Nr5bZQ)\n",
    "\n",
    "FIXME: need to update these links. \n",
    "\n",
    "FIXME: Also need to update the example commands below using our new example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/TensorQTL.ipynb cis \\\n",
    "    --genotype-file plink_files_list.txt \\\n",
    "    --phenotype-file MWE.bed.recipe \\\n",
    "    --covariate-file ALL.covariate.pca.BiCV.cov.gz \\\n",
    "    --cwd ./output/ \\\n",
    "    --MAC 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/TensorQTL.ipynb trans \\\n",
    "    --genotype-file plink_files_list.txt \\\n",
    "    --phenotype-file MWE.bed.recipe \\\n",
    "    --covariate-file ALL.covariate.pca.BiCV.cov.gz \\\n",
    "    --cwd ./output/ \\\n",
    "    --MAC 5 --region-name  gene_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/TensorQTL.ipynb trans \\\n",
    "    --genotype-file plink_files_list.txt \\\n",
    "    --phenotype-file MWE.bed.recipe \\\n",
    "    --covariate-file /mnt/vast/hpc/csg/snuc_pseudo_bulk/eight_tissue_analysis/output/data_preprocessing/ALL/covariates/ALL.log2cpm.ALL.covariate.pca.resid.PEER.cov.gz \\\n",
    "    --cwd ./output/trans_tensorQTL/ \\\n",
    "    --region-list /mnt/vast/hpc/csg/snuc_pseudo_bulk/eight_tissue_analysis/reference_data/AD_genes.region_list \\\n",
    "    --customized-cis-windows TADB_enhanced_cis.bed \\\n",
    "    --MAC 5 --region-name ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to the work directory of the analysis.\n",
    "parameter: cwd = path('output')\n",
    "# Phenotype file, or a list of phenotype per region.\n",
    "parameter: phenotype_file = path\n",
    "# A genotype file in PLINK binary format (bed/bam/fam) format, or a list of genotype per chrom\n",
    "parameter: genotype_file = path\n",
    "# Covariate file\n",
    "parameter: covariate_file = path\n",
    "# Prefix for the analysis output\n",
    "parameter: name = \"\"\n",
    "# An optional subset of regions of molecular features to analyze. The last column is the gene names\n",
    "parameter: region_list = path()\n",
    "parameter: region_list_phenotype_column = 4\n",
    "# Set list of sample to be keep\n",
    "parameter: keep_sample = path()\n",
    "# FIXME: please document\n",
    "parameter: interaction = \"\"\n",
    "\n",
    "# An optional list documenting the custom cis window for each region to analyze, with four column, chr, start, end, region ID (eg gene ID).\n",
    "# If this list is not provided, the default `window` parameter (see below) will be used.\n",
    "parameter: customized_cis_windows = path()\n",
    "\n",
    "# The phenotype group file to group molecule_trait into molecule_trait_object\n",
    "# This applies to multiple molecular events in the same region, such as sQTL analysis.\n",
    "parameter: phenotype_group = path() \n",
    "\n",
    "# The name of phenotype corresponding to gene_id or gene_name in the region\n",
    "parameter: chromosome = []\n",
    "# Minor allele count cutoff\n",
    "parameter: MAC = 0\n",
    "\n",
    "# Specify the cis window for the up and downstream radius to analyze around the region of interest in units of bp\n",
    "# This parameter will be set to zero if `customized_cis_windows` is provided.\n",
    "parameter: window = 1000000\n",
    "\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "parameter: walltime = '12h'\n",
    "parameter: mem = '16G'\n",
    "# Container option for software to run the analysis: docker or singularity\n",
    "parameter: container = ''\n",
    "import re\n",
    "\n",
    "# Use the header of the covariate file to decide the sample size\n",
    "import pandas as pd\n",
    "N = len(pd.read_csv(covariate_file, sep = \"\\t\",nrows = 1).columns) - 1\n",
    "\n",
    "# Minor allele frequency cutoff. It will overwrite minor allele cutoff.\n",
    "# You may consider setting it to higher for interaction analysis if you have statistical power concerns\n",
    "parameter: maf_threshold = MAC/(2.0*N)\n",
    "\n",
    "# Filtering significant trans associations (for trans_2 workflow)\n",
    "parameter: pvalue_cutoff = \"\"\n",
    "parameter: qvalue_cutoff = \"0.01\"  # default cutoff is 0.01\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))\n",
    "\n",
    "\n",
    "if (str(genotype_file).endswith(\"bed\") or str(genotype_file).endswith(\"pgen\")) and str(phenotype_file).endswith(\"bed.gz\"):\n",
    "    input_files = [[phenotype_file, genotype_file]]\n",
    "    if len(chromosome) > 0:\n",
    "        input_chroms = [int(x) for x in chromosome]\n",
    "    else:\n",
    "        input_chroms = [0]\n",
    "else:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    molecular_pheno_files = pd.read_csv(phenotype_file, sep = \"\\t\")\n",
    "    if \"#chr\" in molecular_pheno_files.columns:\n",
    "        molecular_pheno_files = molecular_pheno_files.groupby(['#chr','path']).size().reset_index(name='count').drop(\"count\",axis = 1).rename(columns = {\"#chr\":\"#id\"})\n",
    "    genotype_files = pd.read_csv(genotype_file,sep = \"\\t\")\n",
    "    genotype_files[\"#id\"] = [x.replace(\"chr\",\"\") for x in genotype_files[\"#id\"].astype(str)] # e.g. remove chr1 to 1\n",
    "    genotype_files[\"#path\"] = genotype_files[\"#path\"].apply(lambda x: adapt_file_path(x, genotype_file))\n",
    "    molecular_pheno_files[\"#id\"] = [x.replace(\"chr\",\"\") for x in molecular_pheno_files[\"#id\"].astype(str)]\n",
    "    input_files = molecular_pheno_files.merge(genotype_files, on = \"#id\")\n",
    "    # Only keep chromosome specified in --chromosome\n",
    "    if len(chromosome) > 0:\n",
    "        input_files = input_files[input_files['#id'].isin(chromosome)]\n",
    "    input_files = input_files.values.tolist()\n",
    "    input_chroms = [x[0] for x in input_files]\n",
    "    input_files = [x[1:] for x in input_files]\n",
    "    if len(name) == 0:\n",
    "        name = f'{path(input_files[0][0]):bnn}' if len(input_files) == 1 else f'{path(input_files[0][0]):bnnn}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## cis-xQTL association testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[cis_1]\n",
    "# parse input file lists\n",
    "# skip nominal association results if the files exists already\n",
    "# This is false by default which means to recompute everything\n",
    "# This is only relevant when the `parquet` files for nominal results exist but not the other files\n",
    "# and you want to avoid computing the nominal results again\n",
    "parameter: skip_nominal_if_exist = False\n",
    "parameter: permutation = True\n",
    "\n",
    "# Extract interaction name\n",
    "var_interaction = interaction\n",
    "if os.path.isfile(interaction):\n",
    "    interaction_s = pd.read_csv(interaction, sep='\\t', index_col=0)\n",
    "    var_interaction = interaction_s.columns[0] # interaction name\n",
    "test_regional_association = permutation and len(var_interaction) == 0\n",
    "\n",
    "input: input_files, group_by = len(input_files[0]), group_with = \"input_chroms\"\n",
    "output_files = dict([(\"parquet\", f'{cwd:a}/{_input[0]:bnn}{\"_%s\" % var_interaction if interaction else \"\"}.cis_qtl_pairs.{\"\" if input_chroms[_index] == 0 else input_chroms[_index]}.parquet'), # This convention is necessary to match the pattern of map_norminal output\n",
    "                     (\"nominal\", f'{cwd:a}/{_input[0]:bnn}{\"\" if input_chroms[_index] == 0 else \"_chr%s\" % input_chroms[_index]}{\"_%s\" % var_interaction if interaction else \"\"}.cis_qtl.pairs.tsv.gz')])\n",
    "if test_regional_association:\n",
    "    output_files[\"regional\"] = f'{cwd:a}/{_input[0]:bnn}{\"\" if input_chroms[_index] == 0 else \"_chr%s\" % input_chroms[_index]}{\"_%s\" % var_interaction if interaction else \"\"}.cis_qtl.regional.tsv.gz'\n",
    "output: output_files\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[\"nominal\"]:bnnn}'\n",
    "python: expand= \"$[ ]\", stderr = f'{_output[\"nominal\"]:nnn}.stderr', stdout = f'{_output[\"nominal\"]:nnn}.stdout' , container = container\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import tensorqtl\n",
    "    from tensorqtl import genotypeio, cis\n",
    "    from scipy.stats import chi2\n",
    "    import multiprocessing as mp\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    ## Define paths\n",
    "    plink_prefix_path = $[_input[1]:nar]\n",
    "    expression_bed = $[_input[0]:ar]\n",
    "    covariates_file = \"$[covariate_file:a]\"\n",
    "    window = $[window]\n",
    "    interaction = \"$[interaction]\"\n",
    "    ## Load Data\n",
    "    phenotype_df, phenotype_pos_df = tensorqtl.read_phenotype_bed(expression_bed)\n",
    "    phenotype_id = phenotype_pos_df.index.name\n",
    "\n",
    "    ## Analyze only the regions listed\n",
    "    if $[region_list.is_file()]:\n",
    "        region = pd.read_csv(\"$[region_list:a]\", comment=\"#\", header=None, sep=\"\\t\" )\n",
    "        phenotype_column = 1 if len(region.columns) == 1 else  $[region_list_phenotype_column]\n",
    "        keep_region = region.iloc[:,phenotype_column-1].to_list()\n",
    "        phenotype_df = phenotype_df[phenotype_df.index.isin(keep_region)]\n",
    "        phenotype_pos_df = phenotype_pos_df[phenotype_pos_df.index.isin(keep_region)]\n",
    "    covariates_df = pd.read_csv(covariates_file, sep='\\t', index_col=0).T\n",
    "    genotype_df, variant_df = genotypeio.load_genotypes(plink_prefix_path, dosages = True)\n",
    "\n",
    "    \n",
    "    ## use custom sample list to subset the covariates data\n",
    "    if $[keep_sample.is_file()]:\n",
    "        sample_list = pd.read_csv(\"$[keep_sample:a]\", comment=\"#\", header=None, names=[\"sample_id\"], sep=\"\\t\")\n",
    "        covariates_df.loc[sample_list.sample_id]\n",
    "\n",
    "    # Read interaction files or extract from covariates file.\n",
    "    var_interaction = interaction\n",
    "    interaction_s = []\n",
    "    if os.path.isfile(interaction):\n",
    "        # update var_interaction and interaction_s\n",
    "        interaction_s = pd.read_csv(interaction, sep='\\t', index_col=0)\n",
    "        interaction_s = interaction_s[interaction_s.index.isin(covariates_df.index)] \n",
    "        var_interaction = interaction_s.columns[0] # interaction name\n",
    "    # check if the interaction term in interaction table is in covariates file, if yes and interaction_s not yet loaded then, extract it out from covariates file\n",
    "    if var_interaction in covariates_df.columns:\n",
    "        # only load from covariate if it has not been loaded yet\n",
    "        if len(interaction_s) == 0:\n",
    "            interaction_s = covariates_df[var_interaction].to_frame()\n",
    "        covariates_df = covariates_df.drop(columns=[var_interaction])\n",
    "    if len(interaction) and len(interaction_s) == 0:\n",
    "        raise ValueError(f\"Cannot find interaction variable or file {interaction}\")\n",
    "\n",
    "    # drop samples that with missing value in iteraction\n",
    "    if len(interaction_s):\n",
    "        interaction_s = interaction_s.dropna() \n",
    "\n",
    "    ## Retaining only common samples\n",
    "    phenotype_df = phenotype_df[np.intersect1d(phenotype_df.columns, covariates_df.index)]\n",
    "    phenotype_df = phenotype_df[np.intersect1d(phenotype_df.columns, genotype_df.columns)] \n",
    "\n",
    "    if len(interaction_s):\n",
    "        phenotype_df = phenotype_df[np.intersect1d(phenotype_df.columns, interaction_s.index)]\n",
    "        interaction_s = interaction_s[interaction_s.index.isin(phenotype_df.columns)]    \n",
    "        interaction_s = interaction_s.loc[phenotype_df.columns]\n",
    "\n",
    "    covariates_df = covariates_df.transpose()[np.intersect1d(phenotype_df.columns, covariates_df.index)].transpose()\n",
    "    \n",
    "    ## To simplify things, there should really not be \"chr\" prefix\n",
    "    phenotype_pos_df.chr = phenotype_pos_df.chr.astype(str).str.replace(\"chr\", \"\")\n",
    "    variant_df.chrom =  variant_df.chrom.astype(\"str\").str.replace(\"chr\", \"\") \n",
    "\n",
    "    ## use custom cis windows list\n",
    "    if $[customized_cis_windows.is_file()]:\n",
    "        cis_list = pd.read_csv(\"$[customized_cis_windows:a]\", comment=\"#\", header=None, names=[\"chr\",\"start\",\"end\",phenotype_id], sep=\"\\t\")\n",
    "        cis_list.chr = cis_list.chr.astype(str).str.replace(\"chr\", \"\")  ## Again to simplify things for chr format concordance.\n",
    "        if cis_list[['chr', 'ID']].duplicated().sum() != 0: # if cis_list is not unique using identifier ['#chr', 'ID']\n",
    "                cis_list = cis_list.groupby(['ID', 'chr']).agg({ # use union start-end position and make cis_list unique\n",
    "                    'start': 'min',\n",
    "                    'end': 'max'\n",
    "                }).reset_index()\n",
    "\n",
    "                cis_list = cis_list[['chr', 'start', 'end', 'ID']]\n",
    "                #cis_list = cis_list.set_index('chr')\n",
    "        \n",
    "        phenotype_pos_df = phenotype_pos_df.reset_index() #move the phenotype id index to a new column of the dataframe\n",
    "        phenotype_df = phenotype_df.reset_index()\n",
    "        # Ensure phenotype_pos_df is a subset of cis_list based on ['#chr', 'ID']\n",
    "        original_count = len(phenotype_pos_df)\n",
    "        phenotype_pos_df = phenotype_pos_df[phenotype_pos_df.set_index(['chr', 'ID']).index.isin(cis_list.set_index([cis_list.columns[0],cis_list.columns[3]]).index)] \n",
    "        phenotype_df = phenotype_df[phenotype_df.set_index('ID').index.isin(cis_list.set_index(cis_list.columns[3]).index)] \n",
    "        phenotype_df = phenotype_df.set_index('ID')\n",
    "        removed_count = original_count - len(phenotype_pos_df)\n",
    "        print(f\"{removed_count} rows were removed from phenotype_pos_df\")\n",
    "\n",
    "        # Merge the dataframes on 'chr' and 'ID', including 'start' and 'end'\n",
    "        phenotype_pos_df = phenotype_pos_df.merge(cis_list[['chr', 'ID', 'start', 'end']], \n",
    "                                        left_on = [\"chr\",phenotype_id],\n",
    "                                        right_on = [cis_list.columns[0],cis_list.columns[3]],\n",
    "                                        suffixes=('_pheno', ''))\n",
    "\n",
    "        # Function to decide whether to keep or rename columns: \n",
    "        # If 'start' and 'end' values are the same in both dataframes, we keep the original columns without suffixes; \n",
    "        # If they're different, we name columns from cis_list with '_cis' suffixes and keep the name still for columns from phentype_pos_df.\n",
    "        # #in some cases (gene expression for eQTLs) the phenotype_id may be in the cis_list file\n",
    "        def rename_if_different(row, col):\n",
    "            if row[f'{col}'] == row[f'{col}_pheno']:\n",
    "                return row[col]\n",
    "            else:\n",
    "                return pd.Series({f'{col}_pheno': row[f'{col}_pheno'], f'{col}_cis': row[col]})\n",
    "        \n",
    "        # Apply the renaming logic\n",
    "        for col in ['start', 'end']:\n",
    "            if f'{col}_pheno' in phenotype_pos_df.columns:# this condition is to not execute when the original phenotype_pos_df don't have start, end but pos column.\n",
    "                temp = phenotype_pos_df.apply(lambda row: rename_if_different(row, col), axis=1)\n",
    "                phenotype_pos_df = phenotype_pos_df.drop(columns=[f'{col}_pheno']).join(temp)\n",
    "                print(f\"Dropped columns due to value mismatch: {col}_pheno\")\n",
    "        \n",
    "        phenotype_pos_df = phenotype_pos_df.set_index(phenotype_id)[[\"chr\",\"start\",\"end\"]] # The final phenotype_pos_df will have three columns(chr, start, end) and index is the phenotype ID\n",
    "        \n",
    "        if len(phenotype_df.index) != len(phenotype_pos_df.index):\n",
    "            raise ValueError(\"cannot uniquely match all the phentoype data in the input to the customized cis windows provided\")\n",
    "        window = 0 # In the updated tensorQTL, by default if there is a customized cis window, the actual cis window will be start - window & end + window, so it is necessary to change the window parameter to 0\n",
    "\n",
    "    ## Read phenotype group if availble\n",
    "    if $[phenotype_group.is_file()]:\n",
    "        group_s = pd.read_csv($[phenotype_group:r], sep='\\t', header=None, index_col=0).squeeze()\n",
    "    else:\n",
    "        group_s = None\n",
    "\n",
    "    ## cis-QTL mapping: nominal associations for all variant-phenotype pairs\n",
    "    if not ($[skip_nominal_if_exist] and $[_output[\"parquet\"].is_file()]):\n",
    "        if len(interaction_s):\n",
    "            cis.map_nominal(genotype_df, variant_df, \n",
    "                    phenotype_df, \n",
    "                    phenotype_pos_df, \n",
    "                    $[_output[\"parquet\"]:nnnr],\n",
    "                    covariates_df=covariates_df,\n",
    "                    interaction_df=interaction_s, \n",
    "                    maf_threshold_interaction=$[maf_threshold],\n",
    "                    window=window,\n",
    "                    group_s=group_s,\n",
    "                    run_eigenmt=True)\n",
    "        else:\n",
    "            cis.map_nominal(genotype_df, variant_df,\n",
    "                phenotype_df,\n",
    "                phenotype_pos_df,\n",
    "                $[_output[\"parquet\"]:nnnr],\n",
    "                covariates_df=covariates_df, \n",
    "                window=window, \n",
    "                maf_threshold=$[maf_threshold],\n",
    "                run_eigenmt=$['False' if permutation else 'True'],\n",
    "                group_s=group_s)\n",
    "\n",
    "    ## Load the parquet and save it as txt\n",
    "    pairs_df = pd.read_parquet($[_output[\"parquet\"]:r])\n",
    "    ## Remove rows whose 'pval_gi' is null for following t_pval_conversion\n",
    "    if len(interaction_s):\n",
    "        pairs_df = pairs_df.dropna(subset=['pval_gi'])\n",
    "    # print general information of parquet\n",
    "    print('Output Information:')\n",
    "    print(\"This is the file containing the immediate output of TensorQTL's map_nominal function \")\n",
    "    print(os.path.getsize($[_output[\"parquet\"]:r]))\n",
    "\n",
    "    ## Adds the group columns to pairs_df, if there is group_s use group_s, else use phenotype_id\n",
    "    if group_s is not None:\n",
    "        pairs_df = pairs_df.merge(pd.DataFrame( {\"molecular_trait_object_id\": group_s}),left_on = \"phenotype_id\", right_index = True)\n",
    "    else:\n",
    "        pairs_df[\"molecular_trait_object_id\"] = pairs_df.phenotype_id\n",
    "    ## if pos in phenotype_pos_df(start distance and end distance are the same), \n",
    "    ## add the column 'end_distance' with the same value as 'start_distance' to avoid mismatch of column and names\n",
    "    if 'end_distance' not in pairs_df.columns:\n",
    "        # Get the position of 'start_distance' column\n",
    "        start_pos = pairs_df.columns.get_loc('start_distance')\n",
    "        # Create a new DataFrame with the same values as start_distance\n",
    "        new_df = pairs_df.copy()\n",
    "        # Insert end_distance column after start_distance\n",
    "        new_df.insert(start_pos + 1, 'end_distance', pairs_df['start_distance'])\n",
    "        pairs_df = new_df\n",
    "\n",
    "    # rename columns\n",
    "    column_map = {'phenotype_id': 'molecular_trait_id'}\n",
    "   \n",
    "    if len(interaction_s):\n",
    "        # calculate genomic inflation factor lambda on interaction \n",
    "        lambda_col_interaction = pairs_df.groupby(\"molecular_trait_object_id\").apply(lambda x: chi2.ppf(1. - np.median(x.pval_gi), 1)/chi2.ppf(0.5,1))\n",
    "        column_map.update({\n",
    "            'pval_g': 'pvalue', 'b_g': 'bhat', 'b_g_se': 'se',\n",
    "            'pval_i': f'pvalue_{var_interaction}',\n",
    "            'b_i': f'bhat_{var_interaction}',\n",
    "            'b_i_se': f'sebhat_{var_interaction}',\n",
    "            'pval_gi': f'pvalue_{var_interaction}_interaction',\n",
    "            'b_gi': f'bhat_{var_interaction}_interaction',\n",
    "            'b_gi_se': f'sebhat_{var_interaction}_interaction'\n",
    "        })\n",
    "    else:\n",
    "        column_map.update({'pval_nominal': 'pvalue', 'slope': 'bhat', 'slope_se': 'sebhat'})\n",
    "    if $[customized_cis_windows.is_file()]:\n",
    "        column_map.update({'start_distance': 'cis_window_start_distance', 'end_distance': 'cis_window_end_distance'})\n",
    "    else:\n",
    "        column_map.update({'start_distance': 'tss_distance', 'end_distance': 'tes_distance'})\n",
    "    pairs_df.rename(columns=column_map, inplace=True)\n",
    "    pairs_df[\"n\"] = len(phenotype_df.columns.values)\n",
    "    pairs_df = variant_df.merge(pairs_df, right_on='variant_id', left_index=True)\n",
    "    pairs_df.rename(columns={'a1': 'a2', 'a0': 'a1'}, inplace=True)\n",
    "    # sort the table if chrom and pos is not in ascending order\n",
    "    if not all(pairs_df['pos'].iloc[i] <= pairs_df['pos'].iloc[i+1] for i in range(len(pairs_df)-1)):\n",
    "        pairs_df = pairs_df.sort_values(by=['chrom', 'pos'])\n",
    "    # save file\n",
    "    pairs_df.to_csv($[_output[\"nominal\"]:nr], sep='\\t', index = None)\n",
    "    # print general information of pairs_df\n",
    "    print('Output Information:')\n",
    "    print(\"Output Rows:\", len(pairs_df))\n",
    "    print(\"Output Columns:\", pairs_df.columns.tolist())\n",
    "    print(\"Output Preview:\", pairs_df.iloc[1:5, 1:10])\n",
    "\n",
    "    if $[test_regional_association]:\n",
    "        # calculate genomic inflation factor lambda for main variant effect \n",
    "        lambda_col = pairs_df.groupby(\"molecular_trait_object_id\").apply(lambda x: chi2.ppf(1. - np.median(x.pvalue), 1)/chi2.ppf(0.5,1))\n",
    "        cis_df = cis.map_cis(genotype_df, \n",
    "                            variant_df, \n",
    "                            phenotype_df,\n",
    "                            phenotype_pos_df,\n",
    "                            covariates_df=covariates_df, \n",
    "                            seed=999, \n",
    "                            window=window, \n",
    "                            maf_threshold = $[maf_threshold],\n",
    "                            group_s=group_s)\n",
    "        cis_df.index.name = \"molecular_trait_id\"\n",
    "        ## Add groups columns for eQTL analysis\n",
    "        if \"group_id\" not in cis_df.columns:\n",
    "            cis_df[\"group_id\"] = cis_df.index\n",
    "            cis_df[\"group_size\"] = 1\n",
    "        cis_df.rename(columns={\"group_id\": \"molecular_trait_object_id\", \"group_size\": \"n_traits\", \n",
    "                        'start_distance': 'tss_distance', 'end_distance': 'tes_distance',\n",
    "                        \"num_var\": \"n_variants\", \"pval_nominal\": \"p_nominal\", \n",
    "                        'slope': 'bhat', 'slope_se': 'sebhat',\n",
    "                        \"pval_true_df\": \"p_true_df\", \"pval_perm\": \"p_perm\", \"pval_beta\": \"p_beta\"}, inplace = True)\n",
    "        cis_df = cis_df.assign(genomic_inflation = lambda dataframe : dataframe[\"molecular_trait_object_id\"].map(lambda molecular_trait_object_id:lambda_col[molecular_trait_object_id]))\n",
    "        # merge cis_df with variant_df\n",
    "        cis_df = variant_df.merge(cis_df, right_on='variant_id', left_index=True)\n",
    "        cis_df.rename(columns={'a1': 'a2', 'a0': 'a1'}, inplace=True)\n",
    "        # sort the table if chrom and pos is not in ascending order\n",
    "        if not all(cis_df['pos'].iloc[i] <= cis_df['pos'].iloc[i+1] for i in range(len(cis_df)-1)):\n",
    "            cis_df = cis_df.sort_values(by=['chrom', 'pos'])\n",
    "        # save file\n",
    "        cis_df.to_csv(str($[_output[\"nominal\"]:nnnr])+str('.regional.tsv'), sep='\\t', index = None)\n",
    "        # print general information of cis_df\n",
    "        print('Output Information:')\n",
    "        print(\"Output Rows:\", len(cis_df))\n",
    "        print(\"Output Columns:\", cis_df.columns.tolist())\n",
    "        print(\"Output Preview:\", cis_df.iloc[0:5, 0:10])\n",
    "\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[\"nominal\"]:nnn}.stderr', stdout = f'{_output[\"nominal\"]:nnn}.stdout', container = container\n",
    "    library(purrr)\n",
    "    library(tidyr)\n",
    "    library(readr)\n",
    "    library(dplyr)\n",
    "    library(qvalue)\n",
    "  \n",
    "    pairs_df = read_delim($[_output[\"nominal\"]:nr,], delim = '\\t')\n",
    "    compute_qvalues <- function(pvalues) {\n",
    "        tryCatch({\n",
    "            if(length(pvalues) < 2) {\n",
    "                return(pvalues)\n",
    "            } else {\n",
    "                return(qvalue(pvalues)$qvalues)\n",
    "            }\n",
    "        }, error = function(e) {\n",
    "            message(\"Too few p-values to calculate qvalue, fall back to BH\")\n",
    "            qvalue(pvalues, pi0 = 1)$qvalues\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    var_interaction <- \"$[interaction]\"\n",
    "    # Check if 'interaction' is a file\n",
    "    if (file.exists(var_interaction)) {\n",
    "        # Read the file into 'interaction_s' dataframe\n",
    "        interaction_s <- read.delim(var_interaction, row.names = 1)\n",
    "        # Update 'var_interaction' to the first column name of 'interaction_s'\n",
    "        var_interaction <- names(interaction_s)[1]\n",
    "    }\n",
    "\n",
    "    if (is.null(var_interaction) || var_interaction == \"\") {\n",
    "        pairs_df = pairs_df %>% group_by(molecular_trait_id) %>% mutate(qvalue = compute_qvalues(pvalue))\n",
    "    } else {\n",
    "        pairs_df = pairs_df %>% group_by(molecular_trait_id) %>% mutate(qvalue_main = compute_qvalues(pvalue), qvalue_interaction = compute_qvalues($[\"pvalue_%s_interaction\" % var_interaction]))         \n",
    "    }\n",
    "\n",
    "    pairs_df %>% write_delim($[_output[\"nominal\"]:nr],\"\\t\")\n",
    "  \n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[\"nominal\"]:nnn}.stderr', stdout = f'{_output[\"nominal\"]:nnn}.stdout', container = container\n",
    "        bgzip --compress-level 9 $[_output[\"nominal\"]:n] \n",
    "        tabix -S 1 -s 1 -b 2 -e 2 $[_output[\"nominal\"]]\n",
    "\n",
    "done_if(not test_regional_association)\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[\"nominal\"]:nnn}.stderr', stdout = f'{_output[\"nominal\"]:nnn}.stdout', container = container\n",
    "        bgzip --compress-level 9 $[_output[\"nominal\"]:nnn].regional.tsv\n",
    "        tabix -S 1 -s 1 -b 2 -e 2 $[_output[\"nominal\"]:nnn].regional.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[cis_2]\n",
    "done_if(\"regional\" not in _input.labels)\n",
    "input: group_by = \"all\"\n",
    "output_file_prefix = name if len(_input[\"nominal\"]) > 1 else f'{_input[\"nominal\"][0]:bnnnn}'\n",
    "output: f'{cwd}/{output_file_prefix}.cis_qtl_regional_significance.tsv.gz',\n",
    "        f'{cwd}/{output_file_prefix}.cis_qtl_regional_significance.summary.txt'\n",
    "input_files = [str(x) for x in _input[\"regional\"]]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\")\n",
    "    library(\"qvalue\")\n",
    "    emprical_pd = tibble(map(c($[_input[\"regional\"]:r,]), ~read_delim(.x,\"\\t\")))%>%unnest()\n",
    "    emprical_pd[\"q_beta\"] = tryCatch(qvalue(emprical_pd$p_beta)$qvalue, error = function(e){print(\"Too few pvalue to calculate qvalue, fall back to BH\") \n",
    "                                                                                              qvalue(emprical_pd$p_beta,pi0 = 1 )$qvalue})  \n",
    "\n",
    "    emprical_pd[\"q_perm\"] = tryCatch(qvalue(emprical_pd$p_perm)$qvalue, error = function(e){print(\"Too few pvalue to calculate qvalue, fall back to BH\") \n",
    "                                                                                              qvalue(emprical_pd$p_perm,pi0 = 1 )$qvalue})\n",
    "    emprical_pd[\"fdr_beta\"] = p.adjust(emprical_pd$p_beta,\"fdr\")    \n",
    "    emprical_pd[\"fdr_perm\"] = p.adjust(emprical_pd$p_perm,\"fdr\")   \n",
    "\n",
    "\n",
    "    # Calculate the global nominal p-value threshold based on q_beta at FDR 0.05\n",
    "    if (!all(is.na(emprical_pd$p_beta))) {\n",
    "      lb <- emprical_pd %>% \n",
    "        filter(q_beta <= 0.05) %>% \n",
    "        pull(p_beta) %>% \n",
    "        sort()\n",
    "      \n",
    "      ub <- emprical_pd %>% \n",
    "        filter(q_beta > 0.05) %>% \n",
    "        pull(p_beta) %>% \n",
    "        sort()\n",
    "      \n",
    "      if (length(lb) > 0) {\n",
    "        lb_val <- tail(lb, 1)\n",
    "        threshold <- if (length(ub) > 0) (lb_val + head(ub, 1)) / 2 else lb_val\n",
    "        message(sprintf(\"min p-value threshold @ FDR 0.05: %g\", threshold))\n",
    "        \n",
    "        emprical_pd <- emprical_pd %>% \n",
    "          mutate(p_nominal_threshold = qbeta(threshold, beta_shape1, beta_shape2))\n",
    "      }\n",
    "    }\n",
    "\n",
    "    summary = tibble(\"fdr_perm_0.05\" =  sum(emprical_pd[\"fdr_perm\"] < 0.05), \n",
    "                      \"fdr_beta_0.05\" = sum(emprical_pd[\"fdr_beta\"] < 0.05),\n",
    "                      \"q_perm_0.05\" = sum(emprical_pd[\"q_perm\"] < 0.05),\n",
    "                      \"q_beta_0.05\" = sum(emprical_pd[\"q_beta\"] < 0.05),\n",
    "                      \"fdr_perm_0.01\" =  sum(emprical_pd[\"fdr_perm\"] < 0.01), \n",
    "                      \"fdr_beta_0.01\" = sum(emprical_pd[\"fdr_beta\"] < 0.01),\n",
    "                      \"q_perm_0.01\" = sum(emprical_pd[\"q_perm\"] < 0.01),\n",
    "                      \"q_beta_0.01\" = sum(emprical_pd[\"q_beta\"] < 0.01)  )\n",
    "    emprical_pd%>%write_delim(\"$[_output[0]]\",\"\\t\")\n",
    "    summary%>%write_delim(\"$[_output[1]]\",\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Trans-xQTL association testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For transQTL analysis, if you output all the p-values for many genes (default setting) it is suggested to provide the largest memory and CPU threads available on a compute node. eg 250G and >32 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[trans_1]\n",
    "input: input_files, group_by = len(input_files[0]), group_with = \"input_chroms\"\n",
    "output: nominal = f'{cwd:a}/{_input[0]:bnn}{\"\" if input_chroms[_index] == 0 else \"_%s\" % input_chroms[_index]}.trans_qtl.pairs.tsv.gz'\n",
    "\n",
    "parameter: batch_size = 10000\n",
    "parameter: pval_threshold = 1.0\n",
    "# Permutation testing is incorrect when the analysis is done by chrom\n",
    "parameter: permutation = False\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand= \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container =container\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorqtl\n",
    "    from tensorqtl import genotypeio, trans\n",
    "    from scipy.stats import chi2\n",
    "\n",
    "    ## Define paths\n",
    "    plink_prefix_path = $[_input[1]:nar]\n",
    "    expression_bed = $[_input[0]:ar]\n",
    "    covariates_file = \"$[covariate_file:a]\"\n",
    "    window = $[window]\n",
    "    ## Loading Data\n",
    "    phenotype_df, phenotype_pos_df = tensorqtl.read_phenotype_bed(expression_bed)\n",
    "    phenotype_id = phenotype_pos_df.index.name\n",
    "\n",
    "    ## Analyze only the regions listed\n",
    "    if $[region_list.is_file()]:\n",
    "        region = pd.read_csv(\"$[region_list:a]\", comment=\"#\", header=None,sep=\"\\t\")\n",
    "        phenotype_column = 1 if len(region.columns) == 1 else  $[region_list_phenotype_column]\n",
    "        keep_region = region.iloc[:,phenotype_column-1].to_list()\n",
    "        phenotype_df = phenotype_df[phenotype_df.index.isin(keep_region)]\n",
    "        phenotype_pos_df = phenotype_pos_df[phenotype_pos_df.index.isin(keep_region)]\n",
    "\n",
    "    ## use custom cis windows\n",
    "    if $[customized_cis_windows.is_file()]:\n",
    "        cis_list = pd.read_csv(\"$[customized_cis_windows:a]\", comment=\"#\", header=None, names=[\"chr\",\"start\",\"end\",phenotype_id], sep=\"\\t\")\n",
    "        phenotype_pos_df_reset = phenotype_pos_df.reset_index() #move the phenotype id index to a new column of the dataframe\n",
    "        phenotype_pos_df = phenotype_pos_df_reset.merge(cis_list, left_on = [\"chr\",phenotype_id],right_on = [cis_list.columns[0],cis_list.columns[3]])#in some cases (gene expression for eQTLs) the phenotype_id may be in the cis_list file\n",
    "        if len(phenotype_df.index) - phenotype_pos_df_reset[~phenotype_pos_df_reset[phenotype_id].isin(cis_list[phenotype_id])].shape[0]!= len(phenotype_pos_df.index):\n",
    "            raise ValueError(\"cannot uniquely match all the phentoype data in the input to the customized cis windows provided\")\n",
    "        phenotype_pos_df = phenotype_pos_df.set_index(phenotype_id)[[\"chr\",\"start\",\"end\"]] # The final phenotype_pos_df will have three columns(chr, start, end) and index is the phenotype ID\n",
    "        window = 0 # In the updated tensorQTL, by default if there is a customized cis window, the actual cis window will be start - window & end + window, so it is necessary to change the window parameter to 0\n",
    "        ## Retaining only traits in cis_list\n",
    "        if phenotype_pos_df_reset[~phenotype_pos_df_reset[phenotype_id].isin(cis_list[phenotype_id])].shape[0] != 0:\n",
    "            phenotype_df = phenotype_df.loc[phenotype_df.index.isin(cis_list[phenotype_id])]\n",
    "\n",
    "    covariates_df = pd.read_csv(covariates_file, sep='\\t', index_col=0).T\n",
    "    ## use custom sample list to subset the covariates data\n",
    "    if $[keep_sample.is_file()]:\n",
    "        sample_list = pd.read_csv(\"$[keep_sample:a]\", comment=\"#\", header=None, names=[\"sample_id\"], sep=\"\\t\")\n",
    "        covariates_df.loc[sample_list.sample_id]\n",
    "    pr = genotypeio.PlinkReader(plink_prefix_path)\n",
    "    genotype_df = pr.load_genotypes()\n",
    "    variant_df = pr.bim.set_index('snp')[['chrom', 'pos','a0','a1']]\n",
    "\n",
    "    ## Retaining only common samples\n",
    "    phenotype_df = phenotype_df[np.intersect1d(phenotype_df.columns, covariates_df.index)]\n",
    "    phenotype_df = phenotype_df[np.intersect1d(phenotype_df.columns, genotype_df.columns)]\n",
    "    covariates_df = covariates_df.transpose()[np.intersect1d(phenotype_df.columns, covariates_df.index)].transpose()\n",
    "\n",
    "    ## To simplify things, there should really not be \"chr\" prefix\n",
    "    phenotype_pos_df.chr = [x.replace(\"chr\",\"\") for x in phenotype_pos_df.chr]\n",
    "    variant_df.chrom = [x.replace(\"chr\",\"\") for x in variant_df.chrom]\n",
    "\n",
    "    ## Trans analysis\n",
    "    trans_df = trans.map_trans(genotype_df, \n",
    "                            phenotype_df,\n",
    "                            covariates_df, \n",
    "                            batch_size=$[batch_size],\n",
    "                            return_sparse=True, \n",
    "                            return_r2 = True,\n",
    "                            pval_threshold=$[pval_threshold], \n",
    "                            maf_threshold=$[maf_threshold])\n",
    "\n",
    "    ## Filter out cis signal, again if customized cis windows are used, the windows is [start-win,end + win] where win = 0, else it is [start - win, start + win]\n",
    "    trans_df = trans.filter_cis(trans_df, phenotype_pos_df, variant_df, window=window)   \n",
    "\n",
    "    ## Permutation\n",
    "    if $['True' if permutation else 'False']:\n",
    "        perm_df = trans.map_permutations(genotype_df, covariates_df, batch_size=$[batch_size],\n",
    "                             maf_threshold=$[maf_threshold])\n",
    "        trans.apply_permutations(perm_df,trans_df)\n",
    "\n",
    "    ## Output\n",
    "    trans_df.rename(columns={\"phenotype_id\": \"molecular_trait_id\", \n",
    "                            \"pval\": \"pvalue\", \n",
    "                            \"b\": \"bhat\", \"b_se\": \"sebhat\"}, inplace=True)\n",
    "    trans_df[\"n\"] = len(phenotype_df.columns.values)\n",
    "    trans_df = variant_df.merge(trans_df, right_on='variant_id', left_index=True)\n",
    "    trans_df.rename(columns={'a1': 'a2', 'a0': 'a1'}, inplace=True)\n",
    "    # genomic inflation factor\n",
    "    lambda_col = trans_df.groupby(\"molecular_trait_id\").apply(lambda x: chi2.ppf(1. - np.median(x.pvalue), 1)/chi2.ppf(0.5,1))\n",
    "    # Convert the Series to a DataFrame\n",
    "    lambda_col = lambda_col.reset_index()\n",
    "    lambda_col.columns = ['molecular_trait_id', 'genomic_inflation_lambda']\n",
    "    lambda_col.to_csv(\"$[_output:nnn].genomic_inflation.tsv.gz\", sep='\\t', index = None, compression={'method': 'gzip', 'compresslevel': 9})\n",
    "\n",
    "    # sort the table if chrom and pos is not in ascending order\n",
    "    if not all(trans_df['pos'].iloc[i] <= trans_df['pos'].iloc[i+1] for i in range(len(trans_df)-1)):\n",
    "        trans_df = trans_df.sort_values(by=['chrom', 'pos'])\n",
    " \n",
    "    # print general information of trans_df\n",
    "    print('Output Information:')\n",
    "    print(\"Output Rows:\", len(trans_df))\n",
    "    print(\"Output Columns:\", trans_df.columns.tolist())\n",
    "    print(\"Output Preview:\", trans_df.iloc[0:5, 0:10])\n",
    "    \n",
    "    # save output\n",
    "    trans_df.to_csv($[_output[\"nominal\"]:nr], sep='\\t', index = None)\n",
    "\n",
    "\n",
    "R: expand= \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container =container\n",
    "    if ($['FALSE' if permutation and pval_threshold < 1.0 else 'TRUE']){ \n",
    "      library(purrr)\n",
    "      library(tidyr)\n",
    "      library(readr)\n",
    "      library(dplyr)\n",
    "      library(qvalue)\n",
    "      # calculate qvalue\n",
    "      pairs_df = read_delim($[_output[\"nominal\"]:nr,], delim = '\\t')  \n",
    "      pairs_df = pairs_df %>% group_by(molecular_trait_id) %>% mutate(qvalue = qvalue(pvalue)$qvalues)\n",
    "      pairs_df %>% write_delim($[_output[\"nominal\"]:nr],\"\\t\")\n",
    "    } \n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[\"nominal\"]:nnn}.stderr', stdout = f'{_output[\"nominal\"]:nnn}.stdout', container = container\n",
    "        bgzip --compress-level 9 $[_output[\"nominal\"]:n]\n",
    "        tabix -S 1 -s 1 -b 2 -e 2 $[_output[\"nominal\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[trans_2]\n",
    "# This workflow filters significant trans-QTL associations based on p-value or q-value cutoffs\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{name}_{(\"qvalue_%\" % qvalue_cutoff) if qvalue_cutoff else (\"pvalue_%s\" % pvalue_cutoff)}_trans_pairs.tsv'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand=\"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container\n",
    "    #!/bin/bash\n",
    "    set -e\n",
    "    home_dir=\"$[cwd]\"\n",
    "    pvalue_cutoff=\"$[pvalue_cutoff]\"\n",
    "    qvalue_cutoff=\"$[qvalue_cutoff]\"\n",
    "    first_file=$(ls \"${home_dir}\"/*trans_qtl.pairs.tsv.gz | head -n 1)\n",
    "    \n",
    "    # Determine cutoff value \n",
    "    if [ -n \"$pvalue_cutoff\" ] && [ -n \"$qvalue_cutoff\" ]; then\n",
    "        echo \"Both p-value and q-value cutoffs provided, using q-value cutoff\"\n",
    "        cutoff_value=\"$qvalue_cutoff\"\n",
    "        cutoff_col=$(zcat \"$first_file\" | awk -F '\\t' -v col='qvalue' 'NR==1{for (i=1; i<=NF; i++) if ($i==col) {print i;exit}}')\n",
    "    elif [ -n \"$pvalue_cutoff\" ]; then\n",
    "        cutoff_value=\"$pvalue_cutoff\"\n",
    "        cutoff_col=$(zcat \"$first_file\" | awk -F '\\t' -v col='pvalue' 'NR==1{for (i=1; i<=NF; i++) if ($i==col) {print i;exit}}')\n",
    "    elif [ -n \"$qvalue_cutoff\" ]; then\n",
    "        cutoff_value=\"$qvalue_cutoff\"\n",
    "        cutoff_col=$(zcat \"$first_file\" | awk -F '\\t' -v col='qvalue' 'NR==1{for (i=1; i<=NF; i++) if ($i==col) {print i;exit}}')\n",
    "    else\n",
    "        echo \"No cutoff provided, using default q-value cutoff of 0.01\"\n",
    "        cutoff_value=\"0.01\"\n",
    "        cutoff_col=$(zcat \"$first_file\" | awk -F '\\t' -v col='qvalue' 'NR==1{for (i=1; i<=NF; i++) if ($i==col) {print i;exit}}')\n",
    "    fi\n",
    "    \n",
    "    echo \"Using column $cutoff_col with cutoff value $cutoff_value\"\n",
    "\n",
    "    output_file=\"$[_output[0]]\"\n",
    "    mkdir -p $(dirname \"$output_file\")\n",
    "    zcat \"$first_file\" | head -n 1 > \"$output_file\"\n",
    "    \n",
    "    for gz_file in \"${home_dir}\"/*trans_qtl.pairs.tsv.gz; do\n",
    "        echo \"Processing $gz_file...\"\n",
    "        # Append significant data from each .gz file to the collective file\n",
    "        zcat \"$gz_file\" | awk -v cutoff=\"$cutoff_value\" -v col=\"$cutoff_col\" 'NR == 1 {next} NR > 1 && $col < cutoff' >> \"$output_file\"\n",
    "    done\n",
    "    \n",
    "    echo \"Total significant associations found: $(wc -l < \"$output_file\") (including header)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.23.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
