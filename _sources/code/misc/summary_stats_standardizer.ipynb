{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "considerable-vacation",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Summary statistics standardization and export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9ed4e-2bd0-49ae-9c42-658cb9711cd4",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This pipeline module contains codes to process summary statistics from conventional QTL association scan to standard formats for public distribution. It will also export multiple QTL studies to formats easily accessible for data integration methods to query and analyze the summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-license",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview of design\n",
    "\n",
    "### Individual xQTL studies\n",
    "\n",
    "1. Reorganize the QTL marginal statistics to standard formats (see section `Column name standardization` for details).\n",
    "2. Report separately cis-QTL and trans-QTL. Additionally report filtered results, e.g. QTLs that survived multiple-testing correction.\n",
    "3. Our column conventions are based on studies such as GTEx and eQTL category. Our design is \"modular\" in the sense that we do not provide information that could be trivially annotated after, such as **rsID, gene symbol, gene biotype, gene start and end positions**; or for information that can be inferred from other columns such as type of allele (SNP or INDEL).\n",
    "4. We use GRCh38 reference allele and alternative allele, and the effect allele is adjusted, as necessary, to the alternative allele.\n",
    "5. Summary statistics will be in conventional TSV format which can be converted to [GWAS-VCF format](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02248-0) as needed.\n",
    "\n",
    "### Multiple xQTL integration\n",
    "\n",
    "- We will **NOT** standardize integrative results. Instead we will distribute the original outputs from the data integration methods we chose. \n",
    "- For meta-analysis we'll rely on output from [METASOFT](http://genetics.cs.ucla.edu/meta/) using `effect_id` as SNP ID, created from `variant`, `molecular_trait_id` and `molecular_trait_object_id` as needed.\n",
    "\n",
    "To build internal database for multiple xQTL data-sets, we:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-botswana",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. Include variants that presents in at least one xQTL. \n",
    "2. Unify allele strand and frequency flips to GRCh38 reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c32c06-80fd-45c5-8d0a-69a9464cdd5c",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Column name standardization\n",
    "\n",
    "The header of actual output sumstat depends on how we configure it (see section `Input` for details). However, all of them will have the column of `chromosome, position, ref, alt, variant_id, beta, se, pvalue`. \n",
    "\n",
    "### Software (input) headers\n",
    "\n",
    "For example, when the input sumstat is from TensorQTL, the column specification is:\n",
    "\n",
    "- GENE: Molecular trait identifier.(gene)\n",
    "- CHR: Variant chromosome.\n",
    "- POS: Variant chromosomal position (basepairs).\n",
    "- A0: Variant reference allele (A, C, T, or G).\n",
    "- A1: Variant alternate allele.\n",
    "- TSS_D: Distance of the SNP to the gene transcription start site (TSS)\n",
    "- AF: The allele frequency of this SNPs\n",
    "- MA_SAMPLES: Number of samples carrying the minor allele\n",
    "- MA_COUNT: Total number of minor alleles across individuals\n",
    "- P: Nominal P-value from linear regression\n",
    "- STAT: Slope of the linear regression\n",
    "- SE: Standard error of beta\n",
    "\n",
    "when the input sumstat is from APEX, the column specification is:\n",
    "\n",
    "- GENE: Molecular trait identifier.(gene)\n",
    "- CHR: Variant chromosome.\n",
    "- POS: Variant chromosomal position (basepairs).\n",
    "- A0: Variant reference allele (A, C, T, or G).\n",
    "- A1: Variant alternate allele.\n",
    "- P: Nominal P-value from linear regression\n",
    "- STAT: Slope of the linear regression\n",
    "- SE: Standard error of beta\n",
    "\n",
    "### Our effect level summary\n",
    "\n",
    "Our proposed xQTL summary statistics fields should include (cf. [our xQTL format draft V3, Jan 2021](https://www.niagads.org/adsp/content/xqtl-fileformats-110921v3sharedxlsx), [eQTL catalog](https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/tabix/Columns.md), [GTEx](https://www.gtexportal.org/home/datasets), [metaBrain](https://www.metabrain.nl/)):\n",
    "\n",
    "* **variant** - The variant ID (chromosome_position_ref_alt) e.g. chr19_226776_C_T. Based on GRCh38 coordinates and reference genome, with 'chr' prefix added to the chromosome number.\n",
    "* **chromosome** - GRCh38 chromosome name of the variant (e.g. 1,2,3 ...,X).\n",
    "* **position** - GRCh38 position of the variant.\n",
    "* **ref** - GRCh38 reference allele.\n",
    "* **alt** - GRCh38 alternative allele (also the effect allele).\n",
    "* **imputation_quality** - Optional imputation quality score from the imputation software, can be replaced with NA if not available.\n",
    "* **molecular_trait_object_id** - For phenotypes with multiple correlated alternatives (multiple alternative transcripts or exons within a gene, multple alternative promoters in txrevise, multiple alternative intons in Leafcutter), this defines the level at which the phenotypes were aggregated. Permutation p-values are calculated across this set of alternatives.  \n",
    "* **molecular_trait_id** - ID of the molecular trait used for QTL mapping. Depending on the quantification method used, this can be either a gene id, exon id, transcript id or a txrevise promoter, splicing or 3'end event id. Examples: ENST00000356937, ENSG00000008128.  \n",
    "* **maf** - Minor allele frequency within a QTL mapping context (e.g. cell type or tissues within a study).\n",
    "* **beta** - Regression coefficient from the linear model.\n",
    "* **se** - Standard error of the beta.\n",
    "* **pvalue** - Nominal p-value of association between the variant and the molecular trait.\n",
    "* **n** - Total number of samples without missing data.\n",
    "* **ac** - Count of the alternative allele. \n",
    "* **ma_samples** - Number of samples carrying at least one copy of the minor allele.\n",
    "\n",
    "### Trait level QTL summary (multiple-testing corrected)\n",
    "\n",
    "* **molecular_trait_object_id** \n",
    "* **molecular_trait_id** \n",
    "* **n_traits** - The number of molecular traits over which permutation p-values were calculated (e.g. the number of transcripts per gene). Note that the permutations are performed accross all molecular traits within the same molecular trait object (e.g. all transcripts of a gene) and the results are reported for the most significant variant and molecular trait pair. \n",
    "* **n_variants** - number of genetic variants tested within the cis region of the molecular trait.\n",
    "* **variant** \n",
    "* **chromosome** - GRCh38 chromosome name of the variant (e.g. 1,2,3 ...,X).\n",
    "* **position** - GRCh38 position of the variant.\n",
    "* **ref** - GRCh38 reference allele.\n",
    "* **alt** - GRCh38 alternative allele (also the effect allele).\n",
    "* **p_perm** - Empirical p-value calculated from 1000 permutations.\n",
    "* **p_beta** - Estimated empirical p-value based on the beta distribution. This is the column that you want to use for filtering the results. See the FastQTL [paper](http://dx.doi.org/10.1093/bioinformatics/btv722) for more details. \n",
    "* **qvalue** - FDR based on Storey's q-value.\n",
    "\n",
    "Other summary:\n",
    "\n",
    "- Quantiles of molecular phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-renewal",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Some technical notes\n",
    "\n",
    "1. If there are duplicated INDELs in the summary statistics, they will be removed. For example, two SNPs at 10000 on chr1. one's `A0` is `T`, and `A1` is `TC`. Whereas the other one's `A0` is `TC`, and `A1` is `T`. Both of them will be removed. More about INDEL issues(https://github.com/statgenetics/UKBB_GWAS_dev/issues/81#issuecomment-1015556800). For SNPs, `A0` and `A1` can be easily standardized to ref/alt in GRCh38 reference genome.\n",
    "2. If duplicated `chr:pos` (GWAS) or `gene:chr:pos` (TWAS) exist, run a recursive match for each pair of them between two summary statistic files (`query`(each of inputs) and `subject` (target file)). \n",
    "3. under the same `chr:pos` or `gene:chr:pos`, The variants' `A0` and `A1` are matched by exact, flip, reverse, or flip+reverse models. Only one of them is `True`, the variant in two files are matched. If they are matched by flip or flip+reverse, the sign of `query`'s `STAT` will be inversed. And the `query`'s `A0` and `A1` will be the same as the `subject`'s `A0` and `A1`.  **FIXME: should we standardize it to GRCh38 first?**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-concert",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Make sure you install the pre-requisited before running this notebook:\n",
    "\n",
    "```\n",
    "pip install cugg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-reconstruction",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "- `--cwd`, the path of working directory\n",
    "- `--yml_path`, the path of yaml file\n",
    "- `--keep-ambiguous`, boolean. default False. if add --keep-ambiguous parameter, keep ambiguous alleles which can not be decided from flip or reverse, such as A/T or C/G. Otherwise, remove them. \n",
    "- `--intersect`, boolean. default False. if add --intersect parameter, output intersect SNPs in all input files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-rapid",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### The minimal format of the input yaml file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-jurisdiction",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For GWAS summary statistics, \n",
    "\n",
    "```\n",
    "INPUT:\n",
    "  - ./data/testflip/*.gz:\n",
    "        build: GRCh38\n",
    "        variant: chromosome, position, ref, alt\n",
    "        chromosome: CHR\n",
    "        position: POS\n",
    "        ref: A0\n",
    "        alt: A1\n",
    "        beta: BETA\n",
    "        se: SE\n",
    "        pvalue: P\n",
    "  - ./data/testflip/flip/snps500_flip.regenie.snp_stats.gz:\n",
    "  \n",
    "TARGET: \n",
    "  - ./data/testflip/snps500.regenie.snp_stats.gz:\n",
    "        build: GRCh38\n",
    "        variant: chromosome, position, ref, alt\n",
    "        chromosome: CHR\n",
    "        position: POS\n",
    "        ref: A0\n",
    "        alt: A1\n",
    "        beta: BETA\n",
    "        se: SE\n",
    "        pvalue: P\n",
    "OUTPUT: data/testflip/output/\n",
    "```\n",
    "\n",
    "For xQTL summary statistics, `molecular_trait_object_id` is required because a variant can be made association with multiple molecular traits. \n",
    "\n",
    "```\n",
    "INPUT:\n",
    "  - data/twas/*.txt:\n",
    "        build: GRCh38\n",
    "        variant: chromosome, position, ref, alt\n",
    "        chromosome: CHR\n",
    "        position: POS\n",
    "        ref: A0\n",
    "        alt: A1\n",
    "        molecular_trait_object_id: GENE\n",
    "        beta: BETA\n",
    "        se: SE\n",
    "        pvalue: P \n",
    "TARGET: \n",
    "  - data/twas/DLPFC.chr6.mol_phe.cis_long_table.reformated.txt:\n",
    "        build: GRCh38\n",
    "        variant: chromosome, position, ref, alt\n",
    "        chromosome: CHR\n",
    "        position: POS\n",
    "        ref: A0\n",
    "        alt: A1\n",
    "        molecular_trait_object_id: GENE\n",
    "        beta: BETA\n",
    "        se: SE\n",
    "        pvalue: P \n",
    "OUTPUT: ../data/twas/output/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-hierarchy",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "There are three parts in the input yaml file.\n",
    "- INPUT\n",
    "   - A list of yml file, as the output from yml_generator, each yml file documents a set of input\n",
    "       - the input summary statistic files with the column names in below. \n",
    "       - the input files can be from multiple directory and from different format. The input paths must follow the rules related to Unix shell. the format is to pair the column names with required keys. If not provided, the column names of the input file will be considered as the default keys.\n",
    "       - The input summary statistic file cannot have duplicated chr:pos\n",
    "       - The input summary statstic file cannot have # in its header\n",
    "       -`variant` in yml is the rule to generate a unique identifier for each SNP, the content of variant ID shall be a combination of other columns such as chrom, position, ref, alt, build, but not taken from existing id columns in the original file.\n",
    "- TARGET\n",
    "   - The target file is a reference summary statistic file or a file with at least variant ID relevant columns. When provided with standard `chr, pos, ref, alt` based on GRCh38, it can serve the purpose to standardize the REF/ALT alleles.\n",
    "- OUTPUT\n",
    "   - the path of an output directory for new summary statistic files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe16026-54d2-4a56-9855-c681ae411d53",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "New summary statistic files with common SNPs in all input files. the sign of statistics has been corrected to make it consistent in different data.\n",
    "   - for each input sumstat file, a standardized version of it will be generated.\n",
    "   - The generated sumstat files will have header standardized header names. The minimal set of headers will be \\\"chromosome, position, ref, alt, variant_id, beta, se, pvalue\\\"\n",
    "   - The generated sumstat files will be in gz format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-excitement",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Memory usage\n",
    "For merging two sumstat with ~85000 rows and of size of ~5MB, 1 GB of memory is needed \n",
    "\n",
    "For merging two sumstat with ~2000000 rows and of size of ~1 GB, at least 50 GB of memory is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-injury",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-technique",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "sos run ./summary_stats_merger.ipynb --cwd data --yml_list data/yml_list.txt --keep-ambiguous --intersect\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-precipitation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory where output will be saved to\n",
    "parameter: cwd = path(\"output\")\n",
    "## path to a list of yml file , with columns #chr and dir\n",
    "parameter: yml_list = path\n",
    "import pandas as pd\n",
    "yml_path = pd.read_csv(yml_list,sep = \"\\t\").values.tolist()\n",
    "#if add --keep-ambiguous parameter, keep ambiguous alleles which can not be decided from flip or reverse, such as A/T or C/G. Otherwise, remove them.\n",
    "parameter: keep_ambiguous = False\n",
    "# if add --intersect parameter, output intersect SNPs in all input files.\n",
    "parameter: intersect = False\n",
    "# Containers that contains the necessary packages\n",
    "parameter: container = \"\"\n",
    "parameter: numThreads = 1\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Walltime \n",
    "parameter: walltime = '5h'\n",
    "parameter: mem = '3G'\n",
    "# The directory of the output sumstat\n",
    "parameter: sumstat_list = path\n",
    "sumstat_path = pd.read_csv(sumstat_list,sep = \"\\t\").drop(columns=\"#chr\").values.tolist()\n",
    "name = pd.read_csv(sumstat_list,sep = \"\\t\").drop(columns=\"#chr\").columns.values.tolist()\n",
    "## Whether to rename the Chr name.\n",
    "parameter: remame = False\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-northwest",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "frequent-ballot",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_1 (export utils script)]\n",
    "depends: Py_Module('cugg')\n",
    "output: f'{cwd:a}/utils.py'\n",
    "report: expand = '${ }', output=f'{cwd:a}/utils.py'\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from cugg.sumstat import read_sumstat\n",
    "    from cugg.utils import *\n",
    "    ## Running functions\n",
    "    def unify_sumstat(yml,keep_ambiguous,intersect):\n",
    "        #parse yaml\n",
    "        yml = load_yaml(yml)\n",
    "        input_dict = parse_input(yml['INPUT'])\n",
    "        target_dict = yml['TARGET']\n",
    "        output_path = yml['OUTPUT'][0]\n",
    "        lst_sumstats_file = [ os.path.basename(i) for i in input_dict.keys()]\n",
    "        print('Total number of sumstats: ',len(lst_sumstats_file))\n",
    "        if len(set(lst_sumstats_file))<len(lst_sumstats_file):\n",
    "            raise Exception(\"There are duplicated names in {}\".format(lst_sumstats_file))\n",
    "        #read all sumstats\n",
    "        print(input_dict)\n",
    "        lst_sumstats = {os.path.basename(i):read_sumstat(i,j,) for i,j in input_dict.items()}\n",
    "        nqs = []\n",
    "        #Readin the reference target file: Using one of the input\n",
    "        if os.path.basename(target_dict[0]) in lst_sumstats:\n",
    "            subject = check_indels(lst_sumstats[os.path.basename(target_dict[0])])\n",
    "        #Or using a prepared input\n",
    "        else:\n",
    "            subject = check_indels(read_sumstat(target_dict[0],None,False)[[\"CHR\",\"POS\",\"A0\",\"A1\",\"GENE\"]])\n",
    "        if \"*\" in subject.A0.values: \n",
    "            raise ValueError(f'illegal character \"*\" is in the A0 column of the TARGET, please check the TARGET file with a reference')\n",
    "        for query in lst_sumstats.values():\n",
    "            #check duplicated indels and remove them.\n",
    "            query = check_indels(query)\n",
    "            #under the same chr:pos or gene:chr:pos. match A0 and A1 by exact, flip, reverse, or flip+reverse.\n",
    "            #if duplicated chr_pos or gene_chr_pos exist, run a recursive match for each pair of them between query and subject.\n",
    "            nq,_ = snps_match(query,subject,keep_ambiguous)\n",
    "            nq = nq.loc[:,~nq.columns.duplicated()] # Remove duplicated columns due to order of columns difference in subject and query\n",
    "            nqs.append(nq)\n",
    "        if intersect:\n",
    "            #get common snps\n",
    "            common_snps = set.intersection(*[set(nq.SNP) for nq in nqs])\n",
    "            print('Total number of common SNPs: ',len(common_snps))\n",
    "            #write out new sumstats\n",
    "            for output_sumstats,nq in zip(lst_sumstats_file,nqs):\n",
    "                sumstats = nq[nq.SNP.isin(common_snps)]\n",
    "                sumstats.to_csv(os.path.join(output_path, output_sumstats), sep = \"\\t\", header = True, index = False)\n",
    "        else:\n",
    "            for output_sumstats,nq in zip(lst_sumstats_file,nqs):\n",
    "                #output match SNPs with target SNPs.\n",
    "                nq.to_csv(os.path.join(output_path, output_sumstats), sep = \"\\t\", header = True, index = False)\n",
    "        print('All are done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expanded-civilization",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2 (unify sumstats)]\n",
    "depends: f'{cwd:a}/utils.py'\n",
    "input: for_each = \"yml_path\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = '${ }', input = f'{cwd:a}/utils.py', stderr = f'{cwd:a}/{path(_yml_path[1]):bn}.stderr', stdout = f'{cwd:a}/output.stdout'\n",
    "    yml = \"${_yml_path[1]}\"\n",
    "    keep_ambiguous = ${keep_ambiguous}\n",
    "    intersect = ${intersect}\n",
    "    print(yml, keep_ambiguous,intersect)\n",
    "    unify_sumstat(yml, keep_ambiguous,intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-teach",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_3 ,sumstat_to_vcf_1 ]\n",
    "input:  group_by = \"all\", for_each = \"sumstat_path\"\n",
    "output: [f'{path(x):an}.vcf' for x in _sumstat_path]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stderr = f'{cwd:a}/{path(_sumstat_path[0]):bn}.stderr', stdout = f'{cwd:a}/output.stdout'\n",
    "    from cugg.sumstat import ss_2_vcf\n",
    "    import pandas as pd\n",
    "    from sos.targets import path\n",
    "    def ss_2_vcf(ss_df,name = \"name\"):\n",
    "        ## Geno field\n",
    "        df = pd.DataFrame()\n",
    "        if \"SNP\" not in ss_df.columns:\n",
    "            ss_df['SNP'] = 'chr'+ss_df.CHR.astype(str).str.strip(\"chr\") + ':' + ss_df.POS.astype(str) + '_' + ss_df.A0.astype(str) + '_' + ss_df.A1.astype(str)\n",
    "        df[['#CHROM', 'POS', 'ID', 'REF', 'ALT']] = ss_df[['CHR', 'POS', 'SNP', 'A0', 'A1']].sort_values(['CHR', 'POS'])\n",
    "        ## Info field(Empty)\n",
    "        df['QUAL'] = \".\"\n",
    "        df['FILTER'] = \"PASS\"\n",
    "        df['INFO'] = \".\"\n",
    "        fix_header = [\"SNP\",\"A1\",\"A0\",\"POS\",\"CHR\",\"STAT\",\"SE\",\"P\"]\n",
    "        header_list = []\n",
    "        if \"GENE\" in ss_df.columns:\n",
    "            df['ID'] = ss_df['GENE'] + \":\" + ss_df['SNP']\n",
    "            df['INFO'] = \"GENE=\" + ss_df[\"GENE\"]\n",
    "            fix_header = [\"GENE\",\"SNP\",\"A1\",\"A0\",\"POS\",\"CHR\",\"STAT\",\"SE\",\"P\"]\n",
    "            header_list = ['##INFO=<ID=GENE,Number=1,Type=String,Description=\"The name of genes\">']\n",
    "        ### Fix headers\n",
    "        import time\n",
    "        header = '##fileformat=VCFv4.2\\n' + \\\n",
    "        '##FILTER=<ID=PASS,Description=\"All filters passed\">\\n' + \\\n",
    "        f'##fileDate={time.strftime(\"%Y%m%d\",time.localtime())}\\n'+ \\\n",
    "        '##FORMAT=<ID=STAT,Number=1,Type=Float,Description=\"Effect size estimate relative to the alternative allele\">\\n' + \\\n",
    "        '##FORMAT=<ID=SE,Number=1,Type=Float,Description=\"Standard error of effect size estimate\">\\n' + \\\n",
    "        '##FORMAT=<ID=P,Number=1,Type=Float,Description=\"The Pvalue corresponding to ES\">\\n' \n",
    "        ### Customized Field headers\n",
    "        for x in ss_df.columns:\n",
    "            if x not in fix_header:\n",
    "                Prefix = f'##FORMAT=<ID={x},Number=1,Type='\n",
    "                Type = str(type(ss_df[x][0])).replace(\"<class \\'\",\"\").replace(\"'>\",\"\").replace(\"numpy.\",\"\").replace(\"64\",\"\").capitalize().replace(\"Int\",\"Integer\")\n",
    "                Surfix = f',Description=\"Customized Field {x}\">'\n",
    "                header_list.append(Prefix+Type+Surfix)\n",
    "        ## format and sample field\n",
    "        df['FORMAT'] = \":\".join([\"STAT\",\"SE\",\"P\"] + ss_df.drop(fix_header,axis = 1).columns.values.tolist())\n",
    "        df[f'{name}'] = ss_df['STAT'].astype(str) + \":\" + ss_df['SE'].astype(str) + \":\" + ss_df['P'].astype(str) + \":\" + ss_df.drop(fix_header,axis = 1).astype(str).apply(\":\".join,axis = 1)\n",
    "        ## Rearrangment\n",
    "        df = df[['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO','FORMAT',f'{name}']]\n",
    "        # Add headers\n",
    "        header = header + \"\\n\".join(header_list) + \"\\n\"\n",
    "        return df,header\n",
    "\n",
    "    sumstat_path_list = ${_sumstat_path}\n",
    "    name = ${name}\n",
    "    for x,y in zip(sumstat_path_list,name):\n",
    "        sumstats = pd.read_csv(x,\"\\t\")\n",
    "        sumstats,header = ss_2_vcf(sumstats,y)\n",
    "        with open(f'{path(x):an}.vcf', 'w') as f:\n",
    "            f.write(header)\n",
    "        sumstats.to_csv(f'{path(x):an}.vcf', sep = \"\\t\", header = True, index = False,mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-reception",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_4,sumstat_to_vcf_2]\n",
    "output: f'{cwd}/{_input[0]:bn}.merged.vcf.gz'.replace(name[0],\"_\".join(name))\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = '${ }', stderr = f'{cwd:a}/{_output:bn}.stderr', stdout = f'{cwd:a}/{_output:bn}.stdout',container = container\n",
    "    for i in ${_input:r}; do\n",
    "    bgzip -k -f $i \n",
    "    tabix -p vcf -f  $i.gz; done\n",
    "    bcftools merge ${\" \".join([f'{str(x)}.gz' for x in _input])} --force-samples -m id  -Oz -o ${_output:a}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.22.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
