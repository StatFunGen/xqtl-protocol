{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weird-manner",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Genotype Data Formatting\n",
    "\n",
    "This module implements a collection of workflows used to format genotype data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-linux",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-words",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The module streamlines conversion between PLINK and VCF formats, specifically:\n",
    "\n",
    "1. Conversion between VCF and PLINK formats\n",
    "2. Split data (by specified input, by chromosomes, by genes)\n",
    "3. Merge data (by specified input, by chromosomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-proceeding",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "Depending on the analysis task, input files are specified in one of the following formats:\n",
    "\n",
    "1. A single Whole genome data in VCF format, or in PLINK bim/bed/fam bundle; Or,\n",
    "2. A list of VCF or PLINK bed file\n",
    "3. A singular column file containing a list of VCF or PLINK bed file\n",
    "4. A two column file containing a list of per chromosome VCF or PLINK bed file where the first column is chrom and 2nd column is file name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-employer",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "\n",
    "Genotype data in PLINK format partitioned by chromosome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-metropolitan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example\n",
    "\n",
    "The data and singularity container `bioinfo.sif` can be downloaded from [Synapse](https://www.synapse.org/#!Synapse:syn36416559/files/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-example",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Merge separated bed files into one\n",
    "\n",
    "`ROSMAP_sample_list.txt` is a list that includes all ROSMAP samples we need for analysis, in formatting of FID, IID. This file has been uploaded to ftp: `/ftp_fgc_xqtl/projects/WGS/ROSMAP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-conspiracy",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run pipeline/genotype_formatting.ipynb vcf_to_plink\n",
    "    --genoFile `ls vcf_qc/*.leftnorm.bcftools_qc.vcf.gz` \\\n",
    "    --cwd Genotype/ \\\n",
    "    --keep_samples ./ROSMAP_sample_list.txt\n",
    "    --container /mnt/vast/hpc/csg/containers/bioinfo.sif \\\n",
    "    -J 22 -q csg -c csg.yml --mem 120G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-armenia",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This step merges all the files and may require anout 300G mem to run, because there are some variants' ID with 80+ characters. And only plink can do the merge job, plink2 doesn't support merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-theory",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run xqtl-protocol/pipeline/genotype_formatting.ipynb merge_plink \\\n",
    "    --genoFile `ls *.leftnorm.bcftools_qc.bed` \\\n",
    "    --name ROSMAP_NIA_WGS.leftnorm.bcftools_qc  \\\n",
    "    --cwd Genotype/ \\\n",
    "    --container /mnt/vast/hpc/csg/containers/bioinfo.sif \\\n",
    "    -J 5 -q csg -c csg.yml --mem 300G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-phone",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Genotype data partition by chromosome\n",
    "\n",
    "This step is necessary for TensorQTL applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-hollow",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing: <1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-modeling",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run pipeline/genotype_formatting.ipynb genotype_by_chrom \\\n",
    "    --genoFile input/protocol_example.genotype.chr21_22.bed \\\n",
    "    --cwd output \\\n",
    "    --chrom `cut -f 1 input/protocol_example.genotype.chr21_22.bim | uniq | sed \"s/chr//g\"` \\\n",
    "    --container containers/bioinfo.sif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-mention",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "INFO: Running genotype_by_chrom_1:\n",
    "INFO: genotype_by_chrom_1 (index=1) is completed.\n",
    "INFO: genotype_by_chrom_1 (index=0) is completed.\n",
    "INFO: genotype_by_chrom_1 output:   /Users/alexmccreight/xqtl-protocol/output/protocol_example.genotype.chr21_22.22.bed /Users/alexmccreight/xqtl-protocol/output/protocol_example.genotype.chr21_22.21.bed in 2 groups\n",
    "INFO: Running genotype_by_chrom_2:\n",
    "INFO: genotype_by_chrom_2 is completed (pending nested workflow).\n",
    "INFO: Running write_data_list:\n",
    "INFO: write_data_list is completed.\n",
    "INFO: write_data_list output:   /Users/alexmccreight/xqtl-protocol/output/protocol_example.genotype.chr21_22.genotype_by_chrom_files.txt\n",
    "INFO: genotype_by_chrom_2 output:   /Users/alexmccreight/xqtl-protocol/output/protocol_example.genotype.chr21_22.genotype_by_chrom_files.txt\n",
    "INFO: Workflow genotype_by_chrom (ID=w5be6d78805f1df2e) is executed successfully with 3 completed steps and 4 completed substeps.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-handy",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adopted-reservation",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run genotype_formatting.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  plink_to_vcf\n",
      "  vcf_to_plink\n",
      "  plink_by_gene\n",
      "  plink_by_chrom\n",
      "  merge_plink\n",
      "  merge_vcf\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        Work directory & output directory\n",
      "  --container ''\n",
      "                        The filename name for containers\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 3G\n",
      "                        Memory expected\n",
      "  --numThreads 20 (as int)\n",
      "                        Number of threads\n",
      "  --genoFile  paths\n",
      "\n",
      "                        the path to a bed file or VCF file, a vector of bed\n",
      "                        files or VCF files, or a text file listing the bed files\n",
      "                        or VCF files to process\n",
      "\n",
      "Sections\n",
      "  plink_to_vcf_1:\n",
      "  vcf_to_plink:\n",
      "  plink_by_gene_1:\n",
      "    Workflow Options:\n",
      "      --window 500000 (as int)\n",
      "                        cis window size\n",
      "      --region-list VAL (as path, required)\n",
      "                        Region definition\n",
      "  plink_by_chrom_1:\n",
      "    Workflow Options:\n",
      "      --chrom VAL VAL ... (as type, required)\n",
      "  plink_by_chrom_2, plink_by_gene_2:\n",
      "  plink_to_vcf_2:\n",
      "  merge_plink:\n",
      "    Workflow Options:\n",
      "      --name VAL (as str, required)\n",
      "                        File prefix for the analysis output\n",
      "  merge_vcf:\n",
      "    Workflow Options:\n",
      "      --name VAL (as str, required)\n",
      "                        File prefix for the analysis output\n"
     ]
    }
   ],
   "source": [
    "sos run genotype_formatting.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-lloyd",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path(\"output\")\n",
    "# The filename name for containers\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "# the path to a bed file or VCF file, a vector of bed files or VCF files, or a text file listing the bed files or VCF files to process\n",
    "parameter: genoFile = paths\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size\n",
    "cwd = f\"{cwd:a}\"\n",
    "\n",
    "import os\n",
    "def get_genotype_file(geno_file_paths):\n",
    "    #\n",
    "    def valid_geno_file(x):\n",
    "        suffixes = path(x).suffixes\n",
    "        if suffixes[-1] == '.bed':\n",
    "            return True\n",
    "        if len(suffixes)>1 and ''.join(suffixes[-2:]) == \".vcf.gz\":\n",
    "            return True\n",
    "        return False\n",
    "    #\n",
    "    def complete_geno_path(x, geno_file):\n",
    "        if not valid_geno_file(x):\n",
    "            raise ValueError(f\"Genotype file {x} should be VCF (end with .vcf.gz) or PLINK bed file (end with .bed)\")\n",
    "        if not os.path.isfile(x):\n",
    "            # relative path\n",
    "            if not os.path.isfile(f'{geno_file:ad}/' + x):\n",
    "                raise ValueError(f\"Cannot find genotype file {x}\")\n",
    "            else:\n",
    "                x = f'{geno_file:ad}/' + x\n",
    "        return x\n",
    "    # \n",
    "    def format_chrom(chrom):\n",
    "        if chrom.startswith('chr'):\n",
    "            chrom = chrom[3:]\n",
    "        return chrom\n",
    "    # Inputs are either VCF or bed, or a vector of them \n",
    "    if len(geno_file_paths) > 1:\n",
    "        if all([valid_geno_file(x) for x in geno_file_paths]):\n",
    "            return paths(geno_file_paths)\n",
    "        else: \n",
    "            raise ValueError(f\"Invalid input {geno_file_paths}\")\n",
    "    # Input is one genotype file or text list of genotype files\n",
    "    geno_file = geno_file_paths[0]\n",
    "    if valid_geno_file(geno_file):\n",
    "        return paths(geno_file)\n",
    "    else: \n",
    "        units = [x.strip().split() for x in open(geno_file).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "        if all([len(x) == 1 for x in units]):\n",
    "            return paths([complete_geno_path(x[0], geno_file) for x in units])\n",
    "        elif all([len(x) == 2 for x in units]):\n",
    "            genos = dict([(format_chrom(x[0]), path(complete_geno_path(x[1], geno_file))) for x in units])\n",
    "        else:\n",
    "            raise ValueError(f\"{geno_file} should contain one column of file names, or two columns of chrom number and corresponding file name\")\n",
    "        return genos\n",
    "                        \n",
    "genoFile = get_genotype_file(genoFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-emerald",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## PLINK to VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "primary-radiation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[plink_to_vcf_1]\n",
    "if isinstance(genoFile, dict):\n",
    "    genoFile = genoFile.values()\n",
    "\n",
    "input: genoFile, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.vcf.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:nn}.stderr', stdout = f'{_output:nn}.stdout', container = container, entrypoint=entrypoint\n",
    "    plink2 --bfile ${_input:n} \\\n",
    "        --recode vcf-iid  \\\n",
    "        --out ${_output:nn} \\\n",
    "        --threads ${numThreads} \\\n",
    "        --memory ${int(expand_size(mem) * 0.9)/1e06} --output-chr chrMT\n",
    "    bgzip -l 9 ${_output:n}\n",
    "    tabix -f -p vcf ${_output}\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    for i in ${_output} ; do \n",
    "        # Capture file metadata\n",
    "        output_info=\"$i\"\n",
    "        output_size=$(ls -lh \"$i\" | awk '{print $5}')\n",
    "        output_rows=$(zcat \"$i\" | wc -l)\n",
    "        output_column=$(zcat \"$i\" | grep -v \"##\" | head -1 | wc -w)\n",
    "        output_header_row=$(zcat \"$i\" | grep \"##\" | wc -l)\n",
    "        output_preview=$(zcat \"$i\" | grep -v \"##\" | head | cut -f 1-11)\n",
    "\n",
    "        # Write captured information to the stdout file\n",
    "        printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %d\\noutput_column: %d\\noutput_header_row: %d\\noutput_preview:\\n%s\\n\" \\\n",
    "            \"$output_info\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_header_row\" \"$output_preview\"\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-deposit",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## VCF to PLINK\n",
    "\n",
    "Export VCF files to PLINK 1.0 format, **without keeping allele orders by default**. The resulting PLINK will lose ref/alt allele information but will go by major/minor allele, as conventionally used in standard PLINK format. Notice that PLINK 1.0 format does not allow for dosages. PLINK 2.0 format support it, but it is generally not supported by downstreams data analysis.  \n",
    "\n",
    "In the following code block the option `--vcf-half-call m`  treat half-call as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-template",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[vcf_to_plink]\n",
    "parameter: remove_duplicates = False\n",
    "parameter: add_chr = True\n",
    "# The path to the file that contains the list of samples to remove (format FID, IID)\n",
    "parameter: remove_samples = path('.')\n",
    "# The path to the file that contains the list of samples to keep (format FID, IID)\n",
    "parameter: keep_samples = path('.')\n",
    "fail_if(not (keep_samples.is_file() or keep_samples == path('.')), msg = f'Cannot find ``{keep_samples}``')\n",
    "fail_if(not (remove_samples.is_file() or remove_samples == path('.')), msg = f'Cannot find ``{remove_samples}``')\n",
    "\n",
    "if isinstance(genoFile, dict):\n",
    "    genoFile = genoFile.values()\n",
    "\n",
    "input: genoFile, group_by = 1\n",
    "output: f'{cwd}/{_input:bnn}.bed'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    plink2 --vcf ${_input} \\\n",
    "        --vcf-half-call m \\\n",
    "        --vcf-require-gt \\\n",
    "        --allow-extra-chr \\\n",
    "        ${('--keep %s' % keep_samples) if keep_samples.is_file() else \"\"} \\\n",
    "        ${('--remove %s' % remove_samples) if remove_samples.is_file() else \"\"} \\\n",
    "        --make-bed --out ${_output:n}  ${\"--rm-dup exclude-all\" if remove_duplicates else \"\" } \\\n",
    "        --threads ${numThreads} \\\n",
    "        --memory ${int(expand_size(mem) * 0.9)/1e06} ${\"--output-chr chrMT\" if add_chr else \"\"}  \n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=${_output:n}.stdout\n",
    "        for i in ${_output} ; do \n",
    "            echo \"output_info: $i \" >> $stdout;\n",
    "            echo \"output_size: $(ls -lh \"$i\" | awk '{print $5}')\" >> $stdout;\n",
    "        done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-dollar",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Split PLINK genotypes into specified regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-belief",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genotype_by_region_1]\n",
    "# cis window size\n",
    "parameter: window = 0\n",
    "# Region definition\n",
    "parameter: region_list = path\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "input: genoFile, for_each = 'regions'\n",
    "output: f'{cwd}/{region_list:bn}_genotype_by_region/{_input:bn}.{_regions[3]}.bed'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    plink2 --bfile ${_input:an} \\\n",
    "        --make-bed \\\n",
    "        --out ${_output[0]:n} \\\n",
    "        --chr ${_regions[0]} \\\n",
    "        --from-bp ${f'0' if (int(_regions[1]) - window) < 0 else f'{(int(_regions[1]) - window)}'} \\\n",
    "        --to-bp ${int(_regions[2]) + window} \\\n",
    "        --allow-no-sex --output-chr chrMT || touch ${_output} \n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        for i in ${_output} ; do \n",
    "            echo \"output_info: $i \"\n",
    "            echo \"output_size: $(ls -lh \"$i\" | awk '{print $5}')\"\n",
    "        done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-information",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Compute LD matrices for given input region\n",
    "\n",
    "### PLINK based implementation\n",
    "\n",
    "**FIXME: Hao, I suggest including all contents for LD matrix storage type benchmarking into this repo, so we justify why we would like to save the data as square 0, float 16 and using npz format**. Perhaps we should start a folder called \"code/auxillary\" to keep notebooks such as these? You can then remove what you have in the `brain-xqtl-analysis` repository after you migrate all the relevant contents here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "personalized-spider",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_by_region_plink_1]\n",
    "# Region definition\n",
    "parameter: region_list = path\n",
    "parameter: float_type = 16\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "input: genoFile, for_each = 'regions'\n",
    "output: f'{cwd}/{region_list:bn}_LD/{_input:bn}.{_regions[0]}_{_regions[1]}_{_regions[2]}.float{float_type}.npz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, volumes = [f'{region_list:ad}:{region_list:ad}'], entrypoint=entrypoint\n",
    "    plink --bfile ${_input:an} \\\n",
    "        --out ${_output:nn} \\\n",
    "        --chr ${_regions[0]} \\\n",
    "        --from-bp ${_regions[1]} \\\n",
    "        --to-bp ${_regions[2]} \\\n",
    "        --r square0 \\\n",
    "        --make-just-bim \\\n",
    "        --threads ${numThreads}\n",
    "\n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    np_ld = np.loadtxt(\"${_output:nn}.ld\", delimiter = \"\\t\", dtype = \"float${float_type}\")\n",
    "    bim = pd.read_csv(\"${_output:nn}.bim\", \"\\t\", header = None)[1].to_numpy()\n",
    "    np.savez_compressed(\"${_output}\", np_ld, bim, allow_pickel = True)\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        echo \"The npz file is a numpy compressed version of the .ld file described below\"\n",
    "        for i in $[_output:nn] ; do \n",
    "        echo \"output_info: $i.ld \"\n",
    "        echo \"output_size:\" `ls -lh $i.ld | cut -f 5  -d  \" \"`\n",
    "        echo \"output_column:\" `head -1 $i | wc -w `\n",
    "        echo \"output_row:\" `wc -l $i `\n",
    "        done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-congress",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### ldstore2 based implementation\n",
    "\n",
    "This is good for larger sample sizes such as data from UK Biobank although we are not facing this challenge in the FunGen-xQTL project.\n",
    "\n",
    "**FIXME: we need to build ldstore2 into a container image**. According to Diana it should be \n",
    "\n",
    "```\n",
    "pip3 install https://files.pythonhosted.org/packages/a8/fd/f98ab7dea176f42cb61b80450b795ef19b329e8eb715b87b0d13c2a0854d/ldstore-0.1.9.tar.gz \n",
    "```\n",
    "\n",
    "**FIXME: Diana, what's the input for this workflow?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-junction",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Create `master` file for `ldstore2`\n",
    "\n",
    "The master file is a semicolon-separated text file and contains no space. It contains the following mandatory column names and one dataset per line:\n",
    "\n",
    "**FIXME: Diana, this documentation is not clearly written. I cannot understand it. What are the mandatory column names? What does it mean by one data-set per line?**\n",
    "\n",
    "- For the Z file, the format should be `rsid:chrom:pos:a1:a2`. Formatting for chromosome should be `01,02,03` etc\n",
    "- List of samples\n",
    "\n",
    "**The LDstore draft is currently availale [here](https://github.com/statgenetics/UKBB_GWAS_dev/blob/master/workflow/111722_LDstore.ipynb) with the code to prepare for the genotypic input [here](https://github.com/statgenetics/UKBB_GWAS_dev/blob/master/workflow/113022_bgenix_ldblocks.ipynb). A minimal working example can be found [here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-shopping",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Split PLINK by Chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-swedish",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genotype_by_chrom_1]\n",
    "stop_if(len(paths(genoFile))>1, msg = \"This workflow expects one input genotype file.\")\n",
    "parameter: chrom = list\n",
    "chrom = list(set(chrom))\n",
    "input: genoFile, for_each = \"chrom\"\n",
    "output: f'{cwd}/{_input:bn}.{_chrom}.bed'\n",
    "# look up for genotype file\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, volumes = [f'{genoFile:ad}:{genoFile:ad}'], entrypoint=entrypoint\n",
    "    ##### Get the locus genotypes for $[_chrom]\n",
    "    plink2 --bfile $[_input:an] \\\n",
    "    --make-bed \\\n",
    "    --out $[_output[0]:n] \\\n",
    "    --chr $[_chrom] \\\n",
    "    --threads $[numThreads] \\\n",
    "    --memory $[int(expand_size(mem) * 0.9)/1e06] \\\n",
    "    --allow-no-sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-sharp",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genotype_by_chrom_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{_input[0]:nn}.{step_name[:-2]}_files.txt'\n",
    "sos_run(\"write_data_list\", data_files = _input, out = _output, ext = \"bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-oregon",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[plink_to_vcf_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{_input[0]:nnn}.{step_name[:-2]}_files.txt'\n",
    "sos_run(\"write_data_list\", data_files = _input, out = _output, ext = \"vcf.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-bangladesh",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genotype_by_region_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{_input[0]:nn}.{step_name[:-2]}_files.txt'\n",
    "sos_run(\"write_data_list\", data_files = _input, out = _output, ext = \"bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-harvard",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_by_region_*_2]\n",
    "parameter: region_list = path\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{region_list:bn}_LD/{genoFile:bn}.ld.list'\n",
    "sos_run(\"write_data_list\", data_files = _input, out = _output, ext = \"npy.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-pride",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[write_data_list]\n",
    "parameter: out = path\n",
    "parameter: ext = str\n",
    "parameter: data_files = paths\n",
    "input: data_files\n",
    "output: out\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import os, sys\n",
    "\n",
    "    # Extracting the id list outside for better readability\n",
    "    n = len(\"${ext}\".split(\".\"))+1\n",
    "    id_list = [x.split(\".\")[-n] for x in [${_input:r,}]]\n",
    "    files = [${_input:r,}]\n",
    "\n",
    "    # check if some files are empty remove them from the id_list\n",
    "    non_empty_files = []\n",
    "    non_empty_ids = []\n",
    "\n",
    "    for file, id in zip(files, id_list):\n",
    "        try:\n",
    "            # Check if file is empty\n",
    "            if os.path.getsize(file) > 0:\n",
    "                non_empty_files.append(file)\n",
    "                non_empty_ids.append(id)\n",
    "            else:\n",
    "                print(f\"Empty file found: {file}\", file=sys.stderr)\n",
    "        except OSError as e:\n",
    "            print(f\"Error accessing file {file}: {e}\", file=sys.stderr)\n",
    "\n",
    "    if not non_empty_files:\n",
    "        raise ValueError(\"No non-empty files found. Exiting.\")\n",
    "    \n",
    "    data_tempt = pd.DataFrame({\n",
    "        \"#id\": non_empty_ids,\n",
    "        \"#path\": non_empty_files\n",
    "    })\n",
    "\n",
    "    data_tempt.to_csv(\"${_output}\", index=False, sep=\"\\t\")\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    i=\"${_output}\"\n",
    "    output_size=$(ls -lh $i | cut -f 5 -d ' ')\n",
    "    output_rows=$(cat $i | wc -l | cut -f 1 -d ' ')\n",
    "    output_column=$(cat $i | head -1 | wc -w)\n",
    "    output_preview=$(cat $i | grep -v \"##\" | head | cut -f 1,2,3,4,5,6)\n",
    "    \n",
    "    printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %s\\noutput_column: %s\\noutput_preview:\\n%s\\n\" \\\n",
    "        \"$i\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-objective",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Split VCF by Chromosome\n",
    "\n",
    "**FIXME: add this as needed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-payroll",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merge PLINK files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-exchange",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[merge_plink]\n",
    "skip_if(len(genoFile) == 1)\n",
    "# File prefix for the analysis output\n",
    "parameter: name = str\n",
    "# The path to the file that contains the list of samples to keep (format FID, IID)\n",
    "parameter: keep_samples = path('.')\n",
    "parameter: extra_plink_opts = []\n",
    "input: genoFile, group_by = 'all'\n",
    "output: f\"{cwd}/{name}{_input[0]:x}\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    echo ${_input:n} | tr ' ' '\\n' | tail -n +2 >  ${_output:n}.merge_list\n",
    "    plink${\"2\" if str(_input).endswith(\"pgen\") else \" --keep-allele-order\"} \\\n",
    "    --${\"p\" if str(_input).endswith(\"pgen\") else \"b\"}file ${_input[0]:n} \\\n",
    "    --${\"p\" if str(_input).endswith(\"pgen\") else \"\"}merge-list ${_output:n}.merge_list \\\n",
    "    --make-${\"pgen\" if str(_input).endswith(\"pgen\") else \"bed\"} \\\n",
    "    --out ${_output:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory ${int(expand_size(mem) * 0.9)/1e06} ${('--keep %s' % keep_samples) if keep_samples.is_file() else \"\"} ${\" \".join([(\"--%s\" % x) for x in extra_plink_opts])}\n",
    "    rm -f ${_output:n}.merge_list\n",
    "\n",
    "    i=\"${_output}\"\n",
    "    output_size=$(ls -lh $i | cut -f 5 -d ' ')\n",
    "    output_rows=$(zcat $i | wc -l | cut -f 1 -d ' ')\n",
    "    output_column=$(zcat $i | head -1 | wc -w)\n",
    "    output_preview=$(cat $i | grep -v \"##\" | head | cut -f 1,2,3,4,5,6)\n",
    "    \n",
    "    printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %s\\noutput_column: %s\\noutput_preview:\\n%s\\n\" \\\n",
    "        \"$i\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-thumbnail",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merge VCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-duncan",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[merge_vcf]\n",
    "skip_if(len(genoFile) == 1)\n",
    "# File prefix for the analysis output\n",
    "parameter: name = str\n",
    "input: genoFile, group_by = 'all'\n",
    "output:  f\"{cwd}/{name}.vcf.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    bcftools concat -Oz ${_input} > ${_output}\n",
    "    tabix -p vcf ${_output}\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    for i in ${_output} ; do \n",
    "        # Capture file metadata\n",
    "        output_info=\"$i\"\n",
    "        output_size=$(ls -lh \"$i\" | awk '{print $5}')\n",
    "        output_rows=$(zcat \"$i\" | wc -l)\n",
    "        output_column=$(zcat \"$i\" | grep -v \"##\" | head -1 | wc -w)\n",
    "        output_header_row=$(zcat \"$i\" | grep \"##\" | wc -l)\n",
    "        output_preview=$(zcat \"$i\" | grep -v \"##\" | head | cut -f 1-11)\n",
    "\n",
    "        # Write captured information to the stdout file\n",
    "        printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %d\\noutput_column: %d\\noutput_header_row: %d\\noutput_preview:\\n%s\\n\" \\\n",
    "            \"$output_info\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_header_row\" \"$output_preview\" >> ${_output:n}.stdout\n",
    "    done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
