{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dramatic-measurement",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Phenotype Data Formatting\n",
    "\n",
    "**FIXME: this entire pipeline needs to be improved**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7b314-106e-45d5-b261-cd830f091775",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1f31e-669c-4a91-8b32-af684377fd0c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "We include a collection of workflows to format molecular phenotype data. These include workflows to separate phenotypes by chromosome, by user-provided regions, a workflow to subset bam files and a workflow to extract samples from phenotype files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174242f9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "The input for this workflow is the collection of data for 1 molecular phenotype as described in the format of:\n",
    "\n",
    "1. a complete residualized (covariates regressed out) molecular phenotype data \n",
    "2. a region list\n",
    "\n",
    "These input are outputs from previous pipelines such as `covariate_preprocessing` and `gene_annotation`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6691f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "\n",
    "1. A list of phenotype file (bed+index) for each chrom, annotated with genomic coordiates, suitable for TensorQTL analysis.\n",
    "2. A lists of phenotype file (bed+index) for each gene,  annotated with genomic coordiates, suitable for fine-mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-horse",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example Steps\n",
    "The data and singularity image used are available on [Synapse](https://www.synapse.org/#!Synapse:syn52369482).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135197a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### iii. Partition by chromosome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f207e-9efd-41f7-b315-ac964d7c2c2b",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This is necessary for cis TensorQTL analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586927f6-cf0a-447f-8f60-ec973f986191",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing < 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5f326",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!sos run phenotype_formatting.ipynb phenotype_by_chrom \\\n",
    "    --cwd ../../../output_test/phenotype_by_chrom \\\n",
    "    --phenoFile ../../../output_test/phenotype_by_chrom/protocol_example.protein.bed.gz \\\n",
    "    --chrom `for i in {21..22}; do echo chr$i; done`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c4a15",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "INFO: Running phenotype_by_chrom_1:\n",
    "INFO: phenotype_by_chrom_1 (index=0) is completed.\n",
    "INFO: phenotype_by_chrom_1 (index=1) is completed.\n",
    "INFO: phenotype_by_chrom_1 output:   output/phenotype_by_chrom/protocol_example.protein.bed.chr22.bed.gz output/phenotype_by_chrom/protocol_example.protein.bed.chr21.bed.gz in 2 groups\n",
    "INFO: Running phenotype_by_chrom_2:\n",
    "INFO: phenotype_by_chrom_2 is completed.\n",
    "INFO: phenotype_by_chrom_2 output:   output/phenotype_by_chrom/protocol_example.protein.bed.phenotype_by_chrom_files.txt\n",
    "INFO: Workflow phenotype_by_chrom (ID=w83c4137c28693564) is executed successfully with 2 completed steps and 3 completed substeps.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53850dc7-b064-4a82-8dea-1e255511150e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae297e-0114-488b-b8b4-9ea909e2a737",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "| Step | Substep | Problem | Possible Reason | Solution |\n",
    "|------|---------|---------|------------------|---------|\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04bb75",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5d7bfa",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run phenotype_formatting.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  phenotype_by_chrom\n",
      "  phenotype_by_region\n",
      "  bam_subsetting\n",
      "  gct_extract_samples\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        Work directory & output directory\n",
      "  --container ''\n",
      "                        The filename namefor output data\n",
      "  --entrypoint  ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
      "\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 20 (as int)\n",
      "                        Number of threads\n",
      "  --phenoFile VAL (as path, required)\n",
      "                        Path to the input molecular phenotype data.\n",
      "  --name  f'{phenoFile:bn}'\n",
      "\n",
      "                        name for the analysis output\n",
      "\n",
      "Sections\n",
      "  phenotype_by_chrom_1:\n",
      "    Workflow Options:\n",
      "      --chrom VAL VAL ... (as type, required)\n",
      "                        list of chroms to extract\n",
      "  phenotype_by_chrom_2:\n",
      "  phenotype_by_region_1:\n",
      "    Workflow Options:\n",
      "      --region-list VAL (as path, required)\n",
      "                        An index text file with 4 columns specifying the chr,\n",
      "                        start, end and name of regions to analyze\n",
      "  phenotype_by_region_2:\n",
      "  bam_subsetting:\n",
      "    Workflow Options:\n",
      "      --region VAL VAL ... (as type, required)\n",
      "                        Input to `samtools view` coordinates, for example,\n",
      "                        --region chr21 chr22\n",
      "  gct_extract_samples:  Extract samples from expression data generated by\n",
      "                        RNASeQC\n",
      "    Workflow Options:\n",
      "      --keep-samples VAL (as path, required)\n"
     ]
    }
   ],
   "source": [
    "!sos run phenotype_formatting.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c427e-b7cf-45f0-a5af-87e2a7c18a1e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monthly-america",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import os\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path(\"output\")\n",
    "# The filename namefor output data\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "# Path to the input molecular phenotype data.\n",
    "parameter: phenoFile = path\n",
    "# name for the analysis output\n",
    "parameter: name = str(phenoFile).removesuffix('.gz').removesuffix('.bed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-addition",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Process of molecular phenotype file\n",
    "This workflow produce a bed+tabix file for all the molecular pheno data that are included in the region list to feed into downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-semester",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[phenotype_by_chrom_1]\n",
    "# list of chroms to extract \n",
    "parameter: chrom = list\n",
    "chrom = list(set(chrom))\n",
    "# Path to the input molecular phenotype data.\n",
    "input: phenoFile, for_each = \"chrom\"\n",
    "output: f'{cwd}/{name}.{_chrom}.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "bash: expand = \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout',container = container, entrypoint=entrypoint\n",
    "    zcat $[_input] | head -1 > $[_output:n]\n",
    "    tabix $[_input] $[_chrom] >> $[_output:n] \n",
    "    bgzip -f $[_output:n]\n",
    "    tabix -p bed $[_output] -f\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -v \"##\"   | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-gibson",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[phenotype_by_chrom_2]\n",
    "# Path to the input molecular phenotype data.\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{name}.{step_name[:-2]}_files.txt',f'{cwd}/{name}.{step_name[:-2]}_files.region_list.txt'\n",
    "import pandas as pd\n",
    "chrom = [str(x).split(\".\")[-3].replace(\"chr\",\"\") for x in _input]\n",
    "chrom_df = pd.DataFrame({\"#id\" : chrom ,\"#dir\" : _input})\n",
    "chrom_df.to_csv(_output[0],index = 0,sep = \"\\t\")\n",
    "chrom_df[\"#chr\"] = [f'chr{x}' for x in chrom]\n",
    "phenoFile = pd.read_csv(phenoFile,sep = \"\\t\", usecols = [0,1,2,3]).merge(chrom_df[[\"#chr\",\"#dir\"]],left_on = \"#chr\",right_on = \"#chr\").rename(columns={\"#dir\": \"path\"})\n",
    "phenoFile.to_csv(_output[1], index = 0, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b48a57-5955-46b0-8e0f-9eeab0e35469",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[phenotype_annotate_by_tad]\n",
    "parameter: TAD_list = path\n",
    "parameter: phenotype_per_tad = 2 # This is the minimum number of epigenomics marker for a tadb to be considered having a functions.\n",
    "input: phenoFile,TAD_list\n",
    "output: f'{cwd}/{_input[0]:b}.{_input[1]:b}.{phenotype_per_tad}_pheno_per_region.region_list'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout',container = container, entrypoint=entrypoint\n",
    "    library(tidyverse)\n",
    "\n",
    "    tabix_region <- function(file, region){\n",
    "    data.table::fread(cmd = paste0(\"tabix -h \", file, \" \", region)) %>%\n",
    "     as_tibble() %>%\n",
    "     mutate(\n",
    "        !!names(.)[1] := as.character(.[[1]]),\n",
    "        !!names(.)[2] := as.numeric(.[[2]])\n",
    "    ) \n",
    "    }\n",
    "    TAD_list = read_delim(${_input[1]:ar})%>%mutate(region  = paste0(`#chr`,\":\",start,\"-\",end),\n",
    "                                        path = ${_input[0]:ar},  \n",
    "                                        keep = map_lgl( region,~tabix_region(${_input[0]:ar}, .x)%>%nrow >= ${phenotype_per_tad} )) \n",
    "    TAD_list%>%filter(keep)%>%select(`#chr`,start,end,ID = index, path)%>%write_delim(${_output:ar},\"\\t\")\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -v \"##\"   | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802147f1-8640-4f39-b8e3-4aeb90cafc48",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[phenotype_by_chrom_gct_1]\n",
    "# list of chroms to extract \n",
    "parameter: chrom = list\n",
    "chrom = list(set(chrom))\n",
    "# Path to the input molecular phenotype data.\n",
    "input: phenoFile, for_each = \"chrom\"\n",
    "output: f'{cwd:a}/{name}.{_chrom}.gct'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "bash: expand = \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout',container = container, entrypoint=entrypoint\n",
    "    zcat $[_input] | head -1 > $[_output:n]\n",
    "    tabix $[_input] $[_chrom] >> $[_output:n] \n",
    "    cat   $[_output:n] |  awk '{$1=$2=$3=\"\"; print $0}' >> $[_output]\n",
    "    rm $[_output:n]\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -v \"##\"   | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done\n",
    "\n",
    "[phenotype_by_chrom_gct_2]\n",
    "# Path to the input molecular phenotype data.\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{name}.{step_name[:-2]}_files.txt',f'{cwd}/{name}.{step_name[:-2]}_files.region_list.txt'\n",
    "import pandas as pd\n",
    "chrom = [str(x).split(\".\")[-2].replace(\"chr\",\"\") for x in _input]\n",
    "chrom_df = pd.DataFrame({\"#id\" : chrom ,\"#dir\" : _input})\n",
    "chrom_df.to_csv(_output[0],index = 0,sep = \"\\t\")\n",
    "chrom_df[\"#chr\"] = [f'chr{x}' for x in chrom]\n",
    "phenoFile = pd.read_csv(phenoFile,sep = \"\\t\", usecols = [0,1,2,3]).merge(chrom_df[[\"#chr\",\"#dir\"]],left_on = \"#chr\",right_on = \"#chr\").rename({\"#dir\":\"path\"})\n",
    "phenoFile.to_csv(_output[1], index = 0, sep = \"\\t\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-andrews",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[phenotype_by_region_1]\n",
    "# An index text file with 4 columns specifying the chr, start, end and name of regions to analyze\n",
    "parameter: region_list = path\n",
    "regions = [x.strip().split() for x in open(region_list).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "# Get the unique chormosome that have regions to be analyzed.\n",
    "def extract_chrom(lst):\n",
    "    return list(set([item[0] for item in lst]))\n",
    "chrom = extract_chrom(regions)\n",
    "# Path to the input molecular phenotype data.\n",
    "input: phenoFile, for_each = \"regions\"\n",
    "output: f'{cwd}/{region_list:bn}_phenotype_by_region/{name}.{_regions[3]}.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "bash: expand = \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout',container = container, entrypoint=entrypoint\n",
    "    tabix -h  $[_input] $[_regions[0]]:$[_regions[1]]-$[_regions[2]] > $[_output:n]\n",
    "    bgzip -f $[_output:n]\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" \n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"` \n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"` \n",
    "        echo \"output_column:\" `zcat $i | grep -v \"##\"   | head -1 | wc -w `\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `\n",
    "        echo \"output_preview:\"\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6 ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-impact",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[phenotype_by_region_2]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{name}.{step_name[:-2]}_files.txt'\n",
    "import pandas as pd\n",
    "region_df = pd.DataFrame({\"#id\" :  [str(x).split(\".\")[-3] for x in _input]  ,\"dir\" : _input})\n",
    "region_df.to_csv(_output,index = 0,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-collect",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[bam_subsetting]\n",
    "# Input to `samtools view` coordinates, for example, --region chr21 chr22\n",
    "parameter: region = list\n",
    "# Path to the input molecular phenotype data.\n",
    "parameter: phenoFile = paths\n",
    "input: phenoFile , group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.subsetted.bam'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container, entrypoint=entrypoint\n",
    "    samtools view -b ${_input} ${region} > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-supplier",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract samples from expression data generated by RNASeQC\n",
    "[gct_extract_samples]\n",
    "parameter: keep_samples = path\n",
    "input: phenoFile\n",
    "output: f'{_input[0]:nn}.sample_matched.gct.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = \"$[ ]\", stderr = f'{_output:nn}.stderr', stdout = f'{_output:nn}.stdout', container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\")\n",
    "    phenoFile = read_delim($[_input[0]:ar], \"\\t\", col_names = T, comment = \"#\")\n",
    "    sample_lookup = read_delim($[keep_samples:ar], \"\\t\" ,col_names = T, comment = \"#\")\n",
    "    ## Make phenoFile consistant with sampleLookup, remove samples by select()\n",
    "    int = intersect(colnames(phenoFile),unlist(sample_lookup[,1]))\n",
    "    phenoFile_tmp = phenoFile%>%select(c(colnames(phenoFile)[1],all_of(int)))\n",
    "    ## Add 2 header lines, https://github.com/getzlab/rnaseqc/blob/286f99dfd4164d33014241dd4f3149da0cddf5bf/src/RNASeQC.cpp#L426\n",
    "    cat(paste(\"#1.2\\n#\", nrow(phenoFile_tmp), ncol(phenoFile_tmp) - 2, \"\\n\"), file=$[_output:nr], append=FALSE)\n",
    "    phenoFile_tmp%>%write_delim($[_output:nr],delim = \"\\t\",col_names = T, append = T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
