{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Stratified LD Score Regression \n",
    "This notebook implements the pipepline of [S-LDSC](https://www.nature.com/articles/ng.3954) for LD score and functional enrichment analysis.\n",
    "\n",
    "**Important: the S-LDSC implementation comes for the [polyfun](https://github.com/omerwe/polyfun/tree/master) package, not the original LDSC from `bulik/ldsc` GitHub repo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "The purpose of this pipeline is to use LD Score Regression (LDSC) to analyze the heritability and enrichment of genome annotations across GWAS traits. By integrating genome annotation files and GWAS summary statistics, this pipeline allows single tau analysis (individual annotation contributions) and joint tau analysis (independent contributions of multiple annotations after removing shared effects).\n",
    "\n",
    "The pipeline is developed to integrate GWAS summary statistics data, annotation data, and LD reference panel data to compute functional enrichment for each of the epigenomic annotations that the user provides using the S-LDSC model. We will first start off with an introduction, instructions to set up, and the minimal working examples. Then the workflow code that can be run using SoS on any data will be at the end. \n",
    "\n",
    "## A brief review on Stratified LD score regression\n",
    "\n",
    "Here I briefly review LD Score Regression and what it is used for. For more in depth information on LD Score Regression please read the following three papers:\n",
    "\n",
    "1. \"LD Score regression distinguishes confounding from polygenicity in genome-wide association studies\" by Sullivan et al (2015)\n",
    "\n",
    "2. \"Partitioning heritability by functional annotation using genome-wide association summary statistics\" by Finucane et al (2015)\n",
    "\n",
    "3. \"Linkage disequilibrium–dependent architecture of human complex traits shows action of negative selection\" by Gazal et al (2017)\n",
    "\n",
    "As stated in Sullivan et al 2015, confounding factors and polygenic effects can cause inflated test statistics and other methods cannot distinguish between inflation from confounding bias and a true signal. LD Score Regression (LDSC) is a technique that aims to identify the impact of confounding factors and polygenic effects using information from GWAS summary statistics. \n",
    "\n",
    "This approach involves using regression to mesaure the relationship between Linkage Disequilibrium (LD) scores and test statistics of SNPs from the GWAS summary statistics. Variants in LD with a \"causal\" variant show an elevation in test statistics in association analysis proportional to their LD (measured by $r^2$) with the causal variant within a certain window size (could be 1 cM, 1kB, etc.). In contrast, inflation from confounders such as population stratification that occur purely from genetic drift will not correlate with LD. For a polygenic trait, SNPs with a high LD score will have more significant χ2 statistics on average than SNPs with a low LD score. Thus, if we regress the $\\chi^2$ statistics from GWAS against LD Score, the intercept minus one is an estimator of the mean contribution of confounding bias to the inflation in the test statistics. The regression model is known as LD Score regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### LDSC model\n",
    "\n",
    "Under a polygenic assumption, in which effect sizes for variants are drawn independently from distributions with variance proportional to  $1/(p(1-p))$ where p is the minor allele frequency (MAF), the expected $\\chi^2$ statistic of variant j is:\n",
    "\n",
    "$$E[\\chi^2|l_j] = Nh^2l_j/M + Na + 1 \\quad (1)$$\n",
    "\n",
    "where $N$ is the sample size; $M$ is the number of SNPs, such that $h^2/M$ is the average heritability explained per SNP; $a$ measures the contribution of confounding biases, such as cryptic relatedness and population stratification; and $l_j = \\sum_k r^2_{jk}$ is the LD Score of variant $j$, which measures the amount of genetic variation tagged by $j$. A full derivation of this equation is provided in the Supplementary Note of Sullivan et al (2015). An alternative derivation is provided in Supplementary Note of Zhu and Stephens (2017) AoAS.\n",
    "\n",
    "From this we can see that LD Score regression can be used to compute SNP-based heritability for a phenotype or trait, from GWAS summary statistics and does not require genotype information like other methods such as REML do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Stratified LDSC\n",
    "\n",
    "Heritability is the proportion of phenotypic variation (VP) that is due to variation in genetic values (VG) and thus can tell us how much of the difference in observed phenotypes in a sample is due to difference in genetics in the sample. It can also be extended to analyze partitioned heritability for a phenotype/trait split over categories. \n",
    "\n",
    "For Partitioned Heritability or Stratified LD Score Regression (S-LDSC) more power is added to our analysis by leveraging LD Score information as well as using SNPs that haven't reached Genome Wide Significance to partition heritability for a trait over categories which many other methods do not do. \n",
    "\n",
    "\n",
    "S-LDSC relies on the fact that the $\\chi^2$ association statistic for a given SNP includes the effects of all SNPs tagged by this SNP meaning that in a region of high LD in the genome the given SNP from the GWAS represents the effects of a group of SNPs in that region.\n",
    "\n",
    "S-LDSC determines that a category of SNPs is enriched for heritability if SNPs with high LD to that category have more significant $\\chi^2$ statistics than SNPs with low LD to that category.\n",
    "\n",
    "Here, enrichment of a category is defined as the proportion of SNP heritability in the category divided by the proportion of SNPs in that category.\n",
    "\n",
    "More precisely, under a polygenic model, the expected $\\chi^2$ statistic of SNP $j$ is\n",
    "\n",
    "$$E[\\chi^2_j] = N\\sum_CT_Cl(j,C) + Na + 1 \\quad (2)$$\n",
    "\n",
    "where $N$ is sample size, C indexes categories, $ℓ(j, C)$ is the LD score of SNP j with respect to category $l(j,C) = \\sum_{k\\epsilon C} r^2_{jk}$, $a$ is a term that measures the contribution of confounding biases, and if the categories are disjoint, $\\tau_C$ is the per-SNP heritability in category $C$; if the categories overlap, then the per-SNP heritability of SNP j is $\\sum_{C:j\\epsilon C} \\tau_C$.  Equation 2 allows us to estimate $\\tau_C$ via a (computationally simple) multiple regression of $\\chi^2$ against $ℓ(j, C)$, for either a quantitative or case-control study. \n",
    "\n",
    "To see how these methods have been applied to real world data as well as a further discussion on methods and comparisons to other methods please read the three papers listed at the top of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Tau Estimation and Enrichment Analysis\n",
    "To quantify the contribution of functional annotations to trait heritability and evaluate their statistical significance, while accounting for linkage disequilibrium (LD) structure and annotation-specific properties.\n",
    "\n",
    "#### Tau (τ):\n",
    "The standardized per-SNP heritability contribution of an annotation.\n",
    "\n",
    "In joint-annotation analysis:\n",
    "Represents the *independent contribution* of an annotation after controlling for overlapping effects of other annotations.\n",
    "\n",
    "### Statistical Workflow\n",
    "\n",
    "#### 1. Inputs\n",
    "- `.annot.gz`: Annotation files defining SNP membership in functional categories.\n",
    "- `.part_delete`: Jackknife delete-values for τ estimates (200 genomic blocks × annotations).\n",
    "- `.results`: Regression coefficients (τ), enrichment statistics, and proportions.\n",
    "- `.log`: Total heritability ($h_g^2$) and quality control metrics.\n",
    "\n",
    "#### 2. Tau Standardization\n",
    "For each annotation *j*:\n",
    "\n",
    "$$\\tau_j^{std} = \\tau_j \\times \\frac{sd_j \\cdot M_{ref}}{h_g^2}$$\n",
    "\n",
    "- $sd_j$: Standard deviation of annotation *j* across SNPs.\n",
    "- $M_{ref} = 5,961,159$: Reference SNP count for genome-wide scaling.\n",
    "- $h_g^2$: Total heritability (normalizes τ to per-unit contribution).\n",
    "- $\\tau_j^{std}$ defined as the additive change in per-SNP heritability associated to a 1 standard deviation increase in the value of the annotation, divided by the average per-SNP heritability over all SNPs for the trait.\n",
    "#### 3. Enrichment Calculation\n",
    "\n",
    "Enrichment is defined as the proportion of heritability explained by variants in the target annotation divided by the proportion of variants in the annotation.\n",
    "\n",
    "##### a. Direct Enrichment Statistics\n",
    "For each annotation *j*:\n",
    "\n",
    "$$E_j = \\frac{\\text{Prop.\\_h2}_j}{\\text{Prop.\\_SNPs}_j}= \\tau_j \\times \\frac{M_{ref}}{h_g^2}$$\n",
    "\n",
    "Where:\n",
    "- $\\text{Prop.\\_h2}_j$: Proportion of heritability explained by annotation *j*\n",
    "- $\\text{Prop.\\_SNPs}_j$: Proportion of SNPs in annotation *j*\n",
    "\n",
    "##### b. Standardized Enrichment Statistics\n",
    "\n",
    "The goal is to compute the p-value based on the assumption of a normal distribution.\n",
    "\n",
    "$$\\text{EnrichStat}_j = \\frac{h_g^2}{M_{ref}} \\times \\left[\\frac{\\text{Prop.\\_h2}_j}{\\text{Prop.\\_SNPs}_j} - \\frac{1-\\text{Prop.\\_h2}_j}{1-\\text{Prop.\\_SNPs}_j}\\right]$$\n",
    "\n",
    "Standard error derivation:\n",
    "$$Z_j = \\Phi^{-1}(\\text{Enrichment\\_p}_j/2)$$\n",
    "$$SE_{\\text{EnrichStat}_j} = \\frac{\\text{EnrichStat}_j}{Z_j}$$\n",
    "\n",
    "Where:\n",
    "- $\\Phi^{-1}$: Inverse normal cumulative distribution function\n",
    "- $\\text{Enrichment\\_p}_j$: Enrichment p-value from .results file\n",
    "\n",
    "$$\n",
    "Z_j = \\frac{\\text{EnrichStat}_j}{SE_{\\text{EnrichStat}_j}}\n",
    "$$\n",
    "\n",
    "\n",
    "### Meta-Analysis of Partitioned Heritability\n",
    "\n",
    "### Purpose\n",
    "To integrate τ estimates across multiple traits or annotation groups, improving power and generalizability.\n",
    "\n",
    "#### Random-Effects Meta-Analysis\n",
    "\n",
    "Models heterogeneity across traits:\n",
    "\n",
    "$$\\tau_{meta} = \\frac{\\sum w_i\\tau_i}{\\sum w_i}, \\quad w_i = \\frac{1}{SE_i^2 + \\sigma^2}$$\n",
    "\n",
    "- $\\sigma^2$: Between-trait variance component.\n",
    "- $SE_i$: Standard error of τ for trait *i*.\n",
    "\n",
    "For enrichment meta-analysis:\n",
    "- Effect sizes use direct enrichment statistics $(E_j, SE_{E_j})$\n",
    "- P-values derived from standardized statistics meta-analysis $({\\text{EnrichStat}_j}, SE_{\\text{EnrichStat}_j})$\n",
    "\n",
    "Z-score:\n",
    "$$Z = \\frac{\\tau_{meta}}{SE_{meta}}$$\n",
    "\n",
    "P-value:\n",
    "$$p = 2 \\times \\Phi(-|Z|)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "### 1. Annotation File\n",
    "\n",
    "- **Purpose**: Specifies genome annotation files for single or joint tau analysis.\n",
    "- **Formats**:\n",
    "    - Text file (`.txt`) containing paths to annotation files, annotation files can be rds/tsv/txt format.\n",
    "    - Alternatively, specific annotation files for a single chromosome can be provided directly.\n",
    "    - **Examples**:\n",
    "        - **Single Annotation** (`joint_tau = False`):\n",
    "            \n",
    "            ```\n",
    "            #id   path\n",
    "            1     /home/al4225/project/quantile_twas/analysis/output/157genes/step2_rq_lr_summary/enrich_info_by_context_class/AC_DeJager_eQTL.unique_qr.rds\n",
    "            22    /home/al4225/project/quantile_twas/analysis/output/157genes/step2_rq_lr_summary/enrich_info_by_context_class/AC_DeJager_eQTL.unique_qr.rds\n",
    "            \n",
    "            ```\n",
    "            \n",
    "        - **Joint Annotation** (`joint_tau = True`):\n",
    "            \n",
    "            ```\n",
    "            #id   path1                     path2\n",
    "            1     /path/to/annotation1.rds   /path/to/annotation2.rds\n",
    "            22    /path/to/annotation1.rds   /path/to/annotation2.rds\n",
    "            \n",
    "            ```\n",
    "            \n",
    "    - **Example Files**:\n",
    "\n",
    "    - **Format** (the score column is optional, if this column doen't exit, add score as 1):\n",
    "        - 1) is_range = False\n",
    "        ```\n",
    "            chr   pos   score\n",
    "            1    10001   1\n",
    "            1    10002   1\n",
    "        ```\n",
    "        - 2) is_range = True\n",
    "        ```\n",
    "            chr   start   end   score\n",
    "            1    10001   20001  1\n",
    "            1    30001   40001  1\n",
    "        ```        \n",
    "### 2. Reference Annotation File\n",
    "\n",
    "- **Purpose**: Provides reference annotation files in `.annot.gz` format for each chromosome.\n",
    "- **Formats**:\n",
    "    - Text file listing annotation files for all chromosomes.\n",
    "    - Alternatively, files for specific chromosomes can be provided directly.\n",
    "    - **Example**: `/home/al4225/project/quantile_twas/analysis/SLDSC/data/reference_annotation.txt`\n",
    "        \n",
    "        ```\n",
    "        #id   path\n",
    "        1     /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/example_anno/ABC_Road_GI_BRN.1.annot.gz\n",
    "        22    /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/example_anno/ABC_Road_GI_BRN.22.annot.gz\n",
    "        \n",
    "        ```\n",
    "        \n",
    "\n",
    "### 3. Genome Reference File\n",
    "\n",
    "- **Purpose**: Specifies genome reference `.bed` files PLINK format for each chromosome.\n",
    "- **Formats**:\n",
    "    - Text file listing reference files for all chromosomes.\n",
    "    - Alternatively, files for specific chromosomes can be provided directly.\n",
    "    - **Example**: `/home/al4225/project/quantile_twas/analysis/SLDSC/data/genome_reference_bfile.txt`\n",
    "        \n",
    "        ```\n",
    "        #id   path\n",
    "        1     /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/plink_files/1000G.EUR.hg38.1.bed\n",
    "        22    /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/plink_files/1000G.EUR.hg38.22.bed\n",
    "        \n",
    "        ```\n",
    "        \n",
    "\n",
    "### 4. SNP List\n",
    "\n",
    "- **File Path**: `/mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/list.txt`\n",
    "- **Purpose**: Specifies SNPs for LDSC analysis.\n",
    "- **Format**: A list of `rsid`s (e.g., 1217311 rows).\n",
    "    \n",
    "    ```\n",
    "    rs12345\n",
    "    rs67890\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "\n",
    "### 5. ALL Group Files\n",
    "\n",
    "- **Purpose**: GWAS summary statistics containning all sumtats.\n",
    "- **Example Files**:\n",
    "    \n",
    "    `/home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt`\n",
    "    \n",
    "\n",
    "- **Format**:\n",
    "    \n",
    "    ```\n",
    "    CAD_META.filtered.sumstats.gz\n",
    "    UKB.Lym.BOLT.sumstats.gz\n",
    "    ```\n",
    "    \n",
    "---\n",
    "\n",
    "## Workflow Steps\n",
    "\n",
    "### Step 1: Annotation File Processing\n",
    "\n",
    "- **Purpose**: Generate LD score files and genome annotations for each chromosome.\n",
    "- **Inputs**:\n",
    "    - `annotation_file`\n",
    "    - `reference_anno_file`\n",
    "    - `genome_ref_file`\n",
    "- **Key Parameters**:\n",
    "    - `joint_tau = False`: Single annotation analysis (one column `path`).\n",
    "    - `joint_tau = True`: Joint annotation analysis (multiple `path1`, `path2` columns starting with `path`).\n",
    "    - `chromosome`: Optional, restricts processing to specific chromosomes.\n",
    "- **Outputs**:\n",
    "    - Per chromosome:\n",
    "        - `.annot.gz`: Genome annotation scores.\n",
    "        - `.l2.ldscore.parquet`: LD score file.\n",
    "        - `.l2.M`: Total SNP count.\n",
    "        - `.l2.M_5_50`: SNP count in LD blocks.\n",
    "        - `.log`: Processing logs.\n",
    "\n",
    "\n",
    "### Step 2: Munge Summary Statistics\n",
    "\n",
    "- **Purpose**: Preprocess GWAS summary statistics into LDSC-compatible format.\n",
    "- **Inputs**:\n",
    "    - GWAS summary statistics.\n",
    "    - HapMap3 SNP list.\n",
    "- **Outputs**:\n",
    "    - Harmonized GWAS summary statistics.\n",
    "\n",
    "\n",
    "### Step 3: Heritability Analysis\n",
    "\n",
    "- **Purpose**: Estimate heritability (`h²`) of traits using LDSC regression.\n",
    "- **Inputs**:\n",
    "    - `sumstat_dir`: GWAS summary statistics.\n",
    "    - `target_anno_dir`: Annotation directory.\n",
    "    - `baseline_ld_dir`: Baseline LD scores.\n",
    "- **Outputs**:\n",
    "    - Per trait:\n",
    "        - `.results`: Heritability estimates and enrichment (`Enrichment`, `Prop_h2`).\n",
    "        - `.log`: Analysis logs.\n",
    "        - `.part_delete`: SNP-level contributions for annotations (target + baseline).\n",
    "        - `.delete`: Aggregate contributions across SNPs for each annotation.\n",
    "\n",
    "\n",
    "### Step 4: Initial Processed Statistics\n",
    "\n",
    "- **Purpose**: Calculate tau statistics and prepare intermediate files for meta-analysis.\n",
    "- **Inputs**:\n",
    "    - All GWAS traits and their annotation scores.\n",
    "- **Outputs**:\n",
    "    - RDS file containing:\n",
    "        - `single_tau`: Contribution of each annotation to heritability.\n",
    "        - `enrichment`: Enrichment ratio of heritability to SNP density.\n",
    "        - `joint_tau`: Independent effect of each annotation after removing shared contributions.\n",
    "\n",
    "\n",
    "### Step 5: Meta Analysis\n",
    "\n",
    "- **Purpose**: Perform meta-analysis across trait groups.\n",
    "- **Inputs**:\n",
    "    - `trait_group_paths`: Paths to files listing traits in each group.\n",
    "    - `trait_group_names`: Names of trait groups.\n",
    "- **Outputs**:\n",
    "    - Results for each trait group:\n",
    "        - `mean`, `p-value`, `SE`.\n",
    "    - Includes:\n",
    "        - `single_tau`: Single annotation effects.\n",
    "        - `enrichment`: Heritability enrichment.\n",
    "        - `joint_tau`: Independent contributions of annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## MWE: \n",
    "### 1. make_annotation_files_ldscore\n",
    "annotation file can be a txt file with #id, and path1 path2 ..., also can be rds files seperate by ',' \n",
    "#### 1.1 single tau analysis, with one annotation as a input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " # case 1: txt file as input\n",
    " sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/ldsc.ipynb make_annotation_files_ldscore \\\n",
    "    --annotation_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/quantile_qtl_annotation/157genes/AC_DeJager_eQTL.shared_heter_qr.txt \\\n",
    "    --reference_anno_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/reference_annotation.txt \\\n",
    "    --genome_ref_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/genome_reference_bfile.txt \\\n",
    "    --snp_list /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/list.txt \\\n",
    "    --annotation_name AC_DeJager_eQTL.shared_heter_qr \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output --chromosome 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Alternatively, we can also use files with specific chromosome, instead of txt list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# single file format\n",
    " sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/ldsc.ipynb make_annotation_files_ldscore \\\n",
    "    --annotation_file /home/al4225/project/quantile_twas/analysis/output/157genes/step2_rq_lr_summary/enrich_info_by_context_class/AC_DeJager_eQTL.unique_qr.rds \\\n",
    "    --reference_anno_file /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/example_anno/ABC_Road_GI_BRN.1.annot.gz \\\n",
    "    --genome_ref_file /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/plink_files/1000G.EUR.hg38.1.bed \\\n",
    "    --snp_list /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/list.txt \\\n",
    "    --annotation_name AC_DeJager_eQTL.shared_heter_qr \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output --chromosome 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "##### 1.2 joint tau\n",
    "with more than one annotation as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# input case1: txt file format\n",
    " sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/ldsc.ipynb make_annotation_files_ldscore \\\n",
    "    --annotation_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/quantile_qtl_annotation/test/AC_DeJager_eQTL.joint_tau_example.txt \\\n",
    "    --reference_anno_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/reference_annotation.txt \\\n",
    "    --genome_ref_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/genome_reference_bfile.txt \\\n",
    "    --snp_list /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/list.txt \\\n",
    "    --annotation_name joint_test \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output --joint_tau --chromosome 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    " # input case 2: single file format, annotation files separate by ','.\n",
    " sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/ldsc.ipynb make_annotation_files_ldscore \\\n",
    "    --annotation_file /home/al4225/project/quantile_twas/analysis/output/157genes/step2_rq_lr_summary/enrich_info_by_context_class/AC_DeJager_eQTL.unique_qr.rds,/home/al4225/project/quantile_twas/analysis/output/157genes/step2_rq_lr_summary/enrich_info_by_context_class/AC_DeJager_eQTL.shared_homo.rds \\\n",
    "    --reference_anno_file /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/example_anno/ABC_Road_GI_BRN.1.annot.gz \\\n",
    "    --genome_ref_file /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/plink_files/1000G.EUR.hg38.1.bed \\\n",
    "    --snp_list /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/list.txt \\\n",
    "    --annotation_name joint_test \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output/mwe/joint_tau --joint_tau --chromosome 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### 2. get_heritability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/ldsc.ipynb get_heritability \\\n",
    "    --target_anno_dir /home/al4225/project/quantile_twas/analysis/SLDSC/output/AC_DeJager_eQTL.unique_qr/ \\\n",
    "    --sumstat_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/sumstat \\\n",
    "    --baseline_ld_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/baselineLD_v2.2 \\\n",
    "    --frqfile_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/plink_files \\\n",
    "    --weights_dir /home/al4225/project/quantile_twas/analysis/SLDSC/data/weights \\\n",
    "    --annotation_name AC_DeJager_eQTL.unique_qr \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output/AC_DeJager_eQTL.unique_qr/heritability \\\n",
    "    --all_traits_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### 3. processed_stats\n",
    "#### 3.1 single tau analysis, with one annotation as a input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# processed stats cwd has to be the same with get_heritability\n",
    "sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/ldsc.ipynb processed_stats \\\n",
    "    --target_anno_dir /home/al4225/project/quantile_twas/analysis/SLDSC/output/AC_DeJager_eQTL.unique_qr/test_anno_joint/AC_DeJager_eQTL.unique_qr \\\n",
    "    --baseline_ld_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/baselineLD_v2.2 \\\n",
    "    --annotation_name AC_DeJager_eQTL.unique_qr \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output/AC_DeJager_eQTL.unique_qr/heritability \\\n",
    "    --trait_group_paths \"/home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_brain_trait.txt /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_blood_trait.txt\" \\\n",
    "    --trait_group_names \"All Brain Blood\" \\\n",
    "    --all_traits_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt -s build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "##### 3.2 joint tau\n",
    "with more than one annotation as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# processed stats cwd has to be the same with get_heritability\n",
    "sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/ldsc.ipynb processed_stats \\\n",
    "    --target_anno_dir /home/al4225/project/quantile_twas/analysis/SLDSC/output/joint_test/ \\\n",
    "    --baseline_ld_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/baselineLD_v2.2 \\\n",
    "    --annotation_name joint_test \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output/joint_test/heritability \\\n",
    "    --trait_group_paths \"/home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_brain_trait.txt /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_blood_trait.txt\" \\\n",
    "    --trait_group_names \"All Brain Blood\" \\\n",
    "    --all_traits_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt -s build --joint_tau --joint_annotation_names \"AC.eqtl.unique_qr AC.eqtl.shared_heter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to the work directory of the analysis.\n",
    "parameter: cwd = path('output')\n",
    "# Prefix for the analysis output\n",
    "parameter: annotation_name = str\n",
    "parameter: joint_annotation_names = [] #if joint analysis is used, pass annotation names\n",
    "parameter: joint_tau = False\n",
    "parameter: python_exec = \"python\" # e.g. \"/home/you/.conda/envs/polyfun/bin/python\"\n",
    "parameter: polyfun_path   = path # e.g. \"/home/you/tools/polyfun\"\n",
    "\n",
    "# for make_annotation_files_ldscore workflow:\n",
    "parameter: annotation_file = path()\n",
    "parameter: reference_anno_file = path()\n",
    "parameter: genome_ref_file = path() # with .bed \n",
    "parameter: chromosome = []\n",
    "parameter: snp_list = path()\n",
    "parameter: ld_wind_kb = 1000\n",
    "\n",
    "# for get_heritability workflow\n",
    "parameter: all_traits_file = path() # txt file, each row contains all GWAS summary statistics name: e.g. CAD_META.filtered.sumstats.gz\n",
    "parameter: sumstat_dir = path() # Directory containing GWAS summary statistics\n",
    "parameter: target_anno_dir = path()  # Directory containing target annotation files: output of ldscore\n",
    "parameter: baseline_ld_dir = path()  # Directory containing baseline LD score files\n",
    "parameter: frqfile_dir = path()  # Directory containing allele frequency files\n",
    "parameter: weights_dir = path()  # Directory containing LD weights\n",
    "\n",
    "# Number of threads\n",
    "parameter: numThreads = 16\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "parameter: walltime = '12h'\n",
    "parameter: mem = '16G'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Make Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[make_annotation_files_ldscore]\n",
    "parameter: score_column = 3\n",
    "parameter: is_range = False # parameter to handle range expansion\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))\n",
    "\n",
    "# Process input files based on file type\n",
    "if (str(annotation_file).endswith(('rds', 'tsv', 'txt', 'tsv.gz', 'txt.gz')) and \n",
    "    str(reference_anno_file).endswith('annot.gz')):\n",
    "    # Case 1: Direct file paths\n",
    "    # joint tau: annotation_fills: rds/tsv/txt files separete with \",\"    \n",
    "    if joint_tau:\n",
    "        target_files = str(annotation_file).split(',')\n",
    "        input_files = [[*target_files, reference_anno_file, genome_ref_file]]\n",
    "    else:\n",
    "        # single annotation\n",
    "        input_files = [[annotation_file, reference_anno_file, genome_ref_file]]\n",
    "    \n",
    "    if len(chromosome) > 0:\n",
    "        input_chroms = [int(x) for x in chromosome]\n",
    "    else:\n",
    "        input_chroms = [0]\n",
    "else:\n",
    "    # Case 2: Files with #id and cols starting with path columns\n",
    "    target_files = pd.read_csv(annotation_file, sep=\"\\t\")\n",
    "    reference_files = pd.read_csv(reference_anno_file, sep=\"\\t\")\n",
    "    genome_ref_files = pd.read_csv(genome_ref_file, sep=\"\\t\")\n",
    "    \n",
    "    # Standardize #id \n",
    "    target_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in target_files[\"#id\"].astype(str)]\n",
    "    reference_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in reference_files[\"#id\"].astype(str)]\n",
    "    genome_ref_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in genome_ref_files[\"#id\"].astype(str)]\n",
    "    \n",
    "    # process target annotation files\n",
    "    path_columns = [col for col in target_files.columns if col.startswith('path')]\n",
    "    for col in path_columns:\n",
    "        target_files[col] = target_files[col].apply(lambda x: adapt_file_path(x, annotation_file))\n",
    "    \n",
    "    # process reference and genome files\n",
    "    reference_files[\"path\"] = reference_files[\"path\"].apply(lambda x: adapt_file_path(x, reference_anno_file))\n",
    "    genome_ref_files[\"path\"] = genome_ref_files[\"path\"].apply(lambda x: adapt_file_path(x, genome_ref_file))\n",
    "    \n",
    "    # Merge the files based on #id\n",
    "    input_files = target_files.merge(reference_files, on=\"#id\").merge(genome_ref_files, on=\"#id\")\n",
    "    \n",
    "    # Filter by specified chromosomes, if any\n",
    "    if len(chromosome) > 0:\n",
    "        input_files = input_files[input_files['#id'].isin(chromosome)]\n",
    "    \n",
    "    # Extract relevant columns as a list of file paths\n",
    "    input_files = input_files.values.tolist()\n",
    "    input_chroms = [x[0] for x in input_files]  # Chromosome IDs\n",
    "    \n",
    "    if joint_tau:\n",
    "        # joint tau, keep all path columns\n",
    "        input_files = [[*x[1:len(path_columns)+1], x[-2], x[-1]] for x in input_files]\n",
    "    else:\n",
    "        # single annotation\n",
    "        input_files = [[x[1], x[-2], x[-1]] for x in input_files]\n",
    "        \n",
    "input: input_files, group_by = len(input_files[0]), group_with = \"input_chroms\"\n",
    "output: dict([\n",
    "   ('annot', f'{cwd:a}/{annotation_name}/{annotation_name}.{input_chroms[_index]}.annot.gz'),\n",
    "   ('ldscore', f'{cwd:a}/{annotation_name}/{annotation_name}.{input_chroms[_index]}.l2.ldscore.parquet'),\n",
    "   ('Mfile',  f'{cwd:a}/{annotation_name}/{annotation_name}.{input_chroms[_index]}.l2.M') # set output for generation .M files\n",
    "])\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bnn}'\n",
    "\n",
    "R: expand= \"${ }\", stderr = f'{_output[\"annot\"]}.stderr', stdout = f'{_output[\"annot\"]}.stdout'\n",
    "    library(data.table)\n",
    "\n",
    "    # Function to clean chromosome notation\n",
    "    clean_chr <- function(x) {\n",
    "        as.numeric(gsub(\"^chr\", \"\", x))\n",
    "    }\n",
    "\n",
    "    # Function to process range data - only expands ranges for the specified chromosome\n",
    "    process_range_data <- function(data, chr_value) {\n",
    "        # First filter for the specific chromosome\n",
    "        data$chr <- clean_chr(data$chr)\n",
    "        data <- data[data$chr == chr_value,]\n",
    "        \n",
    "        if(nrow(data) == 0) {\n",
    "            return(NULL)\n",
    "        }\n",
    "        \n",
    "        # Only expand ranges for the matched chromosome\n",
    "        expanded_rows <- lapply(1:nrow(data), function(j) {\n",
    "            row <- data[j,]\n",
    "            pos_seq <- seq(row$start, row$end-1)\n",
    "            \n",
    "            # Create base data frame with chr and pos\n",
    "            result <- data.frame(\n",
    "                chr = rep(row$chr, length(pos_seq)),\n",
    "                pos = pos_seq\n",
    "            )\n",
    "            \n",
    "            # Add additional columns if they exist\n",
    "            if(ncol(data) > 3) {\n",
    "                for(col in 4:ncol(data)) {\n",
    "                    result[[names(data)[col]]] <- rep(row[[col]], length(pos_seq))\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return(result)\n",
    "        })\n",
    "        result <- unique(rbindlist(expanded_rows))\n",
    "        return(result)\n",
    "    }\n",
    "\n",
    "    # Main annotation processing function\n",
    "    process_annotation <- function(target_anno, ref_anno, score_column_value) {\n",
    "        target_anno <- as.data.frame(target_anno)\n",
    "        ref_anno <- as.data.frame(ref_anno)\n",
    "        \n",
    "        # Clean chromosome notation\n",
    "        target_anno$chr <- clean_chr(target_anno$chr)\n",
    "        ref_anno$CHR <- clean_chr(ref_anno$CHR)\n",
    "        \n",
    "        chr_value = unique(ref_anno$CHR)\n",
    "        pos <- which(target_anno$chr == chr_value)\n",
    "        anno_scores <- rep(0, nrow(ref_anno))\n",
    "        \n",
    "        match_pos <- match(target_anno$pos, ref_anno$BP)\n",
    "        valid_pos <- as.numeric(na.omit(match_pos))\n",
    "        \n",
    "        if (score_column_value <= ncol(target_anno)) {\n",
    "            anno_scores[valid_pos] <- target_anno[[score_column_value]][!is.na(match_pos)]\n",
    "        } else {\n",
    "            anno_scores[valid_pos] <- 1\n",
    "            print(\"Warning: Score column does not exist. Setting scores to 1\")\n",
    "        }\n",
    "        \n",
    "        return(anno_scores)\n",
    "    }\n",
    "\n",
    "    # Read reference annotation file\n",
    "    ref_anno <- fread(${_input[-2]:ar})\n",
    "    ref_anno <- as.data.frame(ref_anno)\n",
    "    if(\"ANNOT\" %in% colnames(ref_anno)) {\n",
    "        ref_anno <- ref_anno[,-which(colnames(ref_anno) == \"ANNOT\")]\n",
    "    }\n",
    "    result_anno <- ref_anno\n",
    "\n",
    "    score_column_value <- ${score_column}  \n",
    "    is_joint <- ${\"TRUE\" if joint_tau else \"FALSE\"}\n",
    "    is_range <- ${\"TRUE\" if is_range else \"FALSE\"}\n",
    "\n",
    "    if(is_joint) {\n",
    "        target_files = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[:-2]])})\n",
    "        for(i in seq_along(target_files)) {\n",
    "            # Read file based on format\n",
    "            if(endsWith(target_files[i], \"rds\")) {\n",
    "                target_anno <- readRDS(target_files[i])\n",
    "            } else {  # Handle TSV/TXT files\n",
    "                target_anno <- fread(target_files[i])\n",
    "                \n",
    "                # Process range data if needed\n",
    "                if(is_range) {\n",
    "                    names(target_anno)[1:3] <- c(\"chr\", \"start\", \"end\")\n",
    "                    target_anno <- process_range_data(target_anno, unique(ref_anno$CHR))\n",
    "                    if(is.null(target_anno)) {\n",
    "                        anno_scores <- rep(0, nrow(ref_anno))\n",
    "                    } else {\n",
    "                        anno_scores <- process_annotation(target_anno, ref_anno, score_column_value)\n",
    "                    }\n",
    "                } else {\n",
    "                    names(target_anno)[1:2] <- c(\"chr\", \"pos\")\n",
    "                    anno_scores <- process_annotation(target_anno, ref_anno, score_column_value)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(paste(\"Processing annotation file\", i))\n",
    "            result_anno[[paste0(\"ANNOT_\", i)]] <- anno_scores\n",
    "        }\n",
    "    } else {\n",
    "        # Read file based on format\n",
    "        if(endsWith(${_input[0]:ar}, \"rds\")) {\n",
    "            target_anno <- readRDS(${_input[0]:ar})\n",
    "        } else {  # Handle TSV/TXT files\n",
    "            target_anno <- fread(${_input[0]:ar})\n",
    "            \n",
    "            # Process range data if needed\n",
    "            if(is_range) {\n",
    "                names(target_anno)[1:3] <- c(\"chr\", \"start\", \"end\")\n",
    "                target_anno <- process_range_data(target_anno, unique(ref_anno$CHR))\n",
    "                if(is.null(target_anno)) {\n",
    "                    anno_scores <- rep(0, nrow(ref_anno))\n",
    "                } else {\n",
    "                    anno_scores <- process_annotation(target_anno, ref_anno, score_column_value)\n",
    "                }\n",
    "            } else {\n",
    "                names(target_anno)[1:2] <- c(\"chr\", \"pos\")\n",
    "                anno_scores <- process_annotation(target_anno, ref_anno, score_column_value)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        result_anno$ANNOT <- anno_scores\n",
    "    }\n",
    "\n",
    "    # Write results\n",
    "    fwrite(result_anno, ${_output[\"annot\"]:nr}, \n",
    "        quote=FALSE, col.names=TRUE, row.names=FALSE, sep=\"\\t\")\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[\"annot\"]:nnn}.stderr', stdout = f'{_output[\"annot\"]:nnn}.stdout'\n",
    "   gzip -f $[_output[\"annot\"]:n] \n",
    "\n",
    "bash: expand=\"${ }\", stderr = f'{_output[1]}.stderr', stdout = f'{_output[1]}.stdout'\n",
    "    echo \"Running chromosome ${input_chroms[_index]}\"  \n",
    "    ${python_exec} ${polyfun_path}/compute_ldscores.py \\\n",
    "      --annot ${_output[\"annot\"]:a} \\\n",
    "      --bfile ${genotype_ref_file:d}/ADSP_chr${input_chroms[_index]} \\\n",
    "      --ld-wind-kb ${ld_wind_kb} \\\n",
    "      --out ${_output[\"ldscore\"]} \\\n",
    "      --allow-missing \n",
    "\n",
    "python: expand=True, stderr=f'{_output[\"Mfile\"]}.stderr', stdout=f'{_output[\"Mfile\"]}.stdout'\n",
    "    # https://github.com/omerwe/polyfun/wiki/2.-Using-and-creating-functional-annotations#creating-your-own-annotations\n",
    "    import pyarrow.parquet as pq\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    PREAMBLE = ['CHR','SNP','BP','A1','A2','CM']\n",
    "    parquet_file = Path(r\"${_output['ldscore']:ar}\")\n",
    "    m_file       = Path(r\"${_output['Mfile']:ar}\")\n",
    "    tbl = pq.read_table(parquet_file).combine_chunks()\n",
    "    df  = tbl.to_pandas()\n",
    "    # remove prefix and calculate sums\n",
    "    anno_df = df.drop(columns=[c for c in PREAMBLE if c in df.columns], errors='ignore')\n",
    "    col_sums = anno_df.values.sum(axis=0)\n",
    "    # save as M file\n",
    "    np.savetxt(m_file, col_sums.reshape(1,-1), fmt='%i', delimiter=' ')\n",
    "    print(f\"[OK] wrote {m_file} with {col_sums.size} annotation columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Munge Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# The script munge_polyfun_sumstats.py automatically detects whether signed statistics \n",
    "# (Z, BETA/SE, etc.) are present and computes Z-scores if needed. \n",
    "[munge_sumstats_polyfun]\n",
    "parameter: sumstats  = path\n",
    "parameter: n       = 0\n",
    "parameter: min_info = 0.6\n",
    "parameter: min_maf  = 0.001\n",
    "parameter: keep_hla = False\n",
    "parameter: chi2_cut = 30\n",
    "input: sumstats\n",
    "output: f\"{_input:n}.munged.gz\"\n",
    "bash: expand=True, stderr=f'{_output:nn}.stderr', stdout=f'{_output:nn}.stdout'\n",
    "    {python_exec} {polyfun_path}/munge_polyfun_sumstats.py \\\n",
    "        --sumstats {_input} \\\n",
    "        --out {_output} \\\n",
    "        {'--n {}'.format(n) if n>0 else ''} \\\n",
    "        {'--min-info {}'.format(min_info)} \\\n",
    "        {'--min-maf {}'.format(min_maf)} \\\n",
    "        {'--chi2-cutoff {}'.format(chi2_cut)} \\\n",
    "        {'--keep-hla' if keep_hla else ''} \\\n",
    "        --remove-strand-ambig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Calculate Functional Enrichment using Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_heritability]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "\n",
    "bash: expand = \"${ }\"\n",
    "    output_dir=\"${cwd}\"\n",
    "    mkdir -p $output_dir\n",
    "    while read -r trait; do\n",
    "        ${python_exec} ${polyfun_path}/ldsc.py \\\n",
    "            --h2 ${sumstat_dir}/$trait \\\n",
    "            --ref-ld-chr ${target_anno_dir}/${annotation_name}.,${baseline_ld_dir}/baseline_chr \\\n",
    "            --out ${cwd}/$trait \\\n",
    "            --overlap-annot \\\n",
    "            --w-ld-chr ${weights_dir}/weights_chr \\\n",
    "            --not-M-5-50\n",
    "    done < ${all_traits_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[processed_stats_1]\n",
    "output: f'{cwd:a}/{annotation_name}.joint_tau.initial_processed_stats.rds' if joint_tau else f'{cwd:a}/{annotation_name}.single_tau.initial_processed_stats.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "\n",
    "R: expand= \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout'\n",
    "    library(data.table)\n",
    "    print(paste(\"Joint tau analysis:\", ${str(joint_tau).upper()}))\n",
    "    is_joint <- ${\"TRUE\" if joint_tau else \"FALSE\"}\n",
    "    print(paste(\"Is joint analysis:\", is_joint))\n",
    "       \n",
    "    calculate_sd_annot <- function(cell_path, annot_index = 1) {\n",
    "    ll <- list.files(cell_path, pattern = \"\\\\.annot\\\\.gz$\", full.names = TRUE)\n",
    "    num = numeric(length(annot_index))\n",
    "    den = 0\n",
    "    \n",
    "    for(m in ll) {\n",
    "        dat <- fread(m, fill = TRUE)\n",
    "        if(length(annot_index) > 1) {\n",
    "            cols <- paste0(\"ANNOT_\", annot_index) \n",
    "            num <- num + ((nrow(dat)-1) * sapply(dat[, ..cols], var))\n",
    "        } else {\n",
    "            if((4 + annot_index) > ncol(dat)) stop(paste(\"Index out of range:\", m))\n",
    "            num <- num + ((nrow(dat)-1) * var(dat[, 4+annot_index, with=FALSE]))\n",
    "        }\n",
    "        den <- den + (nrow(dat)-1)\n",
    "    }\n",
    "    sqrt(num/den)\n",
    "    }\n",
    "\n",
    "    check_file_exists <- function(file_path) {\n",
    "        if (!file.exists(file_path)) {\n",
    "            warning(paste(\"File does not exist:\", file_path))\n",
    "            return(FALSE)\n",
    "        }\n",
    "        return(TRUE)\n",
    "    }\n",
    "\n",
    "    # Process single trait\n",
    "    process_trait <- function(trait, cwd) {\n",
    "    files <- list(\n",
    "        result = paste0(cwd, \"/\", trait, \".results\"),\n",
    "        log = paste0(cwd, \"/\", trait, \".log\"),\n",
    "        delete = paste0(cwd, \"/\", trait, \".part_delete\")\n",
    "    )\n",
    "    \n",
    "    if(!all(sapply(files, file.exists))) {\n",
    "        warning(paste(\"Missing files for trait:\", trait))\n",
    "        return(NULL)\n",
    "    }\n",
    "\n",
    "    tryCatch({\n",
    "        results <- fread(files$result)\n",
    "        h2g <- as.numeric(gsub(\".*h2: ([0-9.]+).*\", \"\\\\1\", \n",
    "                            grep(\"Total Observed scale h2:\", readLines(files$log), value=TRUE)))\n",
    "        delete_values <- fread(files$delete)\n",
    "        \n",
    "        list(trait = trait, h2g = h2g, results = results, delete_values = delete_values)\n",
    "    }, error = function(e) {\n",
    "        warning(paste(\"Error processing trait:\", trait, \"-\", e$message))\n",
    "        NULL\n",
    "    })\n",
    "    }\n",
    "\n",
    "    # Process tau analysis\n",
    "    process_tau_analysis <- function(trait_data, sd_annots, base_index = NULL, is_joint = FALSE) {\n",
    "    Mref <- 5961159\n",
    "    \n",
    "    if(is_joint) {\n",
    "        indices <- 1:(nrow(trait_data$results) - base_index)\n",
    "        sc_matrices <- matrix(0, nrow=nrow(trait_data$delete_values), ncol=length(indices))\n",
    "        \n",
    "        for(i in seq_along(indices)) {\n",
    "            coef <- sd_annots[i] * Mref / trait_data$h2g\n",
    "            sc_matrices[,i] <- trait_data$delete_values[[indices[i]]] * coef\n",
    "        }\n",
    "        \n",
    "        list(joint_stats = list(\n",
    "            h2g = trait_data$h2g,\n",
    "            sd_annots = sd_annots,\n",
    "            sc_matrices = sc_matrices\n",
    "        ))\n",
    "    } else {\n",
    "        coef <- sd_annots * Mref / trait_data$h2g\n",
    "        sc_matrix <- trait_data$delete_values[[1]] * coef\n",
    "        \n",
    "        list(single_tau = list(\n",
    "            h2g = trait_data$h2g,\n",
    "            sd_annot = sd_annots,\n",
    "            sc_matrix = matrix(as.vector(trait_data$delete_values[[1]] * coef), ncol=1)\n",
    "        ))        \n",
    "    }\n",
    "    }\n",
    "\n",
    "    # Process enrichment\n",
    "    process_enrichment <- function(trait_data) {\n",
    "    Mref <- 5961159\n",
    "    enrichment_p <- as.numeric(trait_data$results[1, \"Enrichment_p\"])\n",
    "    enrich_ratio <- ((trait_data$results[1, \"Prop._h2\"] / trait_data$results[1, \"Prop._SNPs\"]) - \n",
    "                        (1 - trait_data$results[1, \"Prop._h2\"]) / (1 - trait_data$results[1, \"Prop._SNPs\"]))\n",
    "    enrich_term <- trait_data$h2g / Mref * enrich_ratio\n",
    "    \n",
    "    list(\n",
    "        enrichment_summary = data.table(\n",
    "            Enrichment = trait_data$results[1, \"Enrichment\"],\n",
    "            Enrichment_std_error = trait_data$results[1, \"Enrichment_std_error\"],\n",
    "            Prop._h2 = trait_data$results[1, \"Prop._h2\"],\n",
    "            Prop._SNPs = trait_data$results[1, \"Prop._SNPs\"],\n",
    "            Enrichment_p = enrichment_p\n",
    "        ),\n",
    "        meta_enrstat = list(\n",
    "            enrich_stat = as.numeric(enrich_term),\n",
    "            enrich_z = as.numeric(qnorm(enrichment_p / 2)),\n",
    "            enrich_sd = as.numeric(enrich_term / qnorm(enrichment_p / 2))\n",
    "        ),\n",
    "        meta_enr = list(\n",
    "            Enrichment = as.numeric(trait_data$results[1, \"Enrichment\"]),\n",
    "            Enrichment_std_error = as.numeric(trait_data$results[1, \"Enrichment_std_error\"]) \n",
    "        )\n",
    "    )\n",
    "    }\n",
    "\n",
    "    # Main analysis\n",
    "    traits <- scan(\"${all_traits_file}\", what = \"character\")\n",
    "    results <- list()\n",
    "    problematic_traits <- c()\n",
    "\n",
    "    if (is_joint) {\n",
    "    # Get baseline info and indices\n",
    "    base <- fread(paste0(\"${baseline_ld_dir}\", \"/baselineLD.22.annot.gz\"))\n",
    "    base_index <- ncol(base) - 4\n",
    "    \n",
    "    tab2 <- fread(paste0(\"${cwd}\", \"/\", traits[1], \".results\"))\n",
    "    indices <- 1:(nrow(tab2) - base_index)\n",
    "    \n",
    "    sd_annots <- calculate_sd_annot(\"${target_anno_dir}\", indices)\n",
    "    \n",
    "    for (trait in traits) {\n",
    "        if (!is.null(trait_data <- process_trait(trait, \"${cwd}\"))) {\n",
    "            results[[trait]] <- process_tau_analysis(trait_data, sd_annots, base_index, TRUE)\n",
    "        } else {\n",
    "            problematic_traits <- c(problematic_traits, trait)\n",
    "        }\n",
    "    }\n",
    "    } else {\n",
    "    sd_annot <- calculate_sd_annot(\"${target_anno_dir}\")\n",
    "    \n",
    "    for (trait in traits) {\n",
    "        if (!is.null(trait_data <- process_trait(trait, \"${cwd}\"))) {\n",
    "            results[[trait]] <- c(\n",
    "                process_tau_analysis(trait_data, sd_annot, NULL, FALSE),\n",
    "                list(enrichment = process_enrichment(trait_data))\n",
    "            )\n",
    "        } else {\n",
    "            problematic_traits <- c(problematic_traits, trait)\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "\n",
    "    if (length(problematic_traits) > 0) {\n",
    "    warning(\"Problematic traits:\", paste(problematic_traits, collapse=\", \"))\n",
    "    }\n",
    "\n",
    "    saveRDS(results, \"${_output[0]}\", compress = \"xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[processed_stats_2]\n",
    "parameter: trait_group_paths = []     \n",
    "parameter: trait_group_names = []\n",
    "input: group_by = \"all\" \n",
    "output: f'{cwd:a}/{step_name}/{\"joint_tau\" if joint_tau else \"single_tau\"}.{annotation_name}.meta_processed_stats.rds'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "\n",
    "R: expand = '${ }', stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout'\n",
    "  library(data.table)\n",
    "  library(rmeta)\n",
    "\n",
    "  processed_path = ${_input:ar}\n",
    "  results_data <- readRDS(processed_path)\n",
    "  is_joint <- ${str(joint_tau).upper()}\n",
    "  \n",
    "  process_paths <- function(input_str) {\n",
    "      cleaned <- gsub(\"^\\\\[|\\\\]$\", \"\", input_str)\n",
    "      cleaned <- gsub(\"'\", \"\", cleaned)\n",
    "      paths <- strsplit(cleaned, \",\\\\s*|\\\\s+\")[[1]]\n",
    "      paths <- trimws(paths)\n",
    "      return(paths)\n",
    "  }\n",
    "\n",
    "  group_paths <- process_paths(\"${trait_group_paths}\")\n",
    "  group_names <- process_paths(\"${trait_group_names}\")\n",
    "  problematic_traits <- list()\n",
    "  joint_names <- process_paths(\"${joint_annotation_names}\")\n",
    "  if (is_joint) {\n",
    "      joint_tau_results <- list()\n",
    "      \n",
    "      for (i in 1:length(group_paths)) {\n",
    "          traits <- readLines(group_paths[i])\n",
    "          group_stats <- list()\n",
    "          \n",
    "          for (trait in traits) {\n",
    "              if(trait %in% names(results_data) && !is.null(results_data[[trait]]$joint_stats)) {\n",
    "                  group_stats[[trait]] <- results_data[[trait]]$joint_stats\n",
    "              } else {\n",
    "                  warning(paste(\"Invalid data for trait:\", trait))\n",
    "                  problematic_traits[[trait]] <- \"Invalid data\"\n",
    "              }\n",
    "          }\n",
    "          \n",
    "          if(length(group_stats) > 0) {\n",
    "              n_annotations <- ncol(group_stats[[1]]$sc_matrices)\n",
    "              meta_result <- matrix(0, nrow = n_annotations, ncol = 3)\n",
    "              colnames(meta_result) <- c(\"Mean\", \"SD\", \"P\")\n",
    "              if(length(joint_names) > 0) {\n",
    "                  rownames(meta_result) <- joint_names  # Add row names\n",
    "              }              \n",
    "              for(j in 1:n_annotations) {\n",
    "                  df <- matrix(0, nrow = length(group_stats), ncol = 2)\n",
    "                  for(k in 1:length(group_stats)) {\n",
    "                      sc_values <- group_stats[[k]]$sc_matrices[,j]\n",
    "                      df[k,] <- c(mean(sc_values), sqrt(199^2/200 * var(sc_values)))\n",
    "                  }\n",
    "                  \n",
    "                  meta <- meta.summaries(df[,1], df[,2], method = \"random\")\n",
    "                  meta_result[j,] <- c(\n",
    "                      meta$summary, \n",
    "                      meta$se.summary,\n",
    "                      2 * pnorm(-abs(meta$summary/meta$se.summary))\n",
    "                  )\n",
    "              }\n",
    "              joint_tau_results[[group_names[i]]] <- meta_result\n",
    "          }\n",
    "      }\n",
    "      saveRDS(joint_tau_results, '${_output[0]}')\n",
    "      \n",
    "  } else {\n",
    "      tau_results <- list()\n",
    "      enrichment_results <- list()\n",
    "      \n",
    "      for (i in 1:length(group_paths)) {\n",
    "          traits <- readLines(group_paths[i])\n",
    "          \n",
    "          group_stats <- Filter(function(x) {\n",
    "              !is.null(x) && !is.null(x$single_tau$sc_matrix)\n",
    "          }, results_data[traits])\n",
    "          \n",
    "          if (length(group_stats) > 0) {\n",
    "              sc_data <- lapply(group_stats, function(x) {\n",
    "                  mean_sc <- mean(x$single_tau$sc_matrix[, 1])\n",
    "                  se_sc <- sqrt(199^2 / 200 * var(x$single_tau$sc_matrix[, 1]))\n",
    "                  return(c(mean_sc, se_sc))\n",
    "              })\n",
    "              sc_matrix <- do.call(rbind, sc_data)\n",
    "              \n",
    "              meta_tau <- meta.summaries(sc_matrix[, 1], sc_matrix[, 2], method = \"random\")\n",
    "              tau_results[[group_names[i]]] <- c(meta_tau$summary, \n",
    "                                              meta_tau$se.summary,\n",
    "                                              2 * pnorm(-abs(meta_tau$summary / meta_tau$se.summary)))\n",
    "              \n",
    "              enr_data <- lapply(group_stats, function(x) {\n",
    "                  return(c(x$enrichment$meta_enr$Enrichment,\n",
    "                          x$enrichment$meta_enr$Enrichment_std_error))\n",
    "              })\n",
    "              enr_matrix <- do.call(rbind, enr_data)\n",
    "              enrstat_data <- lapply(group_stats, function(x) {\n",
    "                  return(c(x$enrichment$meta_enrstat$enrich_stat, x$enrichment$meta_enrstat$enrich_sd))})\n",
    "              enrstat_matrix <- do.call(rbind, enrstat_data)\n",
    "\n",
    "              meta_enr <- meta.summaries(enr_matrix[, 1], enr_matrix[, 2], method = \"random\")\n",
    "              meta_enrstat <- meta.summaries(enrstat_matrix[, 1], enrstat_matrix[, 2], method = \"random\")   \n",
    "              enrichment_results[[group_names[i]]] <- c(meta_enr$summary,\n",
    "                                                      meta_enr$se.summary,\n",
    "                                                       2 * pnorm(-abs(meta_enrstat$summary / meta_enrstat$se.summary)))\n",
    "          } else {\n",
    "              warning(paste(\"No valid data for group:\", group_names[i]))\n",
    "          }\n",
    "      }\n",
    "\n",
    "\n",
    "                \n",
    "      tau_df <- do.call(rbind, tau_results)\n",
    "      enrichment_df <- do.call(rbind, enrichment_results)\n",
    "      \n",
    "      colnames(tau_df) <- c(\"Mean\", \"SD\", \"P\")\n",
    "      colnames(enrichment_df) <- c(\"Mean\", \"SD\", \"P\")\n",
    "      rownames(tau_df) <- group_names\n",
    "      rownames(enrichment_df) <- group_names\n",
    "      \n",
    "      results <- list(tau = tau_df, enrichment = enrichment_df)\n",
    "      saveRDS(results, '${_output[0]}')\n",
    "  }\n",
    "\n",
    "  if (length(problematic_traits) > 0) {\n",
    "      warning(\"The following traits had issues:\")\n",
    "      print(problematic_traits)\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
