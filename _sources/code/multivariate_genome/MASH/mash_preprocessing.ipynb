{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deadly-accreditation",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Extract genome-wide data for multivariate analysis\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook prepares input data for Utimate Decomposition to generate mixture prior (for mvSuSiE) or to use for MASH analysis. It outputs 3 sets of data: $Z_s$, $Z_n$ and $Z_r$ (strong, null and random)\n",
    "\n",
    "* $Z_s$: **this is now extracted from genome-wide cis analysis fine-mapping results**. We extract the top loci data frame of each condition, where the CS threshold is set to be 0.7. Then we merge the z-scores of them into one data frame.\n",
    "* $Z_n$: (null $Z$-scores): we first extract up to $M$ candidate SNPs from each region which satisify $|z| \\le 2$, then overlap it with the list of independent SNPs to keep only independent variants, then finally take the union of the extracted.\n",
    "* $Z_r$: we randomly extract variants based on input independent list of variants.\n",
    "\n",
    "**FIXME: We need to apply the independent list of variants Anqi developed and use it here to filter and get $Z_n$ and $Z_r$. This logic shoud be added to `processing_1`. Also, it might be a good idea we take some of these utility functions into pecotmr package for better maintenance. For example `processing_1` the function to load regional summary stats from tensorQTL into a matrix should be packed into pecotmr; plus this one function `handle_nan_etc`. processing_2 can stay as is; the `susie_signal` step can also go into `pecotmr` as a way for users to summarize signals from SuSiE for other purposes**\n",
    "\n",
    "## Input\n",
    "1. **Marginal summary statistics files**: Bgzipped summary statistics for chromosomes 1-22, generated by tensorQTL cis-analysis and indexed by `tabix`.\n",
    "2. **Fine-mapping results file index**: Path to lists of fine-mapped RDS files from finemapping output.\n",
    "2. **Genome region partition** (optional): Defines genomic regions for each gene as enhanced cis regions where we should extract $Z_n$ and $Z_r$ from. This list is used for fine-mapping, so if the complete list of fine-mapping RDS (rather than a handful of it) is already avaiable (#2 above) then there is no need to provide this file. Otherwise, it's going to be limited to only certaion regions, which is also good for testing purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-cornell",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "A list of 10 elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-assignment",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "```\n",
    "List of 10\n",
    " $ random.z: num [1:36, 1:2] -0.785 -0.785 -0.785 -0.785 -0.785 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:97960:A:G\" \"1:138565:G:A\" \"1:15112:C:T\" \"1:189947:G:A\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ null.z  : num [1:36, 1:2] -0.785 -0.785 -0.785 -0.785 -0.785 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:93692:C:T\" \"1:273645:A:G\" \"1:10442:CCTA:.\" \"1:198942:A:C\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ random.b: num [1:36, 1:2] -0.123 -0.123 -0.123 -0.123 -0.123 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:97960:A:G\" \"1:138565:G:A\" \"1:15112:C:T\" \"1:189947:G:A\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ null.b  : num [1:36, 1:2] -0.123 -0.123 -0.123 -0.123 -0.123 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:93692:C:T\" \"1:273645:A:G\" \"1:10442:CCTA:.\" \"1:198942:A:C\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ null.s  : num [1:36, 1:2] 0.157 0.157 0.157 0.157 0.157 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:93692:C:T\" \"1:273645:A:G\" \"1:10442:CCTA:.\" \"1:198942:A:C\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ random.s: num [1:36, 1:2] 0.157 0.157 0.157 0.157 0.157 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:97960:A:G\" \"1:138565:G:A\" \"1:15112:C:T\" \"1:189947:G:A\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ strong.b:Classes ‘data.table’ and 'data.frame':\t1 obs. of  2 variables:\n",
    "  ..$ A: num -0.217\n",
    "  ..$ B: num -0.217\n",
    "  ..- attr(*, \".internal.selfref\")=<externalptr> \n",
    " $ strong.s:Classes ‘data.table’ and 'data.frame':\t1 obs. of  2 variables:\n",
    "  ..$ A: num 0.0481\n",
    "  ..$ B: num 0.0481\n",
    "  ..- attr(*, \".internal.selfref\")=<externalptr> \n",
    " $ strong.z:Classes ‘data.table’ and 'data.frame':\t1 obs. of  2 variables:\n",
    "  ..$ A: num -4.5\n",
    "  ..$ B: num -4.5\n",
    "  ..- attr(*, \".internal.selfref\")=<externalptr> \n",
    " $ XtX     : num [1:2, 1:2] 20.3 20.3 20.3 20.3\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-georgia",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-painting",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# generate random and null only\n",
    "sos run pipeline/mash_preprocessing.ipynb processing \\\n",
    "    --name protocol_example_protein \\\n",
    "    --sum_files test_pQTL_asso_list \\\n",
    "               test_pQTL_asso_list \\\n",
    "    --region_file test.region \\\n",
    "    --pheno_names A B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-reform",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# generate strong only\n",
    "sos run pipeline/mash_preprocessing.ipynb susie_signal \\\n",
    "    --name protocol_example_protein \\\n",
    "    --susie_list protocol_example_protein.susie_output.txt \\\n",
    "    --pheno_names A B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-webster",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# generate mashr input directly\n",
    "sos run pipeline/mash_preprocessing.ipynb mash_input \\\n",
    "    --name protocol_example_protein \\\n",
    "    --sum_files test_pQTL_asso_list \\\n",
    "               test_pQTL_asso_list \\\n",
    "    --region_file test.region \\\n",
    "    --susie_list protocol_example_protein.susie_output.txt \\\n",
    "    --pheno_names A B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparable-prisoner",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import glob\n",
    "parameter: name = str\n",
    "parameter: pheno_names = paths\n",
    "# Path to work directory where output locates\n",
    "parameter: cwd = path(\"./output\")\n",
    "# Containers that contains the necessary packages\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "\n",
    "# This is in principle required; but in practice it can be optional if we are not exactly stringent about getting independent SNPs\n",
    "parameter: independent_variant_list = path\n",
    "ran_null = file_target(f\"{cwd}/{name}.random.null.rds\")\n",
    "strong = file_target(f\"{cwd}/{name}.strong.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-saying",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Get the random and null effects per analysis unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-gather",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[extract_tensorqtl_1]\n",
    "parameter: sum_files = paths\n",
    "parameter: region_file = path\n",
    "import re\n",
    "import pandas as pd\n",
    "def find_matching_files_for_region(chr_id):\n",
    "    chr_number = chr_id[3:]  # subset 1 from chr1\n",
    "    pattern_str = r\"\\.{chr_number}\\.\"\n",
    "    pattern = re.compile(pattern_str.format(chr_number=chr_number))\n",
    "    paths = []\n",
    "    for sum_file in sum_files:\n",
    "        with open(sum_file, 'r') as af:\n",
    "            for aline in af:\n",
    "                if pattern.search(aline):\n",
    "                    paths.append(aline.strip())\n",
    "    return \",\".join(paths)\n",
    "\n",
    "updated_regions = []\n",
    "with open(region_file, 'r') as regions:\n",
    "    header = regions.readline().strip()\n",
    "    updated_regions.append(header + \"\\tpath\\tregion\")\n",
    "    for line in regions:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        chr_id, start, end, gene_id = parts\n",
    "        paths = find_matching_files_for_region(chr_id)\n",
    "        updated_regions.append(f\"{chr_id}\\t{start}\\t{end}\\t{gene_id}\\t{paths}\\t{chr_id}:{start}-{end}\")\n",
    "\n",
    "meta_df = pd.DataFrame([line.split(\"\\t\") for line in updated_regions[1:]], columns=updated_regions[0].split(\"\\t\"))\n",
    "meta = meta_df[['gene_id', 'path', 'region']].to_dict(orient='records')\n",
    "\n",
    "input: for_each='meta'\n",
    "output: f'{cwd:a}/{name}_cache/{name}.{_meta[\"gene_id\"]}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container, entrypoint=entrypoint\n",
    "    region <- \"${_meta['region']}\"\n",
    "    # FIXME I am sure there is a more elegant way to put together the path, via SoS\n",
    "    phenotype_path <- unlist(strsplit(\"${_meta['path']}\", \",\"))\n",
    "    dat <- tryCatch(\n",
    "      {\n",
    "        # Try to run the function\n",
    "         pecotmr::load_multitrait_tensorqtl_sumstat(phenotype_path = phenotype_path, region = region, \n",
    "          trait_names = c(${pheno_names:r,}), filter_file = NULL, remove_any_missing = TRUE, max_rows_selected = 300)\n",
    "      },\n",
    "      error = function(e) {\n",
    "        warning(\"Attempt remove chr in region ID to load the data.\")\n",
    "        # If an error occurs, modify the region and try again\n",
    "        pecotmr::load_multitrait_tensorqtl_sumstat(phenotype_path = phenotype_path, region =  gsub(\"chr\", \"\", region), \n",
    "          trait_names = c(${pheno_names:r,}), filter_file = NULL, remove_any_missing = TRUE, max_rows_selected = 300)\n",
    "      }\n",
    "    )\n",
    "    saveRDS(dat, ${_output:r}, compress=\"xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-yellow",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# extract data for MASH from summary stats\n",
    "[extract_tensorqtl_2]\n",
    "parameter: seed = 999\n",
    "parameter: n_random = 50\n",
    "parameter: n_null = 50\n",
    "parameter: z_only = False\n",
    "# Columns: \"#chr\", sumstat(merged.vcf.gz)\n",
    "parameter: table_name = \"\"\n",
    "parameter: bhat = \"bhat\"\n",
    "parameter: sbhat = \"sbhat\"\n",
    "parameter: expected_ncondition = 0\n",
    "parameter: per_chunk =100\n",
    "\n",
    "##  conditions can be excluded if needs arise. If nothing to exclude keep the default 0\n",
    "parameter: exclude_condition = []\n",
    "parameter: datadir = \"\"\n",
    "parameter: na_remove = \"TRUE\"\n",
    "\n",
    "input:  group_by = per_chunk\n",
    "output: f\"{cwd}/{name}_cache/{name}_batch{_index+1}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\",stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    library(dplyr)\n",
    "    library(stringr)\n",
    "    set.seed(${seed})\n",
    "    #library(huiiy)\n",
    "\n",
    "    remove_rownames = function(x) {\n",
    "        for (name in names(x)) rownames(x[[name]]) = NULL\n",
    "        return(x)\n",
    "    }\n",
    "    extract_one_data = function(dat, n_random, n_null, filename) {\n",
    "        if (is.null(dat)) return(NULL)\n",
    "        abs_z = abs(dat$${bhat}/dat$${sbhat})\n",
    "        # random samples can include the real signals \n",
    "        sample_idx = 1:nrow(abs_z)\n",
    "        random_idx = sample(sample_idx, min(n_random, length(sample_idx)), replace = F)\n",
    "        random = list(bhat = dat$${bhat}[random_idx,,drop=F], sbhat = dat$${sbhat}[random_idx,,drop=F])\n",
    "        # null samples defined as |z| < 2\n",
    "        null.id = which(apply(abs_z, 1, max) < 2)\n",
    "        if (length(null.id) == 0) {\n",
    "          warning(paste(\"Null data is empty for input file\", filename))\n",
    "          null = list()\n",
    "        } else {\n",
    "          null_idx = sample(null.id, min(n_null, length(null.id)), replace = F)\n",
    "          null = list(bhat = dat$${bhat}[null_idx,,drop=F], sbhat = dat$${sbhat}[null_idx,,drop=F])\n",
    "        }\n",
    "        #dat = (list(random = remove_rownames(random), null = remove_rownames(null)))\n",
    "        dat = (list(random = random, null = null))\n",
    "        return(dat)\n",
    "    }\n",
    "    reformat_data = function(dat, z_only = FALSE) {\n",
    "        # make output consistent in format with \n",
    "        # https://github.com/stephenslab/gtexresults/blob/master/workflows/mashr_flashr_workflow.ipynb      \n",
    "        res = list(random.z = dat$random$bhat/dat$random$sbhat, \n",
    "                  null.z = dat$null$bhat/dat$null$sbhat)\n",
    "        if (!z_only) {\n",
    "          res = c(res, list(random.b = dat$random$bhat,\n",
    "           null.b = dat$null$bhat,\n",
    "           null.s = dat$null$sbhat,\n",
    "           random.s = dat$random$sbhat))\n",
    "      }\n",
    "      return(res)\n",
    "    }\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(is.null(res$random.b)|is.null(res$null.b))) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(one_data)) {\n",
    "          return(res)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "            if (is.null(one_data[[d]])) {\n",
    "              next\n",
    "            } else {\n",
    "                res[[d]] = as.matrix(rbind(res[[d]],as.data.frame(one_data[[d]])))\n",
    "            }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    res = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      # If cannot read the input for some reason then we just skip it, assuming we have other enough data-sets to use.\n",
    "      dat = tryCatch(readRDS(f), error = function(e) return(NULL))${(\"$\"+table_name) if table_name != \"\" else \"\"}\n",
    "      if (is.null(dat)) {\n",
    "          message(paste(\"Skip loading file\", f, \"due to load failure.\"))\n",
    "          next\n",
    "      }\n",
    "      if (${expected_ncondition} > 0 && (ncol(dat$${bhat}) != ${expected_ncondition} || ncol(dat$${sbhat}) != ${expected_ncondition})) {\n",
    "          message(paste(\"Skip loading file\", f, \"because it has\", ncol(dat$${bhat}), \"columns different from required\", ${expected_ncondition}))\n",
    "          next\n",
    "      }\n",
    "      if(length(c(${\",\".join([repr(x) for x in exclude_condition])})) > 0 ){\n",
    "          message(paste(\"Excluding condition ${exclude_condition} from the analysis\"))\n",
    "          dat$bhat = dat$bhat[,-c(${\",\".join(exclude_condition)})]\n",
    "          dat$sbhat = dat$sbhat[,-c(${\",\".join(exclude_condition)})]\n",
    "          dat$Z = dat$Z[,-c(${\",\".join(exclude_condition)})]\n",
    "      }\n",
    "\n",
    "      dat<-tryCatch(extract_one_data(dat, ${n_random}, ${n_null}, f), error = function(e) return(NULL))\n",
    "      res<-tryCatch(merge_data(res, reformat_data(dat , ${\"TRUE\" if z_only else \"FALSE\"})), error = function(e) message(\"Skipping gene due to lack of SNPs\"))     \n",
    "  }\n",
    "  saveRDS(res, ${_output:r}, compress=\"xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-advertising",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[extract_tensorqtl_3]\n",
    "input: group_by = \"all\"\n",
    "output: random_null = f\"{cwd}/{name}.random_null.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "            res[[d]] = rbind(res[[d]], one_data[[d]])\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    dat = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      dat = merge_data(dat, readRDS(f))\n",
    "    }\n",
    "    saveRDS(dat, ${_output:r}, compress=\"xz\")\n",
    " \n",
    "bash: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    rm -rf ${cwd}/${name}_cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-client",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[extract_susie_top_loci]\n",
    "parameter: susie_list = path\n",
    "input: susie_list\n",
    "output: strong = f\"{cwd}/{name}.strong.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container,stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', volumes = [f'{cwd:ad}:{cwd:ad}'], entrypoint=entrypoint\n",
    "    out <- pecotmr::load_susie_top_loci(read.table(\"${_input}\")$V1, c(${pheno_names:r,}))\n",
    "    saveRDS(out, ${_output:r}, compress=\"xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-retention",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash_input]\n",
    "input: random_null, strong\n",
    "output: f\"{cwd}/{name}.mashr_input.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container,stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', volumes = [f'{cwd:ad}:{cwd:ad}'], entrypoint=entrypoint\n",
    "    out <- readRDS(${_input[0]:r})\n",
    "    strong <- readRDS(${_input[1]:r})\n",
    "    out$strong.b <- strong$bhat\n",
    "    out$strong.s <- strong$sbhat\n",
    "    X <- out$strong.z <- strong$z\n",
    "    X[is.na(X)] = 0\n",
    "    out$ZtZ = t(as.matrix(X)) %*% as.matrix(X) / nrow(X)\n",
    "    saveRDS(out, ${_output:r}, compress=\"xz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
