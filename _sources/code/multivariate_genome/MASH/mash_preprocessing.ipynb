{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powerful-antigua",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Extract genome-wide data for multivariate analysis\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook prepares input data for Utimate Decomposition to generate mixture prior (for mvSuSiE) or to use for MASH analysis. It outputs 3 sets of data: $Z_s$, $Z_n$ and $Z_r$ (strong, null and random)\n",
    "\n",
    "* $Z_s$: **this is now extracted from genome-wide cis analysis fine-mapping results**. We extract the top loci data frame of each condition, where the CS threshold is set to be 0.7. Then we merge the z-scores of them into one data frame.\n",
    "* $Z_n$: (null $Z$-scores): we first extract up to $M$ candidate SNPs from each region which satisify $|z| \\le 2$, then overlap it with the list of independent SNPs to keep only independent variants, then finally take the union of the extracted.\n",
    "* $Z_r$: we randomly extract variants based on input independent list of variants.\n",
    "\n",
    "**FIXME: We need to apply the independent list of variants Anqi developed and use it here to filter and get $Z_n$ and $Z_r$. This logic shoud be added to `processing_1`. Also, it might be a good idea we take some of these utility functions into pecotmr package for better maintenance. For example `processing_1` the function to load regional summary stats from tensorQTL into a matrix should be packed into pecotmr; plus this one function `handle_nan_etc`. processing_2 can stay as is; the `susie_signal` step can also go into `pecotmr` as a way for users to summarize signals from SuSiE for other purposes**\n",
    "\n",
    "## Input\n",
    "1. **Marginal summary statistics files**: Bgzipped summary statistics for chromosomes 1-22, generated by tensorQTL cis-analysis and indexed by `tabix`.\n",
    "2. **Fine-mapping results file index**: Path to lists of fine-mapped RDS files from finemapping output.\n",
    "2. **Genome region partition** (optional): Defines genomic regions for each gene as enhanced cis regions where we should extract $Z_n$ and $Z_r$ from. This list is used for fine-mapping, so if the complete list of fine-mapping RDS (rather than a handful of it) is already avaiable (#2 above) then there is no need to provide this file. Otherwise, it's going to be limited to only certaion regions, which is also good for testing purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-suggestion",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "A list of 10 elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-basketball",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "```\n",
    "List of 10\n",
    " $ random.z: num [1:36, 1:2] -0.785 -0.785 -0.785 -0.785 -0.785 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:97960:A:G\" \"1:138565:G:A\" \"1:15112:C:T\" \"1:189947:G:A\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ null.z  : num [1:36, 1:2] -0.785 -0.785 -0.785 -0.785 -0.785 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:93692:C:T\" \"1:273645:A:G\" \"1:10442:CCTA:.\" \"1:198942:A:C\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ random.b: num [1:36, 1:2] -0.123 -0.123 -0.123 -0.123 -0.123 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:97960:A:G\" \"1:138565:G:A\" \"1:15112:C:T\" \"1:189947:G:A\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ null.b  : num [1:36, 1:2] -0.123 -0.123 -0.123 -0.123 -0.123 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:93692:C:T\" \"1:273645:A:G\" \"1:10442:CCTA:.\" \"1:198942:A:C\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ null.s  : num [1:36, 1:2] 0.157 0.157 0.157 0.157 0.157 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:93692:C:T\" \"1:273645:A:G\" \"1:10442:CCTA:.\" \"1:198942:A:C\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ random.s: num [1:36, 1:2] 0.157 0.157 0.157 0.157 0.157 ...\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:36] \"1:97960:A:G\" \"1:138565:G:A\" \"1:15112:C:T\" \"1:189947:G:A\" ...\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    " $ strong.b:Classes ‘data.table’ and 'data.frame':\t1 obs. of  2 variables:\n",
    "  ..$ A: num -0.217\n",
    "  ..$ B: num -0.217\n",
    "  ..- attr(*, \".internal.selfref\")=<externalptr> \n",
    " $ strong.s:Classes ‘data.table’ and 'data.frame':\t1 obs. of  2 variables:\n",
    "  ..$ A: num 0.0481\n",
    "  ..$ B: num 0.0481\n",
    "  ..- attr(*, \".internal.selfref\")=<externalptr> \n",
    " $ strong.z:Classes ‘data.table’ and 'data.frame':\t1 obs. of  2 variables:\n",
    "  ..$ A: num -4.5\n",
    "  ..$ B: num -4.5\n",
    "  ..- attr(*, \".internal.selfref\")=<externalptr> \n",
    " $ XtX     : num [1:2, 1:2] 20.3 20.3 20.3 20.3\n",
    "  ..- attr(*, \"dimnames\")=List of 2\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    "  .. ..$ : chr [1:2] \"A\" \"B\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-whale",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-knitting",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# generate random and null only\n",
    "sos run pipeline/mash_preprocessing.ipynb processing \\\n",
    "    --name protocol_example_protein \\\n",
    "    --sum_files test_pQTL_asso_list \\\n",
    "               test_pQTL_asso_list \\\n",
    "    --region_file test.region \\\n",
    "    --pheno_names A,B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-platinum",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# generate strong only\n",
    "sos run pipeline/mash_preprocessing.ipynb susie_signal \\\n",
    "    --name protocol_example_protein \\\n",
    "    --susie_list protocol_example_protein.susie_output.txt \\\n",
    "    --pheno_names A,B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-volleyball",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# generate mashr input directly\n",
    "sos run pipeline/mash_preprocessing.ipynb mash_input \\\n",
    "    --name protocol_example_protein \\\n",
    "    --sum_files test_pQTL_asso_list \\\n",
    "               test_pQTL_asso_list \\\n",
    "    --region_file test.region \\\n",
    "    --susie_list protocol_example_protein.susie_output.txt \\\n",
    "    --pheno_names A,B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stock-blast",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import glob\n",
    "parameter: name = str\n",
    "parameter: pheno_names = str\n",
    "# Path to work directory where output locates\n",
    "parameter: cwd = path(\"./output\")\n",
    "# Containers that contains the necessary packages\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "parameter: per_chunk =100\n",
    "# Columns: \"#chr\", sumstat(merged.vcf.gz)\n",
    "parameter: table_name = \"\"\n",
    "parameter: bhat = \"bhat\"\n",
    "parameter: sbhat = \"sbhat\"\n",
    "parameter: expected_ncondition = 0\n",
    "##  conditions can be excluded if needs arise. If nothing to exclude keep the default 0\n",
    "parameter: exclude_condition = []\n",
    "parameter: datadir = \"\"\n",
    "parameter: seed = 999\n",
    "parameter: n_random = 50\n",
    "parameter: n_null = 50\n",
    "parameter: z_only = False\n",
    "parameter: na_remove = \"TRUE\"\n",
    "# This is in principle required; but in practice it can be optional if we are not exactly stringent about getting independent SNPs\n",
    "parameter: independent_variant_list = path\n",
    "ran_null = file_target(f\"{cwd}/{name}.random.null.rds\")\n",
    "strong = file_target(f\"{cwd}/{name}.strong.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-characteristic",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Get the random and null effects per analysis unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-housing",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[processing_1]\n",
    "parameter: sum_files = paths\n",
    "parameter: region_file = path\n",
    "import re\n",
    "import pandas as pd\n",
    "def find_matching_files_for_region(chr_id):\n",
    "    chr_number = chr_id[3:]  # subset 1 from chr1\n",
    "    pattern_str = r\"\\.{chr_number}\\.\"\n",
    "    pattern = re.compile(pattern_str.format(chr_number=chr_number))\n",
    "    paths = []\n",
    "    for sum_file in sum_files:\n",
    "        with open(sum_file, 'r') as af:\n",
    "            for aline in af:\n",
    "                if pattern.search(aline):\n",
    "                    paths.append(aline.strip())\n",
    "    return \",\".join(paths)\n",
    "\n",
    "updated_regions = []\n",
    "with open(region_file, 'r') as regions:\n",
    "    header = regions.readline().strip()\n",
    "    updated_regions.append(header + \"\\tpath\\tregion\")\n",
    "    for line in regions:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        chr_id, start, end, gene_id = parts\n",
    "        paths = find_matching_files_for_region(chr_id)\n",
    "        updated_regions.append(f\"{chr_id}\\t{start}\\t{end}\\t{gene_id}\\t{paths}\\t{chr_id}:{start}-{end}\")\n",
    "\n",
    "meta_df = pd.DataFrame([line.split(\"\\t\") for line in updated_regions[1:]], columns=updated_regions[0].split(\"\\t\"))\n",
    "meta = meta_df[['gene_id', 'path', 'region']].to_dict(orient='records')\n",
    "\n",
    "input: for_each='meta'\n",
    "output: f'{cwd:a}/{name}_cache/{name}.{_meta[\"gene_id\"]}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    # Extract and preprocess data from phenotype_path\n",
    "    extract_data <- function(path, region) {\n",
    "        tabix_region(path, region) %>%\n",
    "            mutate(variant = paste(`#CHROM`, POS, REF, ALT, sep = \":\")) %>%\n",
    "            select(-c(3, 6:9)) %>%\n",
    "            distinct(variant, .keep_all = TRUE) %>%\n",
    "            as.matrix\n",
    "    }\n",
    "    # Extract bhat and sbhat\n",
    "    extract_component <- function(df, component_index) {\n",
    "        df %>%\n",
    "            select(6:ncol(df)) %>%\n",
    "            mutate(across(everything(), ~as.numeric(strsplit(as.character(.), \":\")[[1]][component_index]))) %>%\n",
    "            as.matrix\n",
    "    }\n",
    "\n",
    "    load_combined_matrix_data <- function(phenotype_path, region) {\n",
    "        library(dplyr)   \n",
    "        Y <- lapply(phenotype_path, extract_data, region)\n",
    "\n",
    "        # Combine matrices\n",
    "        combined_matrix <- Reduce(function(x, y) merge(x, y, by = c(\"variant\", \"#CHROM\", \"POS\", \"REF\", \"ALT\")), Y) %>%\n",
    "            distinct(variant, .keep_all = TRUE)\n",
    "\n",
    "        dat <- list(\n",
    "            bhat = extract_component(combined_matrix, 1),\n",
    "            sbhat = extract_component(combined_matrix, 2)\n",
    "        )\n",
    "\n",
    "        rownames(dat$bhat) <- rownames(dat$sbhat) <- combined_matrix$variant\n",
    "        colnames(dat$bhat) <- colnames(dat$sbhat) <- unlist(strsplit(\"${pheno_names}\", \",\"))\n",
    "        return(dat)\n",
    "    }\n",
    "  \n",
    "    tabix_region <- function(file, region){\n",
    "        data.table::fread(cmd = paste0(\"tabix -h \", file, \" \", region))%>%as_tibble() \n",
    "    }\n",
    "  \n",
    "    region <- \"${_meta['region']}\"\n",
    "    phenotype_path <- unlist(strsplit(\"${_meta['path']}\", \",\"))\n",
    "\n",
    "    dat <- tryCatch(\n",
    "      {\n",
    "        # Try to run the function\n",
    "         load_combined_matrix_data(phenotype_path = phenotype_path, region = region)\n",
    "      },\n",
    "      error = function(e) {\n",
    "        message(\"gsub chr in region id...\")\n",
    "        # If an error occurs, modify the region and try again\n",
    "         load_combined_matrix_data(phenotype_path = phenotype_path, region =  gsub(\"chr\", \"\", region))\n",
    "      }\n",
    "    )\n",
    "      saveRDS(dat, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-printer",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# extract data for MASH from summary stats\n",
    "[processing_2]\n",
    "input:  group_by = per_chunk\n",
    "output: f\"{cwd}/{name}_cache/{name}_batch{_index+1}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\",stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    library(dplyr)\n",
    "    library(stringr)\n",
    "    set.seed(${seed})\n",
    "    #library(huiiy)\n",
    "    matxMax <- function(mtx) {\n",
    "      return(arrayInd(which.max(mtx), dim(mtx)))\n",
    "    }\n",
    "    remove_rownames = function(x) {\n",
    "        for (name in names(x)) rownames(x[[name]]) = NULL\n",
    "        return(x)\n",
    "    }\n",
    "    handle_nan_etc = function(x) {\n",
    "      x$bhat[which(is.nan(x$bhat))] = 0\n",
    "      x$sbhat[which(is.nan(x$sbhat) | is.infinite(x$sbhat))] = 1E3\n",
    "      return(x)\n",
    "    }\n",
    "    extract_one_data = function(dat, n_random, n_null, infile, na_remove = TRUE) {\n",
    "        if (is.null(dat)) return(NULL)\n",
    "        if(na_remove == TRUE){\n",
    "          na.info = list()\n",
    "          na.info$n_bhat_ori = nrow(dat$${bhat})\n",
    "          dat$bhat = na.omit(dat$${bhat})\n",
    "          na.info$n_bhat = nrow(dat$${bhat})\n",
    "          na.info$n_sbhat_ori = nrow(dat$${sbhat})\n",
    "          dat$sbhat = na.omit(dat$${sbhat})\n",
    "          na.info$n_sbhat_ori = nrow(dat$${sbhat})\n",
    "          msg = paste(c(\"Out of \",na.info$n_bhat_ori,\" SNP, \",na.info$n_bhat,\" was retained for analysis\"), collapse = \"\")\n",
    "          message(msg)\n",
    "          if (na.info$n_bhat == 0){\n",
    "            stop(\"None of the SNP was retained for analysis, skipping genes\") }\n",
    "        }\n",
    "        z = abs(dat$${bhat}/dat$${sbhat})\n",
    "        # random samples can include the real signals \n",
    "        sample_idx = 1:nrow(z)\n",
    "        \n",
    "        random_idx = sample(sample_idx, min(n_random, length(sample_idx)), replace = F)\n",
    "        random = list(bhat = dat$${bhat}[random_idx,,drop=F], sbhat = dat$${sbhat}[random_idx,,drop=F])\n",
    "        # null samples defined as |z| < 2\n",
    "        null.id = which(apply(abs(z), 1, max) < 2)\n",
    "        if (length(null.id) == 0) {\n",
    "          warning(paste(\"Null data is empty for input file\", infile))\n",
    "          null = list()\n",
    "        } else {\n",
    "          null_idx = sample(null.id, min(n_null, length(null.id)), replace = F)\n",
    "          null = list(bhat = dat$${bhat}[null_idx,,drop=F], sbhat = dat$${sbhat}[null_idx,,drop=F])\n",
    "        }\n",
    "        #dat = (list(random = remove_rownames(random), null = remove_rownames(null)))\n",
    "        dat = (list(random = random, null = null))\n",
    "        dat$random = handle_nan_etc(dat$random)\n",
    "        dat$null = handle_nan_etc(dat$null)\n",
    "        return(dat)\n",
    "    }\n",
    "    reformat_data = function(dat, z_only = TRUE) {\n",
    "        # make output consistent in format with \n",
    "        # https://github.com/stephenslab/gtexresults/blob/master/workflows/mashr_flashr_workflow.ipynb      \n",
    "        res = list(random.z = dat$random$bhat/dat$random$sbhat, \n",
    "                  null.z = dat$null$bhat/dat$null$sbhat)\n",
    "        if (!z_only) {\n",
    "          res = c(res, list(random.b = dat$random$bhat,\n",
    "           null.b = dat$null$bhat,\n",
    "           null.s = dat$null$sbhat,\n",
    "           random.s = dat$random$sbhat))\n",
    "      }\n",
    "      return(res)\n",
    "    }\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(is.null(res$random.b)|is.null(res$null.b))) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(one_data)) {\n",
    "          return(res)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "            if (is.null(one_data[[d]])) {\n",
    "              next\n",
    "            } else {\n",
    "                res[[d]] = as.matrix(rbind(res[[d]],as.data.frame(one_data[[d]])))\n",
    "            }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    res = list()\n",
    "    signals.df<-NULL\n",
    " \n",
    "    \n",
    "    for (f in c(${_input:r,})) {\n",
    "      # If cannot read the input for some reason then we just skip it, assuming we have other enough data-sets to use.\n",
    "      dat = tryCatch(readRDS(f), error = function(e) return(NULL))${(\"$\"+table_name) if table_name != \"\" else \"\"}\n",
    "      if (is.null(dat)) {\n",
    "          message(paste(\"Skip loading file\", f, \"due to load failure.\"))\n",
    "          next\n",
    "      }\n",
    "      if (${expected_ncondition} > 0 && (ncol(dat$${bhat}) != ${expected_ncondition} || ncol(dat$${sbhat}) != ${expected_ncondition})) {\n",
    "          message(paste(\"Skip loading file\", f, \"because it has\", ncol(dat$${bhat}), \"columns different from required\", ${expected_ncondition}))\n",
    "          next\n",
    "      }\n",
    "      if(length(c(${\",\".join([repr(x) for x in exclude_condition])})) > 0 ){\n",
    "      message(paste(\"Excluding condition ${exclude_condition} from the analysis\"))\n",
    "      dat$bhat = dat$bhat[,-c(${\",\".join(exclude_condition)})]\n",
    "      dat$sbhat = dat$sbhat[,-c(${\",\".join(exclude_condition)})]\n",
    "      dat$Z = dat$Z[,-c(${\",\".join(exclude_condition)})]\n",
    "      }\n",
    "\n",
    "      dat<-tryCatch(extract_one_data(dat, ${n_random}, ${n_null}, f, ${na_remove}), error = function(e) return(NULL))\n",
    "      res = tryCatch(merge_data(res, reformat_data(dat , ${\"TRUE\" if z_only else \"FALSE\"})), error = function(e) message(\"Skipping gene due to lack of SNPs\"))\n",
    "      \n",
    "    saveRDS(res, ${_output:r})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-browser",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[processing_3]\n",
    "input: group_by = \"all\"\n",
    "output: ran_null = f\"{cwd}/{name}.random.null.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container,stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', volumes = [f'{cwd:ad}:{cwd:ad}']\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "            res[[d]] = rbind(res[[d]], one_data[[d]])\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    dat = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      dat = merge_data(dat, readRDS(f))\n",
    "    }\n",
    "    saveRDS(dat, ${_output:r})\n",
    " \n",
    "bash: expand = \"${ }\", container = container,stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', volumes = [f'{cwd:ad}:{cwd:ad}']\n",
    "    #rm -rf ${cwd}/${name}_cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-permit",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_signal]\n",
    "parameter: susie_list = path\n",
    "input: susie_list\n",
    "output: strong = f\"{cwd}/{name}.strong.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container,stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', volumes = [f'{cwd:ad}:{cwd:ad}']\n",
    "    # Load the required library\n",
    "    library(data.table)\n",
    "\n",
    "    rds_files <- read.table(\"${_input}\")$V1\n",
    "    pheno_names <- unlist(strsplit(\"${pheno_names}\", \",\"))\n",
    "    read_and_extract <- function(file) {\n",
    "      dat <- readRDS(file)\n",
    "\n",
    "      bhats <- lapply(dat, function(x) as.data.table(x$qtl_identified)[, .(variants, bhat)])\n",
    "      sbhats <- lapply(dat, function(x) as.data.table(x$qtl_identified)[, .(variants, sbhat)])\n",
    "\n",
    "      list(\n",
    "        bhat = do.call(cbind, lapply(bhats, `[[`, \"bhat\")),\n",
    "        sbhat = do.call(cbind, lapply(sbhats, `[[`, \"sbhat\")),\n",
    "        variants = bhats[[1]]$variants\n",
    "      )\n",
    "    }\n",
    "\n",
    "    results <- lapply(rds_files, read_and_extract)\n",
    "\n",
    "    out <- list(\n",
    "      bhat = rbindlist(lapply(results, function(x) data.table(variants = x$variants, x$bhat)), fill = TRUE),\n",
    "      sbhat = rbindlist(lapply(results, function(x) data.table(variants = x$variants, x$sbhat)), fill = TRUE)\n",
    "    )\n",
    "\n",
    "    variants <- out$bhat$variants                                            \n",
    "\n",
    "    out$bhat <- out$bhat[, -1, with = FALSE]\n",
    "    out$sbhat <- out$sbhat[, -1, with = FALSE]\n",
    "\n",
    "    # Assuming 'pheno_names' variable is defined elsewhere in your script\n",
    "                                             \n",
    "    colnames(out$bhat) <- colnames(out$sbhat) <- pheno_names\n",
    "\n",
    "    # Calculate 'z'\n",
    "    out$z <- out$bhat/ out$sbhat\n",
    "    rownames(out$bhat) <- rownames(out$sbhat) <- rownames(out$z) <- variants    \n",
    "    saveRDS(out, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-lyric",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mash_input]\n",
    "input: ran_null, strong\n",
    "output: f\"{cwd}/{name}.mashr_input.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container,stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', volumes = [f'{cwd:ad}:{cwd:ad}']\n",
    "    ran_null <- readRDS(${_input[0]:r})\n",
    "    strong <- readRDS(${_input[1]:r})\n",
    "    out <- ran_null\n",
    "    out$strong.b <- strong$bhat\n",
    "    out$strong.s <- strong$sbhat\n",
    "    X <- out$strong.z <- strong$z\n",
    "    X[is.na(X)] = 0\n",
    "    out$ZtZ = t(as.matrix(X)) %*% as.matrix(X) / nrow(X)\n",
    "    saveRDS(out, ${_output:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
