{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "final-concert",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Quantifying expression from RNA-seq data\n",
    "\n",
    "This pipeline aims to call expression levels from RNA-seq data starting from `fastq.gz` sequences. It implements the GTEx pipeline for GTEx / TOPMed project. Please refer to [this page](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md) for detail.\n",
    "\n",
    "**Various reference data needs to be prepared before using this workflow**. [Here we provide a module](../../data_preprocessing/reference_data.html) to download and prepare the reference data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-kingston",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Methods overview\n",
    "**FIXME:Maybe change the image?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worth-double",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> ../../images/rna_quantification.jpeg: None</div>"
      ],
      "text/plain": [
       ">>> ../../images/rna_quantification.jpeg:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown variable ../../images/rna_quantification.jpeg"
     ]
    }
   ],
   "source": [
    "%preview ../../images/rna_quantification.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-portal",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The procedure is decribed also in [GTEx portal](https://gtexportal.org/home/documentationPage#staticTextAnalysisMethods). Here is a recap of some important details.\n",
    "\n",
    "\n",
    "### Gene-level quantifications\n",
    "\n",
    "read counts and TPM values were produced with RNA-SeQC v2.4.2 (DeLuca et al., Bioinformatics, 2012 ), using the following read-level filters:\n",
    "\n",
    "- Reads were uniquely mapped (corresponding to a mapping quality of 255 for START BAMs).\n",
    "- Reads were aligned in proper pairs.\n",
    "- The read alignment distance was <=6 (i.e., alignments must not contain more than six non-reference bases).\n",
    "- Reads were fully contained within exon boundaries. Reads overlapping introns were not counted.\n",
    "\n",
    "These filters were applied using the “-strictMode” flag in RNA-SeQC. The TPM values have not been normalized or corrected for any covariates.\n",
    "\n",
    "### Exon-level quantifications\n",
    "\n",
    "for exon-level read counts, if a read overlapped multiple exons, then a fractional value equal to the portion of the read contained within that exon was allotted. Transcript-level quantifications were calculated using RSEM v1.3.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-blade",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "A meta-data file, white space delimited without header, containing 3 columns: sample ID, fastq file 1, fastq file 2:\n",
    "\n",
    "```\n",
    "sample_1 samp1_r1.fq.gz samp1_r2.fq.gz\n",
    "sample_2 samp2_r1.fq.gz samp2_r2.fq.gz\n",
    "sample_3 samp3_r1.fq.gz samp3_r2.fq.gz\n",
    "```\n",
    "\n",
    "All the fastq files should be available under specified folder (default assumes the same folder as where the meta-data file is)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-browser",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "\n",
    "TPM count for gene level RNA expression via `rnaseqc`, and isoform level RNA expression via `RSEM`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-outline",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "\n",
    "A toy `fastq` data can be found on [Google Drive](https://drive.google.com/drive/u/0/folders/11kQv7PXozsKkgeqADH-28bC_kZ-w_oHo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-animation",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To generate `fastqc` report,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-accuracy",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run RNA_calling.ipynb fastqc \\\n",
    "    --cwd output/rnaseq/fastqc \\\n",
    "    --samples data/sample_fastq.list \\\n",
    "    --data-dir data \\\n",
    "    --container container/rna_quantification.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-terrace",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To call gene-level RNA expression using `rnaseqc`,\n",
    "The `--mem 40G -J 3 -c csg.yml -q csg` part is crucial for it ask for the required memory to conduct the STAR alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-preservation",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/RNA_calling.ipynb rnaseqc_call \\\n",
    "    --cwd output \\ \n",
    "    --samples data/sample_fastq.list \\\n",
    "    --data-dir data \\\n",
    "    --STAR-index reference_data/STAR_Index/ \\\n",
    "    --gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.gene.ERCC.gtf \\\n",
    "    --container containers/rna_quantification.sif \\\n",
    "    --reference-fasta reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy.ERCC.fasta \\\n",
    "    --ref-flat reference_data/Homo_sapiens.GRCh38.103.chr.reformated.ERCC.gtf.ref.flat \\\n",
    "    --mem 40G -J 3 -c csg.yml -q csg     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-spencer",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To call transcript level RNA expression using `RSEM`:\n",
    "On a cluster, the `--mem 40G -J 3 -c csg.yml -q csg` part is crucial for it ask for the required memory to conduct the STAR alignment. However, while running on local computer, `--mem 40G` will not actually ask for 40G of mem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-consultation",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/RNA_calling.ipynb rsem_call \\\n",
    "    --cwd output \\ \n",
    "    --samples data/sample_fastq.list \\\n",
    "    --data-dir data \\\n",
    "    --STAR-index reference_data/STAR_Index/ \\\n",
    "    --RSEM-index reference_data/RSEM_Index/ \\\n",
    "    --gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.gene.ERCC.gtf \\\n",
    "    --container containers/rna_quantification.sif \\\n",
    "    --reference-fasta reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy.ERCC.fasta \\\n",
    "    --ref-flat reference_data/Homo_sapiens.GRCh38.103.chr.reformated.ERCC.gtf.ref.flat \\\n",
    "    --mem 40G -J 3 -c csg.yml -q csg     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-captain",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-recovery",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run RNA_calling.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-latvia",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fresh-employment",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files. MUST BE FULL PATH\n",
    "parameter: cwd = path\n",
    "# Sample meta data list\n",
    "parameter: samples = path\n",
    "# Raw data directory, default to the same directory as sample list\n",
    "parameter: data_dir = path(f\"{samples:d}\")\n",
    "\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Memory for Java virtual mechine (`picard`)\n",
    "parameter: java_mem = \"3G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')\n",
    "# Reference gene model\n",
    "parameter: gtf = path\n",
    "\n",
    "def get_samples(fn, dr):\n",
    "    import os\n",
    "    samples = [x.strip().split() for x in open(fn).readlines()]\n",
    "    names = []\n",
    "    files = []\n",
    "    for i, x in enumerate(samples):\n",
    "        if len(x)<3:\n",
    "            raise ValueError(f\"Line {i+1} of file {fn} must have 3 columns\")\n",
    "        names.append(x[0])\n",
    "        for y in x[1:]:\n",
    "            y = os.path.join(dr, y)\n",
    "            if not os.path.isfile(y):\n",
    "                raise ValueError(f\"File {y} does not exist\")\n",
    "            files.append(y)\n",
    "    if len(files) != len(set(files)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in fastq file list\")\n",
    "    return names, files\n",
    "\n",
    "sample_id, fastq = get_samples(samples, data_dir)\n",
    "# Is the RNA-seq data pair-end\n",
    "is_paired_end = 0 if len(fastq) == len(sample_id) else 1 \n",
    "from sos.utils import env\n",
    "env.logger.info('Input samples are `f{\"paired-end\" if is_paired_end else \"single-end\"}` sequences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-outline",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 0: QC before alignment\n",
    "\n",
    "This step utilize `fastqc` and will generate two QC report in `html` format\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `fastq1` and `fastq2`: paths to original `fastq.gz` file.\n",
    "\n",
    "### Step Outputs\n",
    "* Two `html` file for QC report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "independent-broadway",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastqc]\n",
    "input: fastq, group_by = is_paired_end + 1 \n",
    "output: f'{cwd}/{_input:bnn}_fastqc.html',f'{cwd}/{_input:bnn}_fastqc/fastqc_data.txt' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    fastqc ${_input} -o ${_output[0]:d}\n",
    "    unzip -o ${_output[0]:n}.zip -d ${cwd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-blink",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 1: Remove adaptor through `fastp`\n",
    "\n",
    "Documentation: [fastp](https://github.com/OpenGene/fastp)\n",
    "\n",
    "We use `fastp` in place of the `Trimmomatic` for fastp's ability to detect adapter from the reads. It was a c++ command line tool published in [Sept 2018](https://academic.oup.com/bioinformatics/article/34/17/i884/5093234). It  will use the following algorithm to detect the adaptors:\n",
    ">The adapter-sequence detection algorithm is based on two assumptions: the first is that only one adapter exists in the data; the second is that adapter sequences exist only in the read tails. These two assumptions are valid for major next-generation sequencers like Illumina HiSeq series, NextSeq series and NovaSeq series. We compute the k-mer (k = 10) of first N reads (N = 1 M). From this k-mer, the sequences with high occurrence frequencies (>0.0001) are considered as adapter seeds. Low-complexity sequences are removed because they are usually caused by sequencing artifacts. The adapter seeds are sorted by its occurrence frequencies. A tree-based algorithm is applied to extend the adapter seeds to find the real complete adapter\n",
    "\n",
    "It was demostrated that fastp can remove all the adaptor automatically and completely faster than Trimmomatic and cutadapt\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `fastq`: 1 set of fq.gz file for each sample. (2 files if paired end, 1 file if single end )\n",
    "\n",
    "### Step Outputs\n",
    "* 1 set of  `fastq.gz` file for alignment. (2 files if paired end, 1 file if single end )\n",
    "* The unpaired reads (i.e.where a read survived, but the partner read did not.) will be discarded by default as those were not used in the following steps. This feature can be added were it was needed. \n",
    "* 1 set of html documenting the quality of input reads(1 html file and 1 json file)\n",
    "\n",
    "### Step options\n",
    "\n",
    "A few options were selected to be customizable so that fastp step can have the same level of flexiblity as the Trimmomatic step had. They are:\n",
    "- min_len: length_required, reads shorter than this will be discarded, default is 15. (int [=15])\n",
    "- window_size: cut_window_size, the window size option shared by cut_front, cut_tail or cut_sliding. Range: 1~1000, default: 4 (int [=4])\n",
    "- leading/trailing : cut_front_mean_quality/cut_tail_mean_quality, the mean quality requirement option for cut_front/cut_tail, which move a sliding window from front/tail, drop the bases in the window if its mean quality < threshold. **Notice the choice of quality score in `fastp` (N=20) is a lot higher than that of `trimmomatic` (N=3)**. Default `fastp` setting is in line with that of [`cutadapt`](https://cutadapt.readthedocs.io/en/stable/guide.html).\n",
    "\n",
    "The default value are set to be the same as the default value of the `fastp` software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-sarah",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fastp_trim_adaptor]\n",
    "# sliding window setting\n",
    "parameter: window_size = 4\n",
    "parameter: required_quality = 20\n",
    "# the mean quality requirement option for cut_front\n",
    "parameter: leading = 20\n",
    "# the mean quality requirement option for cut_tail\n",
    "parameter: trailing = 20\n",
    "# reads shorter than length_required will be discarded\n",
    "parameter: min_len = 15\n",
    "input: fastq, group_by = is_paired_end + 1 , group_with = \"sample_id\"\n",
    "output: [f'{cwd}/{path(x):bnn}.trimmed.fq.gz' for x in _input]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash:  expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "        fastp -i ${f'{_input[0]} -I {_input[1]}' if is_paired_end else _input} -o ${ f'{_output[0]} -O {_output[1]}' if is_paired_end else _output } \\\n",
    "            --detect_adapter_for_pe  -V -h ${_output[0]:n}.html -j ${_output[0]:n}.json -w ${numThreads} \\\n",
    "            --length_required ${min_len}  -W ${window_size} -M ${required_quality} -5 -3 --cut_front_mean_quality ${leading} --cut_tail_mean_quality ${leading}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-looking",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 1 Alternative: Remove adaptor through `Trimmomatic`\n",
    "\n",
    "Documentation: [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)\n",
    "\n",
    "We have replaced this with `fastp`, see above, which performs better than `Trimmomatic` in terms of removing adaptors that `Trimmomatic` cannot detect. `fastp` can also automatically guess the adapter sequences from data and by default no adapter sequence is required for input to `fastp`.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `software_dir`: directory for the software\n",
    "* `fasta_with_adapters_etc`: **filename** for the adapter reference file. According to `Trimmomatic` documention,\n",
    "\n",
    "> As a rule of thumb newer libraries will use `TruSeq3`, but this really depends on your service provider. If you use FASTQC, the \"Overrepresented Sequences\" report can help indicate which adapter file is best suited for your data. \"Illumina Single End\" or \"Illumina Paired End\" sequences indicate single-end or paired-end `TruSeq2` libraries, and the appropriate adapter files are `TruSeq2-SE.fa` and `TruSeq2-PE.fa` respectively. \"TruSeq Universal Adapter\" or \"TruSeq Adapter, Index …\" indicates `TruSeq-3` libraries, and the appropriate adapter files are `TruSeq3-SE.fa` or `TruSeq3-PE.fa`, for single-end and paired-end data respectively. Adapter sequences for `TruSeq2` multiplexed libraries, indicated by \"Illumina Multiplexing \n",
    "…\", and the various RNA library preparations are not currently included.\n",
    "\n",
    "We have `fastqc` workflow previously defined and executed. Users should decide what fasta adapter reference to use based on `fastqc` results (or their own knowledge).\n",
    "\n",
    "### Step Outputs\n",
    "* Two paired `fastq.gz` file for alignment\n",
    "* Two unpaired `fastq.gz` \n",
    "\n",
    ">For single-ended data, one input and one output file are specified, plus the processing steps. For paired-end data, two input files are specified, and 4 output files, 2 for the 'paired' output where both reads survived the processing, and 2 for corresponding 'unpaired' output where a read survived, but the partner read did not.\n",
    "\n",
    "\n",
    "**You need to figure out from fastqc results what adapter reference sequence to use.**, eg `--fasta_with_adapters_etc TruSeq3-PE.fa`. These files can be downloaded from `Trimmomatic` github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prerequisite-electron",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[trimmomatic]\n",
    "# Path to the software. Default set to using our rna_quantification.sif image\n",
    "parameter: trimmomatic_jar = \"/opt/Trimmomatic-0.39/trimmomatic-0.39.jar\"\n",
    "# Illumina clip setting\n",
    "# Path to the reference adaptors\n",
    "parameter: fasta_with_adapters_etc = str\n",
    "parameter: seed_mismatches = 2\n",
    "parameter: palindrome_clip_threshold = 30\n",
    "parameter: simple_clip_threshold = 10\n",
    "# sliding window setting\n",
    "parameter: window_size = 4\n",
    "parameter: required_quality = 20\n",
    "# Other settings\n",
    "parameter: leading = 3\n",
    "parameter: trailing = 3\n",
    "parameter: min_len = 50\n",
    "input: fastq, group_by = 2, group_with = \"sample_id\"\n",
    "output: fq_1 = f'{cwd}/{_sample_id}_paired_{_input[0]:bn}.gz',\n",
    "        fq_1_up = f'{cwd}/{_sample_id}_unpaired_{_input[0]:bn}.gz',\n",
    "        fq_2 = f'{cwd}/{_sample_id}_paired_{_input[1]:bn}.gz',\n",
    "        fq_2_up = f'{cwd}/{_sample_id}_unpaired_{_input[1]:bn}.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    java -jar -Xmx${java_mem} ${trimmomatic_jar} PE -threads ${numThreads} \\\n",
    "        ${_input[0]} \\\n",
    "        ${_input[1]} \\\n",
    "        ${_output[0]} \\\n",
    "        ${_output[1]} \\\n",
    "        ${_output[2]} \\\n",
    "        ${_output[3]} \\\n",
    "        ILLUMINACLIP:${fasta_with_adapters_etc}:${seed_mismatches}:${palindrome_clip_threshold}:${simple_clip_threshold} \\\n",
    "        LEADING:${leading} TRAILING:${trailing} SLIDINGWINDOW:${window_size}:${required_quality} MINLEN:${min_len}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-institute",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 2: Alignment through `STAR`\n",
    "\n",
    "Documentation : [STAR](https://github.com/alexdobin/STAR) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_STAR.py)\n",
    "\n",
    "This step is the main step for `STAR` alignment. \n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* paths to clean `fastq.gz` file from Step 1.\n",
    "* `STAR_index`: directory for the STAR aligment index\n",
    "\n",
    "### Step Outputs\n",
    "\n",
    "* bam file output `${cwd}/{sample_id}.Aligned.sortedByCoord.bam`, will be used in step 3 and 4\n",
    "* bam file output `${cwd}/{sample_id}.Aligned.toTranscriptome.bam`, will be used in step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cleared-scroll",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_align]\n",
    "# STAR indexing file\n",
    "parameter: STAR_index = path\n",
    "\n",
    "# Alignment parameter\n",
    "parameter: outFilterMultimapNmax = 20 \n",
    "parameter: alignSJoverhangMin = 8 \n",
    "parameter: alignSJDBoverhangMin = 1 \n",
    "parameter: outFilterMismatchNmax = 999 \n",
    "parameter: outFilterMismatchNoverLmax = 0.1\n",
    "parameter: alignIntronMin = 20 \n",
    "parameter: alignIntronMax = 1000000 \n",
    "parameter: alignMatesGapMax = 1000000 \n",
    "parameter: outFilterType =  \"BySJout\" \n",
    "parameter: outFilterScoreMinOverLread = 0.33 \n",
    "parameter: outFilterMatchNminOverLread = 0.33 \n",
    "parameter: limitSjdbInsertNsj = 1200000 \n",
    "parameter: outSAMstrandField = \"intronMotif\" \n",
    "parameter: outFilterIntronMotifs = \"None\" \n",
    "parameter: alignSoftClipAtReferenceEnds = \"Yes\" \n",
    "parameter: quantMode = [\"TranscriptomeSAM\", \"GeneCounts\"]\n",
    "parameter: outSAMattrRGline = [\"ID:rg1\", \"SM:sm1\"]\n",
    "parameter: outSAMattributes = [\"NH\", \"HI\", \"AS\", \"nM\", \"NM\", \"ch\"] \n",
    "parameter: chimSegmentMin = 15 \n",
    "parameter: chimJunctionOverhangMin = 15 \n",
    "parameter: chimOutType = [\"Junctions\", \"WithinBAM\", \"SoftClip\"]\n",
    "parameter: chimMainSegmentMultNmax = 1 \n",
    "if int(mem.replace(\"G\",\"\")) <  40:\n",
    "    print(\"Insufficent memory for STAR, changing to 40G\")\n",
    "    star_mem = '40G'\n",
    "else:\n",
    "    star_mem = mem\n",
    "\n",
    "# This option is commented out because it will force the downstream analysis to use 40G, which significantlly slow down the process.\n",
    "input: output_from(\"fastp_trim_adaptor\"),group_with = \"sample_id\"\n",
    "output: cord_bam = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.out.bam',\n",
    "        trans_bam = f'{cwd}/{_sample_id}.Aligned.toTranscriptome.out.bam'\n",
    "        \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = star_mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    # clean up before using run_STAR.py wrapper\n",
    "    rm -rf ${cwd}/${_sample_id}.*.out.*.gz\n",
    "    rm -rf ${cwd}/${_sample_id}._STARpass1\n",
    "    run_STAR.py \\\n",
    "        ${STAR_index} ${_input:r}  ${_sample_id} \\\n",
    "        --output_dir ${cwd} \\\n",
    "        --outFilterMultimapNmax ${outFilterMultimapNmax} \\\n",
    "        --alignSJoverhangMin ${alignSJoverhangMin} \\\n",
    "        --alignSJDBoverhangMin ${alignSJDBoverhangMin} \\\n",
    "        --outFilterMismatchNmax ${outFilterMismatchNmax} \\\n",
    "        --outFilterMismatchNoverLmax ${outFilterMismatchNoverLmax} \\\n",
    "        --alignIntronMin ${alignIntronMin} \\\n",
    "        --alignIntronMax ${alignIntronMax} \\\n",
    "        --alignMatesGapMax ${alignMatesGapMax} \\\n",
    "        --outFilterType ${outFilterType} \\\n",
    "        --outFilterScoreMinOverLread ${outFilterScoreMinOverLread} \\\n",
    "        --outFilterMatchNminOverLread ${outFilterMatchNminOverLread} \\\n",
    "        --limitSjdbInsertNsj ${limitSjdbInsertNsj} \\\n",
    "        --outSAMstrandField ${outSAMstrandField} \\\n",
    "        --outFilterIntronMotifs ${outFilterIntronMotifs} \\\n",
    "        --alignSoftClipAtReferenceEnds ${alignSoftClipAtReferenceEnds} \\\n",
    "        --quantMode ${\" \".join(quantMode)} \\\n",
    "        --outSAMattrRGline ${\" \".join(outSAMattrRGline)} \\\n",
    "        --outSAMattributes ${\" \".join(outSAMattributes)} \\\n",
    "        --chimSegmentMin ${chimSegmentMin} \\\n",
    "        --chimJunctionOverhangMin ${chimJunctionOverhangMin} \\\n",
    "        --chimOutType ${\" \".join(chimOutType)} \\\n",
    "        --chimMainSegmentMultNmax ${chimMainSegmentMultNmax} \\\n",
    "        --threads ${numThreads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20955111-5677-42d6-a19a-b8c6db86ee66",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opponent-zambia",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 3: Mark duplicates reads & QC through `Picard`\n",
    "\n",
    "This step is the first QC step after `STAR` alignment. This step will performed QC to collect multipe metrics regarding the RNASeq using Picard. Then it will also generate a new `.bam` file with duplication marked with the hexadecimal value of `0x0400`, which corresponds to a decimal value of 1024\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step Inputs:\n",
    "\n",
    "* `STAR_bam`: path to the output in Step 2.\n",
    "\n",
    "### Step Outputs:\n",
    "\n",
    "* A collection of metrics file for each of the samples\n",
    "* A new `.bam` file with duplication marked with the hexadecimal value of `0x0400`, which corresponds to a decimal value of 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226b7fc-465f-4116-ae5c-18dcc549653f",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_1, rsem_call_1, picard_qc]\n",
    "# Path to flat reference file, for computing QC metric\n",
    "parameter: ref_flat = path()\n",
    "# Path to the software. Default set to using our rna_quantification.sif image\n",
    "parameter: picard_jar = \"/opt/picard-tools/picard.jar\"\n",
    "# The fasta reference file used to generate star index\n",
    "parameter: reference_fasta = path\n",
    "# For the patterned flowcell models (HiSeq X), change to 2500\n",
    "parameter: optical_distance = 100\n",
    "\n",
    "input: output_from(\"STAR_align\"), group_with = \"sample_id\"\n",
    "output: picard_metrics = f'{cwd}/{_sample_id}.alignment_summary_metrics',\n",
    "        md_bam = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.out.md.bam',\n",
    "        md_metrics = f'{cwd}/{_sample_id}.Aligned.sortedByCoord.md.metrics'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads        \n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "        java -jar -Xmx${java_mem} ${picard_jar} CollectMultipleMetrics \\\n",
    "            -REFERENCE_SEQUENCE ${reference_fasta} \\\n",
    "            -PROGRAM CollectAlignmentSummaryMetrics \\\n",
    "            -PROGRAM CollectInsertSizeMetrics \\\n",
    "            -PROGRAM QualityScoreDistribution \\\n",
    "            -PROGRAM MeanQualityByCycle \\\n",
    "            -PROGRAM CollectBaseDistributionByCycle \\\n",
    "            -PROGRAM CollectGcBiasMetrics \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -INPUT  ${_input[\"cord_bam\"]} \\\n",
    "            -OUTPUT  ${_output[0]:nnn}\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[-1]:n}.stderr', stdout = f'{_output[-1]:n}.stdout'\n",
    "        java -Xmx${java_mem} -jar ${picard_jar} MarkDuplicates \\\n",
    "            -I ${_input[\"cord_bam\"]}  \\\n",
    "            -O ${_output[1]} \\\n",
    "            -PROGRAM_RECORD_ID null \\\n",
    "            -M ${_output[2]} \\\n",
    "            -TMP_DIR ${cwd}\\\n",
    "            -MAX_RECORDS_IN_RAM 500000 -SORTING_COLLECTION_SIZE_RATIO 0.25 \\\n",
    "            -ASSUME_SORT_ORDER coordinate \\\n",
    "            -TAGGING_POLICY DontTag \\\n",
    "            -OPTICAL_DUPLICATE_PIXEL_DISTANCE ${optical_distance} \\\n",
    "            -CREATE_INDEX true \\\n",
    "            -CREATE_MD5_FILE true \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -REMOVE_SEQUENCING_DUPLICATES false \\\n",
    "            -REMOVE_DUPLICATES false \n",
    "\n",
    "stop_if(not ref_flat.is_file())\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stderr'\n",
    "        # Get only line with rRNA and transcript_id\n",
    "        cat ${gtf}| grep rRNA | grep transcript_id > ${gtf}.tmp\n",
    "        samtools view -H ${_input[\"cord_bam\"]}  > ${_input[\"cord_bam\"]}.RI  \n",
    "\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stderr', stdout = f'{_output[0]:n}.rna_metrics.stderr'\n",
    "        import pandas as pd\n",
    "        from collections import defaultdict\n",
    "        chrom = []\n",
    "        start = []\n",
    "        end = []\n",
    "        strand = []\n",
    "        tag = []\n",
    "        annotation_gtf = \"${gtf}.tmp\"\n",
    "        with open(annotation_gtf, 'r') as gtf:\n",
    "            for row in gtf:\n",
    "                row = row.strip().split('\\t')\n",
    "                if row[0][0]=='#' or row[2]!=\"transcript\": continue # skip header\n",
    "                chrom.append(row[0])\n",
    "                start.append(row[3])\n",
    "                end.append(row[4])\n",
    "                strand.append(row[6])\n",
    "                attributes = defaultdict()\n",
    "                for a in row[8].replace('\"', '').split(';')[:-1]:\n",
    "                    kv = a.strip().split(' ')\n",
    "                    if kv[0]!='tag':\n",
    "                        attributes[kv[0]] = kv[1]\n",
    "                    else:\n",
    "                        attributes.setdefault('tags', []).append(kv[1])\n",
    "                tag.append(attributes)\n",
    "        transcript_id = [x[\"transcript_id\"] for x in tag]\n",
    "        RI = pd.DataFrame(data={'chr':chrom, 'start':start, 'end':end, 'strand':strand,'transcript_id' : transcript_id })\n",
    "        RI.to_csv(\"${_input[\"cord_bam\"]}.RI\", index = 0, header = 0, mode = \"a\" )\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.rna_metrics.stdout', stdout = f'{_output[0]:n}.rna_metrics.stderr'\n",
    "        java -jar -Xmx${java_mem} ${picard_jar} CollectRnaSeqMetrics \\\n",
    "            -REF_FLAT ${ref_flat} \\\n",
    "            -RIBOSOMAL_INTERVALS ${_input[\"cord_bam\"]}.RI \\\n",
    "            -STRAND_SPECIFICITY NONE \\\n",
    "            -CHART_OUTPUT ${_output[0]:n}.rna_metrics.pdf \\\n",
    "            -VALIDATION_STRINGENCY STRICT \\\n",
    "            -INPUT ${_input[\"cord_bam\"]}  \\\n",
    "            -OUTPUT  ${_output[0]:n}.rna_metrics\n",
    "        rm ${gtf}.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fancy-thermal",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "buried-arizona",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 4: Post aligment QC through `RNA-SeQC`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-reviewer",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Documentation : [RNA-SeQC](https://github.com/getzlab/rnaseqc) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_rnaseqc.py)\n",
    "\n",
    "This step is second QC step after `STAR` alignment. It will perform RNA-seq quantification as well. \n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `QC_bam`: path to the output in Step 3.\n",
    "* `gtf`: reference genome `.gtf` file, this gtf file need to have the same chr name format as the index used to generate the bam file and must be on collapsed gene gtf \n",
    "\n",
    "### Step Outputs\n",
    "\n",
    "\n",
    "The following output files are generated in the output directory you provide:\n",
    "\n",
    "* {sample}.metrics.tsv : A tab-delimited list of (Statistic, Value) pairs of all statistics and metrics recorded.\n",
    "\n",
    "* {sample}.exon_reads.gct : A tab-delimited GCT file with (Exon ID, Gene Name, coverage) tuples for all exons which had at least part of one read mapped.\n",
    "\n",
    "* {sample}.gene_reads.gct : A tab-delimited GCT file with (Gene ID, Gene Name, coverage) tuples for all genes which had at least one read map to at least one of its exons\n",
    "\n",
    "* {sample}.gene_tpm.gct : A tab-delimited GCT file with (Gene ID, Gene Name, TPM) tuples for all genes reported in the gene_reads.gct file. Note: this file is renamed to .gene_rpkm.gct if the --rpkm flag is present.\n",
    "\n",
    "* {sample}.fragmentSizes.txt : A list of fragment sizes recorded, if a BED file was provided\n",
    "\n",
    "* {sample}.coverage.tsv : A tab-delimited list of (Gene ID, Transcript ID, Mean Coverage, Coverage Std, Coverage CV) tuples for all transcripts encountered in the GTF.\n",
    "\n",
    "### `strand` option\n",
    "\n",
    "If you have strand-specific data, specify the library type `rf` or `fr`. Please refer to [Signal et al (2022)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-022-04572-7) for an explanation what these library types mean. **If you are not sure about the strandedness of your data, you can use the [how_are_we_stranded_here](https://github.com/betsig/how_are_we_stranded_here) project to test and report strandedness of RNA-seq fastq files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tested-source",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_2]\n",
    "\n",
    "# Default empty for unstranded. Options are fr or rf\n",
    "parameter: strand = \"\"\n",
    "input: group_by = 3, group_with = 'sample_id'\n",
    "output: f'{cwd}/{_sample_id}.rnaseqc.gene_tpm.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.gene_reads.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.exon_reads.gct.gz',\n",
    "        f'{cwd}/{_sample_id}.rnaseqc.metrics.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    cd ${cwd} && \\\n",
    "    run_rnaseqc.py \\\n",
    "        ${gtf:a} \\\n",
    "        ${_input[\"md_bam\"]:a} \\\n",
    "        ${_sample_id}.rnaseqc \\\n",
    "        -o ./ \\\n",
    "        ${(\"--stranded \" + strand) if strand else \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-shame",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The RNASEQC results were merged in the following step,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-victim",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rnaseqc_call_3]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{samples:bn}.rnaseqc.gene_tpm.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.gene_readsCount.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.exon_readsCount.gct.gz',\n",
    "        f'{cwd}/{samples:bn}.rnaseqc.metrics.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    def make_gct(gct_path):\n",
    "        # sample_name\n",
    "        sample_name = \".\".join(os.path.basename(gct_path).split(\".\")[:-4])\n",
    "        # read_input\n",
    "        pre_gct = pd.read_csv(gct_path,sep = \"\\t\",\n",
    "                              skiprows= 2,index_col=\"Name\").drop(\"Description\",axis = 1)\n",
    "        pre_gct.index.name = \"gene_ID\"\n",
    "        pre_gct.columns = [sample_name]\n",
    "        return(pre_gct)\n",
    "\n",
    "    def merge_gct(gct_path_list):\n",
    "        gct = pd.DataFrame()\n",
    "        for gct_path in gct_path_list:\n",
    "            #check duplicated indels and remove them.\n",
    "            gct_col = make_gct(gct_path)\n",
    "            gct = gct.merge(gct_col,right_index=True,left_index = True,how = \"outer\")\n",
    "        return gct\n",
    "\n",
    "    input_list = [${_input:r,}]\n",
    "    tpm_list = input_list[0::4]\n",
    "    gc_list = input_list[1::4]\n",
    "    ec_list = input_list[2::4]\n",
    "    gct_path_list_list = [tpm_list,gc_list,ec_list]\n",
    "    output_path = [${_output:r,}][0:3]\n",
    "    for i in range(0,len(output_path)):\n",
    "        output = merge_gct(gct_path_list_list[i])\n",
    "        output.to_csv(output_path[i], sep = \"\\t\")\n",
    "    metrics_list = input_list[3::4]\n",
    "    with open(\"${cwd}/${samples:bn}.rnaseqc.metrics_output_list\", \"w\") as f:\n",
    "        f.write('\\n'.join(metrics_list))\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    aggregate_rnaseqc_metrics.py  ${_output[3]:n}_output_list ${_output[3]:nn}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-liability",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Step 6: Quantify expression through `RSEM`\n",
    "\n",
    "Documentation : [RSEM](https://deweylab.github.io/RSEM/rsem-calculate-expression.html) and [Script in docker](https://github.com/broadinstitute/gtex-pipeline/blob/master/rnaseq/src/run_RSEM.py)\n",
    "\n",
    "This step generate the expression matrix from STAR output. Estimate gene and isoform expression from RNA-Seq data are generated.\n",
    "\n",
    "### Step Input\n",
    "\n",
    "* transcript-level BAM file: path to the output of Step 3.\n",
    "* `RSEM_index`: path to RSEM index\n",
    "\n",
    "### Step Outputs\n",
    "Please see the output section of https://deweylab.github.io/RSEM/rsem-calculate-expression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "settled-sharing",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_2]\n",
    "parameter: RSEM_index = path\n",
    "parameter: max_frag_len = 1000\n",
    "# Default empty for unstranded. Options are fr or rf\n",
    "parameter: strand = \"\"\n",
    "estimate_rspd = True\n",
    "input: output_from(\"STAR_align\"),  group_by = 2, group_with = 'sample_id' # After star there will only be two output per sample id, regardless of paired/unpaired\n",
    "output: f'{cwd}/{_sample_id}.rsem.isoforms.results', f'{cwd}/{_sample_id}.rsem.genes.results',f'{cwd}/{_sample_id}.rsem.stat/{_sample_id}.rsem.cnt'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    run_RSEM.py ${RSEM_index:a} ${_input[\"trans_bam\"]:a} ${_sample_id} \\\n",
    "        -o ${_output[0]:d} \\\n",
    "        --max_frag_len ${max_frag_len} \\\n",
    "        --estimate_rspd ${'true' if estimate_rspd else 'false'} \\\n",
    "        --paired_end ${\"true\" if is_paired_end else \"false\"} \\\n",
    "        --is_stranded ${\"true\" if strand else \"false\"} \\\n",
    "        --threads ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-apartment",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The RSEM results were merged in the following steps, seven files (four for each columns in the isoform output and 3 for each of the genes output) will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-project",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_3]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/{samples:bn}.rsem_transcripts_expected_count.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_tpm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_fpkm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_transcripts_isopct.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_expected_count.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_tpm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem_genes_fpkm.txt.gz',\n",
    "        f'{cwd}/{samples:bn}.rsem.aggregated_quality.metrics.tsv'\n",
    "        \n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    input_list = [${_input:r,}]\n",
    "    with open('${cwd}/${samples:bn}.rsem.isoforms_output_list', \"w\") as f:\n",
    "        f.write('\\n'.join(input_list[0::3]))\n",
    "    with open('${cwd}/${samples:bn}.rsem.genes_output_list', \"w\") as f:\n",
    "        f.write('\\n'.join(input_list[1::3]))\n",
    "\n",
    "\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "         aggregate_rsem_results.py ${cwd}/${samples:bn}.rsem.isoforms_output_list {expected_count,TPM,FPKM,IsoPct} ${_output[0]:nnn}\n",
    "         aggregate_rsem_results.py ${cwd}/${samples:bn}.rsem.genes_output_list {expected_count,TPM,FPKM} ${_output[1]:nnn} \n",
    "\n",
    "R:  container=container, expand= \"${ }\", stderr = f'{_output[-1]:n}.stderr', stdout = f'{_output[-1]:n}.stdout'\n",
    "     readRSEM.cnt <- function (source) {\n",
    "\n",
    "            # RSEM .cnt files gives statistics about the (transcriptome) alignment passed to RSEM:\n",
    "            # Row 1: N0 (# unalignable reads);\n",
    "            #        N1 (# alignable reads);\n",
    "            #        N2 (# filtered reads due to too many alignments);\n",
    "            #        N_tot (N0+N1+N2)\n",
    "            # Row 2: nUnique (# reads aligned uniquely to a gene);\n",
    "            #        nMulti (# reads aligned to multiple genes);\n",
    "            #        nUncertain (# reads aligned to multiple locations in the given reference sequences, which include isoform-level multi-mapping reads)\n",
    "            # Row 3: nHits (# total alignments);\n",
    "            #        read_type (0: single-end read, no quality; 1: single-end read, with quality score; 2: paired-end read, no quality score; 3: paired-end read, with quality score)\n",
    "            # Source: https://groups.google.com/forum/#!topic/rsem-users/usmPKgsC5LU\n",
    "            # Note: N1 = nUnique + nMulti\n",
    "\n",
    "            stopifnot(file.exists(source[1]))\n",
    "            isDir <- file.info(source)$isdir\n",
    "            if (isDir) {\n",
    "                files <- system(paste(\"find\", source, \"-name \\\"*.rsem.cnt\\\"\"), intern=TRUE)\n",
    "                stopifnot(length(files) > 0)\n",
    "                samples <- gsub(\"_rsem.cnt\", \"\", basename(files), fixed=TRUE)\n",
    "            } else {\n",
    "                files <- source\n",
    "                samples <- gsub(\".rsem.cnt\", \"\", basename(files), fixed=TRUE)\n",
    "            }\n",
    "\n",
    "            metrics <- list()\n",
    "            for (i in 1:length(files)) {\n",
    "                m <- read.table(files[i], header=FALSE, sep=\" \", comment.char=\"#\", stringsAsFactors=FALSE, nrows=3, fill=TRUE)\n",
    "                metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                File=files[i],\n",
    "                TotalReads=m[1, 4],\n",
    "                AlignedReads=m[1, 2],\n",
    "                UniquelyAlignedReads=m[2, 1],\n",
    "                stringsAsFactors=FALSE)\n",
    "            }\n",
    "            metrics <- do.call(rbind, metrics)\n",
    "            row.names(metrics) <- metrics$Sample\n",
    "\n",
    "            return(metrics)\n",
    "        }\n",
    "\n",
    "        sourceRSEM = c(${_input:r,})\n",
    "        sourceRSEM = sourceRSEM[seq(3,length(sourceRSEM),3)]\n",
    "        metrics.RSEM = readRSEM.cnt(sourceRSEM)\n",
    "        write.table(metrics.RSEM, file=\"${_output[-1]}\",col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8ce08-51e5-4c28-882e-9ff2784ef832",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 7: summarize with MultiQC\n",
    "\n",
    "[MultiQC](https://multiqc.info/docs/#using-multiqc) \n",
    "\n",
    "\n",
    ">MultiQC is a reporting tool that parses summary statistics from results and log files generated by other bioinformatics tools. MultiQC doesn't run other tools for you - it's designed to be placed at the end of analysis pipelines or to be run manually when you've finished running your tools. When you launch MultiQC, it recursively searches through any provided file paths and finds files that it recognises. It parses relevant information from these and generates a single stand-alone HTML report file. It also saves a directory of data files with all parsed data for further downstream use.\n",
    "\n",
    "MultiQC will automatically generate QC report for anything embedded within the given directory. Therefore providing the directory containing all the output will surfice.\n",
    "\n",
    "The output of MultiQC is a multi-module report each corresponding to the quality report of each step of analysis previously performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd53508-103c-4da4-928b-fd7a4eed87c9",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_4,rnaseqc_call_4]\n",
    "output: f'{cwd}/{samples:bn}.multiqc_report.html'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads\n",
    "report: output = f\"{_output:n}.multiqc_config.yml\"\n",
    "  extra_fn_clean_exts:\n",
    "      - '_rsem'\n",
    "  fn_ignore_dirs:\n",
    "      - '*_STARpass1'\n",
    "bash:  container=container,expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "    multiqc ${_input:d} -v -n ${_output:b} -o ${_output:d} -c ${_output:n}.multiqc_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc2367-eeb7-46bc-b0bb-3a6c0d91b8ea",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rsem_call_5, rnaseqc_call_5]\n",
    "# Path to flat reference file, for computing QC metric\n",
    "parameter: ref_flat = path()\n",
    "stop_if(not ref_flat.is_file() )\n",
    "output: f'{cwd}/{samples}.picard.aggregated_quality.metrics.tsv'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    ## Define Function\n",
    "    readPicard.alignment_summary_metrics <- function (source) {\n",
    "    \n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.alignment_summary_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".alignment_summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".alignment_summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=2)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PF_READS=sum(m$PF_READS[1:2]),\n",
    "                                   PF_READS_ALIGNED=sum(m$PF_READS_ALIGNED[1:2]),\n",
    "                                   PCT_PF_READS_ALIGNED=sum(m$PF_READS_ALIGNED[1:2])/sum(m$PF_READS[1:2]),\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    readPicard.rna_metrics <- function(source) {\n",
    "    \n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.rna_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".rna_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".rna_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PCT_RIBOSOMAL_BASES=m$PCT_RIBOSOMAL_BASES,\n",
    "                                   PCT_CODING_BASES=m$PCT_CODING_BASES,\n",
    "                                   PCT_UTR_BASES=m$PCT_UTR_BASES,\n",
    "                                   PCT_INTRONIC_BASES=m$PCT_INTRONIC_BASES,\n",
    "                                   PCT_INTERGENIC_BASES=m$PCT_INTERGENIC_BASES,\n",
    "                                   PCT_MRNA_BASES=m$PCT_MRNA_BASES,\n",
    "                                   PCT_USABLE_BASES=m$PCT_USABLE_BASES,\n",
    "                                   MEDIAN_CV_COVERAGE=m$MEDIAN_CV_COVERAGE,\n",
    "                                   MEDIAN_5PRIME_BIAS=m$MEDIAN_5PRIME_BIAS,\n",
    "                                   MEDIAN_3PRIME_BIAS=m$MEDIAN_3PRIME_BIAS,\n",
    "                                   MEDIAN_5PRIME_TO_3PRIME_BIAS=m$MEDIAN_5PRIME_TO_3PRIME_BIAS,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    readPicard.duplicate_metrics <- function(source) {\n",
    "    \n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.Aligned.sortedByCoord.md.metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".Aligned.sortedByCoord.md.metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".Aligned.sortedByCoord.md.metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   PERCENT_DUPLICATION=m$PERCENT_DUPLICATION,\n",
    "                                   ESTIMATED_LIBRARY_SIZE=m$ESTIMATED_LIBRARY_SIZE,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    readPicard.wgs_metrics <- function (source) {\n",
    "    \n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.wgs_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".wgs_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".wgs_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   MEDIAN_COVERAGE=m$MEDIAN_COVERAGE,\n",
    "                                   MAD_COVERAGE=m$MAD_COVERAGE,\n",
    "                                   PCT_EXC_DUPE=m$PCT_EXC_DUPE,\n",
    "                                   PCT_EXC_TOTAL=m$PCT_EXC_TOTAL,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    readPicard.insert_size_metrics <- function (source) {\n",
    "    \n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.insert_size_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".insert_size_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".insert_size_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   MEDIAN_INSERT_SIZE=m$MEDIAN_INSERT_SIZE,\n",
    "                                   MODE_INSERT_SIZE=m$MODE_INSERT_SIZE,\n",
    "                                   MEDIAN_ABSOLUTE_DEVIATION=m$MEDIAN_ABSOLUTE_DEVIATION,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    readPicard.gc_bias.summary_metrics <- function (source) {\n",
    "    \n",
    "      stopifnot(length(source) == 1)\n",
    "      stopifnot(file.exists(source))\n",
    "      isDir <- file.info(source)$isdir\n",
    "      if (isDir) {\n",
    "        files <- system(paste(\"find -L\", source, \"-name \\\"*.gc_bias.summary_metrics\\\"\"), intern=TRUE)\n",
    "        stopifnot(length(files) > 0)\n",
    "        samples <- gsub(\".gc_bias.summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      } else {\n",
    "        files <- source\n",
    "        samples <- gsub(\".gc_bias.summary_metrics\", \"\", basename(files), fixed=TRUE)\n",
    "      }\n",
    "    \n",
    "      metrics <- list()\n",
    "      for (i in 1:length(files)) {\n",
    "        m <- read.table(files[i], header=TRUE, sep=\"\\t\", comment.char=\"#\", stringsAsFactors=FALSE, nrows=1)\n",
    "        metrics[[i]] <- data.frame(Sample=samples[i],\n",
    "                                   File=files[i],\n",
    "                                   AT_DROPOUT=m$AT_DROPOUT,\n",
    "                                   GC_DROPOUT=m$GC_DROPOUT,\n",
    "                                   stringsAsFactors=FALSE)\n",
    "      }\n",
    "      metrics <- do.call(rbind, metrics)\n",
    "      row.names(metrics) <- metrics$Sample\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    readPicard <- function(source) {\n",
    "      metrics.aln <- readPicard.alignment_summary_metrics(source)\n",
    "      metrics.rna <- readPicard.rna_metrics(source)\n",
    "      metrics.dup <- readPicard.duplicate_metrics(source)\n",
    "    \n",
    "      stopifnot(all(row.names(metrics.aln) %in% row.names(metrics.rna)) &\n",
    "                all(row.names(metrics.rna) %in% row.names(metrics.dup)) &\n",
    "                all(row.names(metrics.dup) %in% row.names(metrics.aln)))\n",
    "    \n",
    "      metrics.aln$File <- NULL\n",
    "      metrics.rna$File <- NULL\n",
    "      metrics.dup$File <- NULL\n",
    "      metrics.rna$Sample <- NULL\n",
    "      metrics.dup$Sample <- NULL\n",
    "    \n",
    "      metrics <- cbind(metrics.aln, metrics.rna[row.names(metrics.aln), ])\n",
    "      metrics <- cbind(metrics, metrics.dup[row.names(metrics.aln), ])\n",
    "    \n",
    "      return(metrics)\n",
    "    }\n",
    "    \n",
    "    ## Execution  \n",
    "    sourcePicard = ${_input[-1]:dr}\n",
    "    \n",
    "    Picard_qualityMetrics <- readPicard(sourcePicard)\n",
    "    write.table(Picard_qualityMetrics, file=\"${_output}\",col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff79c07-46b7-4b74-a0c5-daf97a9af59a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
