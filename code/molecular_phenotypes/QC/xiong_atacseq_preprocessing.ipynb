{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55783e3b-582f-4eb9-8d1a-f3647fef7c73",
   "metadata": {},
   "source": [
    "# Xiong Lab Single-nuclei ATAC-seq Preprocessing Pipeline\n",
    "---\n",
    "## Overview\n",
    "\n",
    "This pipeline preprocesses single-nucleus ATAC-seq (snATAC-seq) data from the Kellis lab (Xiong et al.) for downstream chromatin accessibility QTL (caQTL) analysis. It processes pseudobulk peak count data across six major brain cell types.\n",
    "\n",
    "**Pipeline Purpose:**\n",
    "- Transform raw pseudobulk peak counts into analysis-ready formats\n",
    "- Remove technical confounders while preserving biological variation\n",
    "- Generate QTL-ready phenotype files for genome-wide caQTL mapping\n",
    "\n",
    "**Supported Cell Types:**\n",
    "- **Mic** - Microglia\n",
    "- **Astro** - Astrocytes\n",
    "- **Oligo** - Oligodendrocytes\n",
    "- **Ex** - Excitatory neurons\n",
    "- **In** - Inhibitory neurons\n",
    "- **OPC** - Oligodendrocyte precursor cells\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Structure\n",
    "\n",
    "This pipeline consists of **three sequential steps**:\n",
    "\n",
    "#### Step 0: Sample ID Mapping\n",
    "\n",
    "**Input:**\n",
    "- Sample mapping file: `rosmap_sample_mapping_data.csv`\n",
    "- Original metadata files: `metadata_{celltype}.csv`\n",
    "- Original count files: `pseudobulk_peaks_counts_{celltype}.csv.gz`\n",
    "\n",
    "**Process:**\n",
    "1. Loads sample ID mapping between individualID and sampleid\n",
    "2. Processes metadata files:\n",
    "   - Adds `sampleid` column after `individualID`\n",
    "   - Maps individualID to sampleid where mapping exists\n",
    "   - Keeps original individualID for unmapped samples\n",
    "3. Processes count matrix files:\n",
    "   - Renames column headers from individualID to sampleid\n",
    "   - Maintains count data integrity\n",
    "\n",
    "#### Step 1: Pseudobulk QC & Calculate Residuals with biological variation\n",
    "\n",
    "**Input:**\n",
    "- Mapped metadata: `metadata_{celltype}.csv` (from Step 0)\n",
    "- Mapped peak counts: `pseudobulk_peaks_counts_{celltype}.csv.gz` (from Step 0)\n",
    "- Sample covariates: `rosmap_cov.txt`\n",
    "- hg38 blacklist: `hg38-blacklist.v2.bed.gz`\n",
    "\n",
    "**Process:**\n",
    "1. Loads pseudobulk peak count matrix and metadata\n",
    "2. **Filters samples with n_nuclei > 20**\n",
    "3. Calculates technical QC metrics per sample:\n",
    "   - `log_n_nuclei`: Log-transformed number of nuclei\n",
    "   - `med_nucleosome_signal`: Median nucleosome signal\n",
    "   - `med_tss_enrich`: Median TSS enrichment score\n",
    "   - `log_med_n_tot_fragment`: Log-transformed median total fragments (sequencing depth)\n",
    "   - `log_total_unique_peaks`: Log-transformed count of unique peaks detected\n",
    "4. Filters blacklisted genomic regions using `foverlaps()`\n",
    "5. Merges with covariates (pmi, study) - **excludes msex and age_death**\n",
    "6. Applies expression filtering with `filterByExpr()`:\n",
    "   - `min.count = 5`: Minimum 5 reads in at least one sample\n",
    "   - `min.total.count = 15`: Minimum 15 total reads across all samples\n",
    "   - `min.prop = 0.1`: Peak must be expressed in ≥10% of samples\n",
    "7. TMM normalization with `calcNormFactors()`\n",
    "8. Saves **filtered raw counts** (used for region-specific analysis if needed)\n",
    "9. Handles sequencingBatch and Library as covariates\n",
    "10. Fits linear model using `voom()` and `lmFit()`:\n",
    "    ```r\n",
    "    model <- ~ log_n_nuclei + med_nucleosome_signal + med_tss_enrich + \n",
    "               log_med_n_tot_fragment + log_total_unique_peaks + \n",
    "               sequencingBatch_factor + Library_factor + pmi + study\n",
    "    ```\n",
    "11. Calculates residuals using `predictOffset()`: `offset + residuals`\n",
    "    - **Preserves biological variation** (sex, age)\n",
    "    - Removes technical variation and study effects\n",
    "\n",
    "**Key Variables Regressed Out:**\n",
    "- Technical: sequencing depth, nuclei count, nucleosome signal, TSS enrichment, batch, library\n",
    "- Study effects: pmi, study cohort\n",
    "\n",
    "**Key Variables Preserved:**\n",
    "- Sex (msex)\n",
    "- Age at death (age_death)\n",
    "\n",
    "\n",
    "#### Step 2: Phenotype Reformatting\n",
    "\n",
    "**Input:**\n",
    "- `{celltype}_residuals.txt` from Step 1 (in `2_residuals/{celltype}/`)\n",
    "\n",
    "**Process:**\n",
    "1. Reads residuals file with proper handling of peak IDs and sample columns\n",
    "2. Parses peak coordinates from peak IDs (format: `chr-start-end`)\n",
    "3. Converts peaks to **midpoint coordinates**:\n",
    "\n",
    "Use for:\n",
    "Genome-wide caQTL mapping with FastQTL, TensorQTL, or MatrixEQTL\n",
    "Analysis that accounts for or investigates sex/age effects\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline Outputs\n",
    "\n",
    "**From Step 0:**\n",
    "`metadata_{celltype}.csv`: Metadata with mapped sampleid\n",
    "`pseudobulk_peaks_counts_{celltype}.csv.gz`: Counts with mapped sampleid headers\n",
    "\n",
    "**From Step 1:**\n",
    "`{celltype}_residuals.txt`: Covariate-adjusted residuals (log2-CPM scale)\n",
    "`{celltype}_filtered_raw_counts.txt`: TMM-normalized counts\n",
    "`{celltype}_results.rds`: Complete analysis results\n",
    "`{celltype}_summary.txt`: QC and filtering statistics\n",
    "`{celltype}_variable_explanation.txt`: Variable documentation\n",
    "\n",
    "**From Step 2:**\n",
    "`{celltype}_kellis_xiong_snatac_phenotype.bed.gz`: Genome-wide QTL-ready BED file\n",
    "\n",
    "---\n",
    "\n",
    "**Input files** needed to run this pipeline can be downloaded [here](https://drive.google.com/drive/folders/1l1RJx5toqg_WOlWW3gy-ynkrodi8oqXv?usp=drive_link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58392fc-da3a-4032-9cc3-6f58fdf6c99b",
   "metadata": {},
   "source": [
    "#### Before you start, let's set your working paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3509701b-e03e-49a7-944f-14539b6a46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir <- \" \" # insert your input dir\n",
    "output_dir <- \" \" #insert your output dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d5f6a-259b-4fa0-ba06-2a83fa19577e",
   "metadata": {},
   "source": [
    "## Step 0: Check sample ID\n",
    "\n",
    "**Purpose:** Maps original sample identifiers (individualID) to standardized sample IDs (sampleid) across metadata and count matrix files.\n",
    "\n",
    "---\n",
    "\n",
    "#### Input:\n",
    "\n",
    "**Sample Mapping Reference:**\n",
    "- `rosmap_sample_mapping_data.csv`: Contains mapping between individualID and sampleid\n",
    "\n",
    "**Metadata Files (per cell type):**\n",
    "- `metadata_Ast.csv`\n",
    "- `metadata_Ex.csv`\n",
    "- `metadata_In.csv`\n",
    "- `metadata_Microglia.csv`\n",
    "- `metadata_Oligo.csv`\n",
    "- `metadata_OPC.csv`\n",
    "\n",
    "**Count Matrix Files (per cell type):**\n",
    "- `pseudobulk_peaks_counts_Ast.csv.gz`\n",
    "- `pseudobulk_peaks_counts_Ex.csv.gz`\n",
    "- `pseudobulk_peaks_counts_In.csv.gz`\n",
    "- `pseudobulk_peaks_counts_Microglia.csv.gz`\n",
    "- `pseudobulk_peaks_counts_Oligo.csv.gz`\n",
    "- `pseudobulk_peaks_counts_OPC.csv.gz`\n",
    "\n",
    "\n",
    "#### Process:\n",
    "\n",
    "**Part 1: Process Metadata Files**\n",
    "\n",
    "1. Loads sample mapping dictionary from `rosmap_sample_mapping_data.csv`\n",
    "2. Creates a keyed data.table for fast lookups: `individualID → sampleid`\n",
    "3. For each metadata file:\n",
    "   - Reads the CSV file\n",
    "   - Finds the position of the `individualID` column\n",
    "   - Creates a new `sampleid` column\n",
    "   - For each sample:\n",
    "     - If mapping exists: uses the mapped sampleid\n",
    "     - If no mapping: uses the original individualID (preserves unmapped samples)\n",
    "   - Inserts `sampleid` column immediately after `individualID` column\n",
    "   - Saves updated metadata file\n",
    "\n",
    "**Part 2: Process Count Matrix Files**\n",
    "\n",
    "1. For each count matrix file (gzipped):\n",
    "   - Extracts header line (first row with column names)\n",
    "   - First column is `peak_id` (kept as-is)\n",
    "   - Remaining columns are sample IDs (individualID format)\n",
    "   - Maps sample IDs to sampleid where mapping exists\n",
    "   - Creates new header with mapped IDs\n",
    "   - Replaces original header with new header\n",
    "   - Recompresses with gzip\n",
    "\n",
    "#### Output:\n",
    "Output Directory: `output/1_files_with_sampleid/`\n",
    "\n",
    "Metadata Files (with sampleid):\n",
    "- `metadata_Ast.csv`\n",
    "- `metadata_Ex.csv`\n",
    "- `metadata_In.csv`\n",
    "- `metadata_Microglia.csv`\n",
    "- `metadata_Oligo.csv`\n",
    "- `metadata_OPC.csv`\n",
    "\n",
    "Count Matrix Files (with sampleid headers):\n",
    "- `pseudobulk_peaks_counts_Ast.csv.gz`\n",
    "- `pseudobulk_peaks_counts_Ex.csv.gz`\n",
    "- `pseudobulk_peaks_counts_In.csv.gz`\n",
    "- `pseudobulk_peaks_counts_Microglia.csv.gz`\n",
    "- `pseudobulk_peaks_counts_Oligo.csv.gz`\n",
    "- `pseudobulk_peaks_counts_OPC.csv.gz`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258d410-4973-4c25-b53e-9a2c3399ce28",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb41f4-134f-48dc-b8ce-b347fda8af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(data.table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2a878-1fd0-4be2-ab1a-bcee12e9ebc1",
   "metadata": {},
   "source": [
    "#### Load input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f415cf24-b424-405b-9032-d225f0ed0310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read mapping file, rows: 1200 \n"
     ]
    }
   ],
   "source": [
    "# 3. Read mapping data\n",
    "map_file <- file.path(input_dir, \"data/rosmap_sample_mapping_data.csv\")\n",
    "map <- fread(map_file)\n",
    "cat(\"Read mapping file, rows:\", nrow(map), \"\\n\")\n",
    "\n",
    "# 4. Create mapping dictionary\n",
    "id_map <- map[, .(individualID, sampleid)]\n",
    "setkey(id_map, individualID)\n",
    "\n",
    "# Define cell types and paths\n",
    "celltype <- c(\"Ast\", \"Ex\", \"In\", \"Microglia\", \"Oligo\", \"OPC\")\n",
    "\n",
    "# Your specific metadata file paths\n",
    "metadata_files <- file.path(input_dir, paste0(\"1_files_with_sampleid/metadata_\", celltype, \".csv\"))\n",
    "\n",
    "\n",
    "for (ct in celltype) {\n",
    "  specific_dir <- file.path(output_dir, \"1_files_with_sampleid\")\n",
    "  if (!dir.exists(specific_dir)) {\n",
    "    dir.create(specific_dir, recursive = TRUE)\n",
    "    cat(\"Created directory:\", specific_dir, \"\\n\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dbf62-9db1-4e0b-a689-71eb2f27c98d",
   "metadata": {},
   "source": [
    "### Process metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e8a4ec-45f7-4678-8e43-d7d9246850bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing metadata file: metadata_Ast.csv \n",
      "Original rows: 93 columns: 10 \n",
      "Output file will be saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_Ast.csv \n",
      "Saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_Ast.csv \n",
      "Converted rows: 93 columns: 10 \n",
      "Mapped IDs: 84 Unmapped IDs: 9 \n",
      "\n",
      "Processing metadata file: metadata_Ex.csv \n",
      "Original rows: 92 columns: 10 \n",
      "Output file will be saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_Ex.csv \n",
      "Saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_Ex.csv \n",
      "Converted rows: 92 columns: 10 \n",
      "Mapped IDs: 83 Unmapped IDs: 9 \n",
      "\n",
      "Processing metadata file: metadata_In.csv \n",
      "Original rows: 93 columns: 10 \n",
      "Output file will be saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_In.csv \n",
      "Saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_In.csv \n",
      "Converted rows: 93 columns: 10 \n",
      "Mapped IDs: 84 Unmapped IDs: 9 \n",
      "\n",
      "Processing metadata file: metadata_Microglia.csv \n",
      "Original rows: 93 columns: 10 \n",
      "Output file will be saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_Microglia.csv \n",
      "Saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_Microglia.csv \n",
      "Converted rows: 93 columns: 10 \n",
      "Mapped IDs: 84 Unmapped IDs: 9 \n",
      "\n",
      "Processing metadata file: metadata_Oligo.csv \n",
      "Original rows: 93 columns: 10 \n",
      "Output file will be saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_Oligo.csv \n",
      "Saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_Oligo.csv \n",
      "Converted rows: 93 columns: 10 \n",
      "Mapped IDs: 84 Unmapped IDs: 9 \n",
      "\n",
      "Processing metadata file: metadata_OPC.csv \n",
      "Original rows: 93 columns: 10 \n",
      "Output file will be saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_OPC.csv \n",
      "Saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/metadata_OPC.csv \n",
      "Converted rows: 93 columns: 10 \n",
      "Mapped IDs: 84 Unmapped IDs: 9 \n",
      "\n",
      "Metadata file processing summary:\n",
      "                     file mapped_ids unmapped_ids total_ids\n",
      "                   <char>      <int>        <int>     <int>\n",
      "1:       metadata_Ast.csv         84            9        93\n",
      "2:        metadata_Ex.csv         83            9        92\n",
      "3:        metadata_In.csv         84            9        93\n",
      "4: metadata_Microglia.csv         84            9        93\n",
      "5:     metadata_Oligo.csv         84            9        93\n",
      "6:       metadata_OPC.csv         84            9        93\n"
     ]
    }
   ],
   "source": [
    "# Function to process metadata files - adds sampleid and uses individualID for unmapped cases\n",
    "process_metadata <- function(file_path, celltype_name) {\n",
    "  cat(\"\\nProcessing metadata file:\", basename(file_path), \"\\n\")\n",
    "  \n",
    "  # Read data\n",
    "  meta <- fread(file_path)\n",
    "  cat(\"Original rows:\", nrow(meta), \"columns:\", ncol(meta), \"\\n\")\n",
    "  \n",
    "  # Find the position of individualID column\n",
    "  id_col_index <- which(colnames(meta) == \"individualID\")\n",
    "  if (length(id_col_index) == 0) {\n",
    "    cat(\"Warning: individualID column not found\\n\")\n",
    "    return(NULL)\n",
    "  }\n",
    "  \n",
    "  # Find the mapped sampleids for each individualID\n",
    "  meta$sampleid <- character(nrow(meta))  # Initialize with empty strings\n",
    "  \n",
    "  for (i in 1:nrow(meta)) {\n",
    "    ind_id <- meta$individualID[i]\n",
    "    mapped_id <- id_map[ind_id, sampleid]\n",
    "    \n",
    "    # If mapping found, use it; otherwise use the original individualID\n",
    "    if (length(mapped_id) > 0 && !is.na(mapped_id)) {\n",
    "      meta$sampleid[i] <- mapped_id\n",
    "    } else {\n",
    "      # Use the original individualID instead of NA\n",
    "      meta$sampleid[i] <- ind_id\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Move sampleid column to the front\n",
    "  setcolorder(meta, c(\"sampleid\", setdiff(names(meta), \"sampleid\")))\n",
    "  \n",
    "  # Save results\n",
    "  output_file <- file.path(output_dir, \"1_files_with_sampleid\",basename(file_path))\n",
    "  cat(\"Output file will be saved to:\", output_file, \"\\n\")\n",
    "  fwrite(meta, output_file)\n",
    "  \n",
    "  # Count mapped and unmapped IDs\n",
    "  mapped_count <- sum(meta$sampleid != meta$individualID)\n",
    "  unmapped_count <- sum(meta$sampleid == meta$individualID)\n",
    "  \n",
    "  cat(\"Saved to:\", output_file, \"\\n\")\n",
    "  cat(\"Converted rows:\", nrow(meta), \"columns:\", ncol(meta), \"\\n\")\n",
    "  cat(\"Mapped IDs:\", mapped_count, \"Unmapped IDs:\", unmapped_count, \"\\n\")\n",
    "  \n",
    "  # Return processing summary\n",
    "  list(\n",
    "    file = basename(file_path),\n",
    "    mapped_ids = mapped_count,\n",
    "    unmapped_ids = unmapped_count,\n",
    "    total_ids = nrow(meta)\n",
    "  )\n",
    "}\n",
    "\n",
    "# Process all metadata files\n",
    "meta_results <- mapply(process_metadata, metadata_files, celltype, SIMPLIFY = FALSE)\n",
    "meta_summary <- do.call(rbind, lapply(meta_results, as.data.table))\n",
    "\n",
    "cat(\"\\nMetadata file processing summary:\\n\")\n",
    "print(meta_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752c617-693c-4fd3-b9b4-a21c9326bec8",
   "metadata": {},
   "source": [
    "### Process count matrix files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c39e1c-5913-411d-bb14-749371fe5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing count matrix file: pseudobulk_peaks_counts_Ast.csv.gz \n",
      "Original columns: 93 \n",
      "Executing command: zcat /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ast.csv.gz | tail -n +2 | cat /scratch/3114076.1.casaq/RtmpQcG6rV/file1bc75a5b135eff - | gzip > /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ast.csv.gz \n",
      "Input file size:  -rw-r--r-- 1 jaempawi xqtl 22M Jan 29 12:08 /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ast.csv.gz \n",
      "Output file size: -rw-r--r-- 1 jaempawi xqtl 22M Feb 12 15:32 /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ast.csv.gz \n",
      "File processing completed and saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ast.csv.gz \n",
      "\n",
      "Processing count matrix file: pseudobulk_peaks_counts_Ex.csv.gz \n",
      "Original columns: 92 \n",
      "Executing command: zcat /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ex.csv.gz | tail -n +2 | cat /scratch/3114076.1.casaq/RtmpQcG6rV/file1bc75a1b4f71a - | gzip > /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ex.csv.gz \n",
      "Input file size:  -rw-r--r-- 1 jaempawi xqtl 24M Jan 29 12:08 /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ex.csv.gz \n",
      "Output file size: -rw-r--r-- 1 jaempawi xqtl 24M Feb 12 15:32 /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ex.csv.gz \n",
      "File processing completed and saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Ex.csv.gz \n",
      "\n",
      "Processing count matrix file: pseudobulk_peaks_counts_In.csv.gz \n",
      "Original columns: 93 \n",
      "Executing command: zcat /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_In.csv.gz | tail -n +2 | cat /scratch/3114076.1.casaq/RtmpQcG6rV/file1bc75a24fc9c54 - | gzip > /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_In.csv.gz \n",
      "Input file size:  -rw-r--r-- 1 jaempawi xqtl 24M Jan 29 12:08 /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_In.csv.gz \n",
      "Output file size: -rw-r--r-- 1 jaempawi xqtl 24M Feb 12 15:33 /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_In.csv.gz \n",
      "File processing completed and saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_In.csv.gz \n",
      "\n",
      "Processing count matrix file: pseudobulk_peaks_counts_Microglia.csv.gz \n",
      "Original columns: 93 \n",
      "Executing command: zcat /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Microglia.csv.gz | tail -n +2 | cat /scratch/3114076.1.casaq/RtmpQcG6rV/file1bc75a5e37a1a8 - | gzip > /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Microglia.csv.gz \n",
      "Input file size:  -rw-r--r-- 1 jaempawi xqtl 16M Jan 29 12:08 /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Microglia.csv.gz \n",
      "Output file size: -rw-r--r-- 1 jaempawi xqtl 16M Feb 12 15:33 /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Microglia.csv.gz \n",
      "File processing completed and saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Microglia.csv.gz \n",
      "\n",
      "Processing count matrix file: pseudobulk_peaks_counts_Oligo.csv.gz \n",
      "Original columns: 93 \n",
      "Executing command: zcat /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Oligo.csv.gz | tail -n +2 | cat /scratch/3114076.1.casaq/RtmpQcG6rV/file1bc75a522197c8 - | gzip > /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Oligo.csv.gz \n",
      "Input file size:  -rw-r--r-- 1 jaempawi xqtl 28M Jan 29 12:08 /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Oligo.csv.gz \n",
      "Output file size: -rw-r--r-- 1 jaempawi xqtl 28M Feb 12 15:33 /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Oligo.csv.gz \n",
      "File processing completed and saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_Oligo.csv.gz \n",
      "\n",
      "Processing count matrix file: pseudobulk_peaks_counts_OPC.csv.gz \n",
      "Original columns: 93 \n",
      "Executing command: zcat /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_OPC.csv.gz | tail -n +2 | cat /scratch/3114076.1.casaq/RtmpQcG6rV/file1bc75a68ad457e - | gzip > /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_OPC.csv.gz \n",
      "Input file size:  -rw-r--r-- 1 jaempawi xqtl 17M Jan 29 12:08 /restricted/projectnb/xqtl/jaempawi/atac_seq/atac_seq_data_xiong/1_files_with_sampleid/pseudobulk_peaks_counts_OPC.csv.gz \n",
      "Output file size: -rw-r--r-- 1 jaempawi xqtl 17M Feb 12 15:33 /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_OPC.csv.gz \n",
      "File processing completed and saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/1_files_with_sampleid/pseudobulk_peaks_counts_OPC.csv.gz \n",
      "\n",
      "Count matrix file processing summary:\n",
      "                                       file total_columns mapped_columns\n",
      "                                     <char>         <int>          <num>\n",
      "1:       pseudobulk_peaks_counts_Ast.csv.gz            93              0\n",
      "2:        pseudobulk_peaks_counts_Ex.csv.gz            92              0\n",
      "3:        pseudobulk_peaks_counts_In.csv.gz            93              0\n",
      "4: pseudobulk_peaks_counts_Microglia.csv.gz            93              0\n",
      "5:     pseudobulk_peaks_counts_Oligo.csv.gz            93              0\n",
      "6:       pseudobulk_peaks_counts_OPC.csv.gz            93              0\n",
      "   unmapped_columns\n",
      "              <num>\n",
      "1:               92\n",
      "2:               91\n",
      "3:               92\n",
      "4:               92\n",
      "5:               92\n",
      "6:               92\n",
      "\n",
      "All files processed!\n"
     ]
    }
   ],
   "source": [
    "# Your specific metadata file paths\n",
    "count_files <- file.path(input_dir, paste0(\"1_files_with_sampleid/pseudobulk_peaks_counts_\", celltype, \".csv.gz\"))\n",
    "\n",
    "\n",
    "# Direct column renaming for count matrix files\n",
    "process_counts_simple <- function(file_path) {\n",
    "  cat(\"\\nProcessing count matrix file:\", basename(file_path), \"\\n\")\n",
    "  \n",
    "  # Get header line only\n",
    "  header_command <- paste0(\"zcat \", file_path, \" | head -n 1\")\n",
    "  header_line <- system(header_command, intern = TRUE)\n",
    "  \n",
    "  # Parse column names\n",
    "  col_names <- unlist(strsplit(header_line, \",\"))\n",
    "  cat(\"Original columns:\", length(col_names), \"\\n\")\n",
    "  \n",
    "  # First column is peak_id, remaining columns are sample IDs\n",
    "  peak_id_col <- col_names[1]\n",
    "  sample_cols <- col_names[-1]\n",
    "  \n",
    "  # Map sample IDs\n",
    "  new_sample_cols <- character(length(sample_cols))\n",
    "  mapped_count <- 0\n",
    "  \n",
    "  for (i in seq_along(sample_cols)) {\n",
    "    ind_id <- sample_cols[i]\n",
    "    mapped_id <- id_map[ind_id, sampleid]\n",
    "    \n",
    "    if (length(mapped_id) > 0 && !is.na(mapped_id)) {\n",
    "      new_sample_cols[i] <- mapped_id\n",
    "      mapped_count <- mapped_count + 1\n",
    "    } else {\n",
    "      # Keep original individualID if no mapping found\n",
    "      new_sample_cols[i] <- ind_id\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Create new header\n",
    "  new_col_names <- c(peak_id_col, new_sample_cols)\n",
    "  \n",
    "  # Create temporary header file\n",
    "  temp_header <- tempfile()\n",
    "  writeLines(paste(new_col_names, collapse = \",\"), temp_header)\n",
    "  \n",
    "  # Output file path\n",
    "  output_file <- file.path(output_dir, \"1_files_with_sampleid\", basename(file_path))\n",
    "  \n",
    "  # Use system command to process the file without chunking\n",
    "  # This extracts the data (excluding header), prepends new header, and compresses\n",
    "  cmd <- paste0(\n",
    "    \"zcat \", file_path, \" | tail -n +2 | cat \", temp_header, \" - | gzip > \", output_file\n",
    "  )\n",
    "  \n",
    "  cat(\"Executing command:\", cmd, \"\\n\")\n",
    "  system_result <- system(cmd)\n",
    "  \n",
    "  # Check if command succeeded\n",
    "  if (system_result != 0) {\n",
    "    cat(\"ERROR: Command failed with exit code\", system_result, \"\\n\")\n",
    "    cat(\"Attempting backup method...\\n\")\n",
    "    \n",
    "    # Backup method using R's built-in file handling\n",
    "    tryCatch({\n",
    "      # Create a named vector for mapping\n",
    "      id_mapping <- setNames(new_sample_cols, sample_cols)\n",
    "      \n",
    "      # Open connections\n",
    "      in_conn <- gzfile(file_path, \"r\")\n",
    "      out_conn <- gzfile(output_file, \"w\")\n",
    "      \n",
    "      # Read and discard the header line\n",
    "      readLines(in_conn, n = 1)\n",
    "      \n",
    "      # Write the new header\n",
    "      writeLines(paste(new_col_names, collapse = \",\"), out_conn)\n",
    "      \n",
    "      # Copy the rest of the file line by line\n",
    "      while (length(line <- readLines(in_conn, n = 1)) > 0) {\n",
    "        writeLines(line, out_conn)\n",
    "      }\n",
    "      \n",
    "      # Close connections\n",
    "      close(in_conn)\n",
    "      close(out_conn)\n",
    "      \n",
    "      cat(\"Backup method successful\\n\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"Backup method also failed:\", e$message, \"\\n\")\n",
    "    })\n",
    "  } else {\n",
    "    # Check file sizes to verify completion\n",
    "    input_size <- system(paste(\"ls -lh\", file_path), intern = TRUE)\n",
    "    output_size <- system(paste(\"ls -lh\", output_file), intern = TRUE)\n",
    "    cat(\"Input file size: \", input_size, \"\\n\")\n",
    "    cat(\"Output file size:\", output_size, \"\\n\")\n",
    "  }\n",
    "  \n",
    "  # Delete temporary file\n",
    "  file.remove(temp_header)\n",
    "  \n",
    "  cat(\"File processing completed and saved to:\", output_file, \"\\n\")\n",
    "  \n",
    "  # Return processing summary\n",
    "  list(\n",
    "    file = basename(file_path),\n",
    "    total_columns = length(col_names),\n",
    "    mapped_columns = mapped_count,\n",
    "    unmapped_columns = length(sample_cols) - mapped_count\n",
    "  )\n",
    "}\n",
    "\n",
    "# Process all count files\n",
    "count_results <- lapply(count_files, process_counts_simple)\n",
    "\n",
    "# Summarize results\n",
    "count_summary <- do.call(rbind, lapply(count_results, as.data.table))\n",
    "cat(\"\\nCount matrix file processing summary:\\n\")\n",
    "print(count_summary)\n",
    "\n",
    "cat(\"\\nAll files processed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97c736-2394-46c3-a76c-2d5bb82a1098",
   "metadata": {},
   "source": [
    "## Step 1: Pseudobulk QC noBIOvar\n",
    "**Purpose:** Performs quality control on pseudobulk ATAC-seq data, filters low-quality samples and peaks, normalizes data, and calculates covariate-adjusted residuals while preserving biological variation (sex, age).\n",
    "\n",
    "---\n",
    "\n",
    "#### Input:\n",
    "\n",
    "**From Step 0 (required):**\n",
    "- `metadata_{celltype}.csv` (in `output/1_files_with_sampleid/`)\n",
    "- `pseudobulk_peaks_counts_{celltype}.csv.gz` (in `output/1_files_with_sampleid/`)\n",
    "\n",
    "**Reference Files:**\n",
    "- `rosmap_cov.txt`: Sample covariates (pmi, study)\n",
    "- `hg38-blacklist.v2.bed.gz`: ENCODE blacklist regions\n",
    "\n",
    "**Cell Types:**\n",
    "- `Mic` (Microglia)\n",
    "- `Astro` (Astrocytes)\n",
    "- `Oligo` (Oligodendrocytes)\n",
    "- `Ex` (Excitatory neurons)\n",
    "- `In` (Inhibitory neurons)\n",
    "- `OPC` (Oligodendrocyte precursor cells)\n",
    "\n",
    "#### Process:\n",
    "\n",
    "1. Load Data\n",
    "2. Sample Quality Filtering\n",
    "3. Calculate Technical QC Metrics\n",
    "4. Process Peak Coordinates\n",
    "5. Filter Blacklisted Regions\n",
    "6. Merge Covariates\n",
    "7. Create DGE Object\n",
    "8. Expression Filtering\n",
    "9. Save Filtered Raw Counts\n",
    "10. TMM Normalization\n",
    "11. Handle Batch and Library Variables\n",
    "12. Build Linear Model\n",
    "13. Voom Transformation & Model Fitting\n",
    "14. Calculate Offsets and Residuals\n",
    "\n",
    "#### Output:\n",
    "Output Directory: `output/2_residuals/{celltype}/`\n",
    "\n",
    "1. Residuals File: `{celltype}_residuals.txt`\n",
    "2. Results Object: `{celltype}_results.rds`\n",
    "3. Summary Report: `{celltype}_summary.txt`\n",
    "4. Variable Explanation: `{celltype}_variable_explanation.txt`\n",
    "5. Filtered Raw Counts: `{celltype}_filtered_raw_counts.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef8b2d-64b4-4d49-9845-93a6ee4b8895",
   "metadata": {},
   "source": [
    "#### Load libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85cd96e-2357-41c8-90ab-bd61e14cf22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:data.table’:\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Loading required package: limma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(data.table)\n",
    "library(stringr)\n",
    "library(dplyr)\n",
    "library(edgeR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d194542-2660-46cd-84ab-362cf147a4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing celltype: Oligo \n"
     ]
    }
   ],
   "source": [
    "# Set cell type and create output directory\n",
    "#args <- commandArgs(trailingOnly = TRUE)\n",
    "\n",
    "celltype <- \"Oligo\"\n",
    "cat(\"Processing celltype:\", celltype, \"\\n\")\n",
    "\n",
    "# Create individual directories for each cell type\n",
    "for (ct in celltype) {\n",
    "  specific_dir <- file.path(output_dir, \"2_residuals\",celltype)\n",
    "  if (!dir.exists(specific_dir)) {\n",
    "    dir.create(specific_dir, recursive = TRUE)\n",
    "    cat(\"Created directory:\", specific_dir, \"\\n\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcf867-e949-41a3-8afe-b66f71217ca7",
   "metadata": {},
   "source": [
    "#### Create predictOffset funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a63f3d9-c699-4e97-85f1-456f684e8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictOffset <- function(fit) {\n",
    "  # Define which variables are factors and which are continuous\n",
    "  usedFactors <- c(\"sequencingBatch\", \"Library\", \"study\") \n",
    "  usedContinuous <- c(\"log_n_nuclei\", \"med_nucleosome_signal\", \"med_tss_enrich\", \"log_med_n_tot_fragment\",\n",
    "                      \"log_total_unique_peaks\", \"pmi\")\n",
    "  \n",
    "  # Filter to only use variables actually in the design matrix\n",
    "  usedFactors <- usedFactors[sapply(usedFactors, function(f) any(grepl(paste0(\"^\", f), colnames(fit$design))))]\n",
    "  usedContinuous <- usedContinuous[sapply(usedContinuous, function(f) any(grepl(paste0(\"^\", f), colnames(fit$design))))]\n",
    "  \n",
    "  # Get indices for factor and continuous variables\n",
    "  facInd <- unlist(lapply(as.list(usedFactors), \n",
    "                         function(f) {return(grep(paste0(\"^\", f), \n",
    "                                                colnames(fit$design)))}))\n",
    "  contInd <- unlist(lapply(as.list(usedContinuous), \n",
    "                          function(f) {return(grep(paste0(\"^\", f), \n",
    "                                                 colnames(fit$design)))}))\n",
    "  \n",
    "  # Add the intercept\n",
    "  all_indices <- c(1, facInd, contInd)\n",
    "  \n",
    "  # Verify design matrix structure (using sorted indices to avoid duplication warning)\n",
    "  all_indices_sorted <- sort(unique(all_indices))\n",
    "  stopifnot(all(all_indices_sorted %in% 1:ncol(fit$design)))\n",
    "  \n",
    "  # Create new design matrix with median values\n",
    "  D <- fit$design\n",
    "  D[, facInd] <- 0  # Set all factor levels to reference level\n",
    "  \n",
    "  # For continuous variables, set to median value\n",
    "  if (length(contInd) > 0) {\n",
    "    medContVals <- apply(D[, contInd, drop=FALSE], 2, median)\n",
    "    for (i in 1:length(medContVals)) {\n",
    "      D[, names(medContVals)[i]] <- medContVals[i]\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Calculate offsets\n",
    "  stopifnot(all(colnames(coefficients(fit)) == colnames(D)))\n",
    "  offsets <- apply(coefficients(fit), 1, function(c) {\n",
    "    return(D %*% c)\n",
    "  })\n",
    "  offsets <- t(offsets)\n",
    "  colnames(offsets) <- rownames(fit$design)\n",
    "  \n",
    "  return(offsets)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35928e75-ca48-49ae-be1d-d1429c3171c3",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f3ffb5c-a52d-4f7d-ba1f-14241612be1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata with 93 samples\n",
      "Filtered to 92 samples with > 20 nuclei\n",
      "Loaded peak data with 363775 peaks\n",
      "Valid samples after nuclei filtering: 92 \n",
      "Valid samples present in peak data: 90 \n",
      "Original peak data dimensions: 363775 × 92 \n",
      "Filtered peak data dimensions: 363775 × 90 \n",
      "Final metadata samples after filtering: 90 \n"
     ]
    }
   ],
   "source": [
    "celltype <- \"Oligo\"\n",
    "meta_path <- paste0(output_dir, \"/1_files_with_sampleid/metadata_\", celltype, \".csv\")\n",
    "peak_path <- paste0(output_dir, \"/1_files_with_sampleid/pseudobulk_peaks_counts_\", celltype, \".csv.gz\")\n",
    "\n",
    "# Blacklist and Covariates are in the source 'data_dir'\n",
    "blacklist_file <- file.path(input_dir, \"data/hg38-blacklist.v2.bed.gz\")\n",
    "covariates_file <- file.path(input_dir, \"data/rosmap_cov.txt\")\n",
    "\n",
    "# Load metadata\n",
    "meta <- fread(meta_path)\n",
    "cat(\"Loaded metadata with\", nrow(meta), \"samples\\n\")\n",
    "\n",
    "# Filter samples with n_nuclei > 20\n",
    "meta_filtered <- meta[n.nuclei > 20]\n",
    "cat(\"Filtered to\", nrow(meta_filtered), \"samples with > 20 nuclei\\n\")\n",
    "\n",
    "# Load peak data\n",
    "peak_data <- fread(peak_path)\n",
    "cat(\"Loaded peak data with\", nrow(peak_data), \"peaks\\n\")\n",
    "\n",
    "# Extract peak_id and set as rownames\n",
    "peak_id <- peak_data$peak_id\n",
    "peak_data <- peak_data[, -1, with = FALSE]  # Remove peak_id column\n",
    "\n",
    "# Filter peak data to keep only samples with >20 nuclei\n",
    "valid_samples <- meta_filtered$sampleid\n",
    "cat(\"Valid samples after nuclei filtering:\", length(valid_samples), \"\\n\")\n",
    "\n",
    "# Find which valid samples actually exist in the peak data\n",
    "available_samples <- intersect(valid_samples, colnames(peak_data))\n",
    "cat(\"Valid samples present in peak data:\", length(available_samples), \"\\n\")\n",
    "\n",
    "# Create filtered peak matrix\n",
    "peak_data_filtered <- peak_data[, ..available_samples, with=FALSE]\n",
    "cat(\"Original peak data dimensions:\", nrow(peak_data), \"×\", ncol(peak_data), \"\\n\")\n",
    "cat(\"Filtered peak data dimensions:\", nrow(peak_data_filtered), \"×\", ncol(peak_data_filtered), \"\\n\")\n",
    "\n",
    "# Convert to matrix for downstream analysis\n",
    "peak_matrix <- as.matrix(peak_data_filtered)\n",
    "rownames(peak_matrix) <- peak_id\n",
    "\n",
    "# Update metadata to match filtered samples\n",
    "meta_filtered <- meta_filtered[sampleid %in% available_samples]\n",
    "cat(\"Final metadata samples after filtering:\", nrow(meta_filtered), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228d5e0-b459-421c-a3aa-e5e8a3a0f992",
   "metadata": {},
   "source": [
    "#### Process technical variables from meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92bd6ab0-27e2-4218-8a35-8f826227d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column name normalization (for easier handling)\n",
    "meta_clean <- meta_filtered %>%\n",
    "  rename(\n",
    "    med_nucleosome_signal = med.nucleosome_signal.ct,\n",
    "    med_tss_enrich = med.tss.enrich.ct,\n",
    "    med_n_tot_fragment = med.n_tot_fragment.ct,\n",
    "    n_nuclei = n.nuclei\n",
    "  )\n",
    "\n",
    "# Calculate peak metrics - total unique peaks per sample and median peak width\n",
    "peak_metrics <- data.frame(\n",
    "  sampleid = colnames(peak_matrix),\n",
    "  total_unique_peaks = colSums(peak_matrix > 0)\n",
    ") %>%\n",
    "  mutate(log_total_unique_peaks = log(total_unique_peaks + 1))\n",
    "\n",
    "# Calculate median peak width for each sample using count as weight\n",
    "calculate_median_peakwidth <- function(peak_matrix, peak_info) {\n",
    "  # Create a data frame with peak widths\n",
    "  peak_widths <- peak_info$end - peak_info$start\n",
    "  \n",
    "  # Initialize a vector to store median peak widths\n",
    "  median_peak_widths <- numeric(ncol(peak_matrix))\n",
    "  names(median_peak_widths) <- colnames(peak_matrix)\n",
    "  \n",
    "  # For each sample, calculate the weighted median peak width\n",
    "  for (i in 1:ncol(peak_matrix)) {\n",
    "    sample_counts <- peak_matrix[, i]\n",
    "    # Only consider peaks with counts > 0\n",
    "    idx <- which(sample_counts > 0)\n",
    "    \n",
    "    if (length(idx) > 0) {\n",
    "      # Method 1: Use counts as weights\n",
    "      weights <- sample_counts[idx]\n",
    "      # Repeat each peak width by its count for weighted calculation\n",
    "      all_widths <- rep(peak_widths[idx], times=weights)\n",
    "      median_peak_widths[i] <- median(all_widths)\n",
    "    } else {\n",
    "      median_peak_widths[i] <- NA\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return(median_peak_widths)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3869e-c624-4666-930f-97c6976c74da",
   "metadata": {},
   "source": [
    "#### Process peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b71aebba-d9c9-4836-8432-c3bba27e9864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of peak coordinates:\n",
      "            peak_name    chr  start    end\n",
      "               <char> <char>  <int>  <int>\n",
      "1: chr1-817077-817577   chr1 817077 817577\n",
      "2: chr1-827285-827785   chr1 827285 827785\n",
      "3: chr1-850237-850737   chr1 850237 850737\n",
      "4: chr1-869660-870160   chr1 869660 870160\n",
      "5: chr1-903662-904162   chr1 903662 904162\n",
      "6: chr1-904504-905004   chr1 904504 905004\n",
      "Number of blacklisted peaks: 29 \n",
      "Number of peaks after blacklist filtering: 363746 \n"
     ]
    }
   ],
   "source": [
    "# Process peak coordinates\n",
    "peak_df <- data.table(\n",
    "  peak_name = peak_id,\n",
    "  chr = sapply(strsplit(peak_id, \"-\"), `[`, 1),\n",
    "  start = as.integer(sapply(strsplit(peak_id, \"-\"), `[`, 2)),\n",
    "  end = as.integer(sapply(strsplit(peak_id, \"-\"), `[`, 3)),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "# Verify peak coordinates were extracted correctly\n",
    "cat(\"Sample of peak coordinates:\\n\")\n",
    "print(head(peak_df))\n",
    "\n",
    "if (file.exists(blacklist_file)) {\n",
    "  blacklist_df <- fread(blacklist_file)\n",
    "  if (ncol(blacklist_df) >= 4) {\n",
    "    colnames(blacklist_df)[1:4] <- c(\"chr\", \"start\", \"end\", \"label\")\n",
    "  } else {\n",
    "    colnames(blacklist_df)[1:3] <- c(\"chr\", \"start\", \"end\")\n",
    "  }\n",
    "  \n",
    "  # Filter blacklisted peaks\n",
    "  setkey(blacklist_df, chr, start, end)\n",
    "  setkey(peak_df, chr, start, end)\n",
    "  overlapping_peaks <- foverlaps(peak_df, blacklist_df, nomatch=0)\n",
    "  blacklisted_peaks <- unique(overlapping_peaks$peak_name)\n",
    "  cat(\"Number of blacklisted peaks:\", length(blacklisted_peaks), \"\\n\")\n",
    "  \n",
    "  filtered_peak_idx <- !(peak_id %in% blacklisted_peaks)\n",
    "  filtered_peak <- peak_matrix[filtered_peak_idx, ]\n",
    "  cat(\"Number of peaks after blacklist filtering:\", nrow(filtered_peak), \"\\n\")\n",
    "} else {\n",
    "  cat(\"Warning: Blacklist file not found at\", blacklist_file, \"\\n\")\n",
    "  cat(\"Proceeding without blacklist filtering\\n\")\n",
    "  filtered_peak <- peak_matrix\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabaa13-d809-4538-806d-d3aea0a37858",
   "metadata": {},
   "source": [
    "#### Load covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5caf744-bf0a-4aa9-95bc-601341111872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable statistics before and after log transformation:\n",
      "n_nuclei: min=39.00, median=849.00, max=4394.00, SD=1080.03\n",
      "log_n_nuclei: min=3.66, median=6.74, max=8.39, SD=1.05\n",
      "med_n_tot_fragment: min=1308.50, median=7521.00, max=30629.00, SD=5373.50\n",
      "log_med_n_tot_fragment: min=7.18, median=8.93, max=10.33, SD=0.69\n",
      "Number of samples after joining: 83 \n",
      "Sample IDs: SM-CTECR, SM-CJK5G, SM-CJEKQ, SM-CJGGY, SM-CJK3S, SM-CTEGU ...\n",
      "Available covariates: sampleid, individualID, sequencingBatch, Library, Celltype4, n_nuclei, avg.pct.read.in.peak.ct, med_nucleosome_signal, med_n_tot_fragment, med_tss_enrich, total_unique_peaks, log_total_unique_peaks, pmi, study, log_n_nuclei, log_med_n_tot_fragment \n"
     ]
    }
   ],
   "source": [
    "covariates_file <- file.path(input_dir,'data/rosmap_cov.txt')\n",
    "\n",
    "if (file.exists(covariates_file)) {\n",
    "  covariates <- fread(covariates_file)\n",
    "  # Check column names and adjust if needed\n",
    "  if ('#id' %in% colnames(covariates)) {\n",
    "    id_col <- '#id'\n",
    "  } else if ('individualID' %in% colnames(covariates)) {\n",
    "    id_col <- 'individualID'\n",
    "  } else {\n",
    "    cat(\"Warning: Could not identify ID column in covariates file. Available columns:\", \n",
    "        paste(colnames(covariates), collapse=\", \"), \"\\n\")\n",
    "    id_col <- colnames(covariates)[1]\n",
    "    cat(\"Using\", id_col, \"as ID column\\n\")\n",
    "  }\n",
    "  \n",
    "  # Select relevant columns - excluding msex and age_death\n",
    "  cov_cols <- intersect(c(id_col, 'pmi', 'study'), colnames(covariates))\n",
    "  covariates <- covariates[, ..cov_cols]\n",
    "  \n",
    "  # Merge with metadata\n",
    "  meta_with_ind <- meta_clean %>%\n",
    "    select(sampleid, everything())\n",
    "  \n",
    "  all_covs <- meta_with_ind %>%\n",
    "    inner_join(peak_metrics, by = \"sampleid\") %>%\n",
    "    inner_join(covariates, by = setNames(id_col, \"sampleid\"))\n",
    "  \n",
    "  # Impute missing values\n",
    "  for (col in c(\"pmi\")) {\n",
    "    if (col %in% colnames(all_covs) && any(is.na(all_covs[[col]]))) {\n",
    "      cat(\"Imputing missing values for\", col, \"\\n\")\n",
    "      all_covs[[col]][is.na(all_covs[[col]])] <- median(all_covs[[col]], na.rm=TRUE)\n",
    "    }\n",
    "  }\n",
    "} else {\n",
    "  cat(\"Warning: Covariates file\", covariates_file, \"not found.\\n\")\n",
    "  cat(\"Proceeding with only technical variables.\\n\")\n",
    "  all_covs <- meta_clean %>%\n",
    "    inner_join(peak_metrics, by = \"sampleid\")\n",
    "}\n",
    "\n",
    "\n",
    "# Perform log transformations on necessary variables\n",
    "# Add a small constant to avoid log(0)\n",
    "epsilon <- 1e-6\n",
    "\n",
    "all_covs$log_n_nuclei <- log(all_covs$n_nuclei + epsilon)\n",
    "all_covs$log_med_n_tot_fragment <- log(all_covs$med_n_tot_fragment + epsilon)\n",
    "\n",
    "# Show distribution of original and log-transformed variables\n",
    "cat(\"\\nVariable statistics before and after log transformation:\\n\")\n",
    "for (var in c(\"n_nuclei\", \"med_n_tot_fragment\")) {\n",
    "  orig_var <- all_covs[[var]]\n",
    "  log_var <- all_covs[[paste0(\"log_\", var)]]\n",
    "  \n",
    "  cat(sprintf(\"%s: min=%.2f, median=%.2f, max=%.2f, SD=%.2f\\n\", \n",
    "              var, min(orig_var), median(orig_var), max(orig_var), sd(orig_var)))\n",
    "  cat(sprintf(\"log_%s: min=%.2f, median=%.2f, max=%.2f, SD=%.2f\\n\", \n",
    "              var, min(log_var), median(log_var), max(log_var), sd(log_var)))\n",
    "}\n",
    "\n",
    "cat(\"Number of samples after joining:\", nrow(all_covs), \"\\n\")\n",
    "cat(\"Sample IDs:\", paste(head(all_covs$sampleid), collapse=\", \"), \"...\\n\")\n",
    "cat(\"Available covariates:\", paste(colnames(all_covs), collapse=\", \"), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a0588-5d0d-471e-857f-754b69836303",
   "metadata": {},
   "source": [
    "#### Create DGE object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccdc7318-b28e-4037-ac3a-c7794d4a72ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid samples: 83 \n"
     ]
    }
   ],
   "source": [
    "valid_samples <- intersect(colnames(filtered_peak), all_covs$sampleid)\n",
    "cat(\"Number of valid samples:\", length(valid_samples), \"\\n\")\n",
    "\n",
    "all_covs_filtered <- all_covs[all_covs$sampleid %in% valid_samples, ]\n",
    "filtered_peak_filtered <- filtered_peak[, valid_samples]\n",
    "\n",
    "dge <- DGEList(\n",
    "  counts = filtered_peak_filtered,\n",
    "  samples = all_covs_filtered\n",
    ")\n",
    "rownames(dge$samples) <- dge$samples$sampleid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a6650-bedd-4a93-ba36-fd4f091cbb99",
   "metadata": {},
   "source": [
    "####  Filter low counts and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "630dc838-d78b-445c-846d-91f1fc0bc56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peaks before filtering: 176039 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in filterByExpr.DGEList(dge, min.count = 5, min.total.count = 15, :\n",
      "“All samples appear to belong to the same group.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peaks after filtering: 176039 \n",
      "Saved filtered raw counts to /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/2_residuals/Oligo/Oligo_filtered_raw_counts.txt \n"
     ]
    }
   ],
   "source": [
    "cat(\"Number of peaks before filtering:\", nrow(dge), \"\\n\")\n",
    "keep <- filterByExpr(dge, \n",
    "                   min.count = 5,     # for one sample, min reads \n",
    "                   min.total.count = 15, # min reads overall\n",
    "                   min.prop = 0.1) \n",
    "\n",
    "dge <- dge[keep, , keep.lib.sizes=FALSE]\n",
    "cat(\"Number of peaks after filtering:\", nrow(dge), \"\\n\") #66154 in OPC\n",
    "\n",
    "# Save filtered raw count data\n",
    "filtered_raw_counts <- dge$counts\n",
    "write.table(filtered_raw_counts,\n",
    "            file = paste0(output_dir, \"/2_residuals/\", celltype, \"/\", celltype, \"_filtered_raw_counts.txt\"), \n",
    "            quote=FALSE, sep=\"\\t\", row.names=TRUE, col.names=TRUE)\n",
    "cat(\"Saved filtered raw counts to\", paste0(output_dir, \"/2_residuals/\", celltype, \"/\", celltype, \"_filtered_raw_counts.txt\"), \"\\n\")\n",
    "\n",
    "dge <- calcNormFactors(dge, method=\"TMM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533471bd-7b96-4bd7-b3b2-00694b69507b",
   "metadata": {},
   "source": [
    "#### Handle batch and library as technical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "152eaa2c-8856-4436-a27f-6064bd93dd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sequencingBatch and Library as technical variables\n",
      "Found 2 unique sequencing batches\n",
      "Batch sizes:\n",
      "batches\n",
      "190820Kel 191203Kel \n",
      "        7        76 \n",
      "Found 7 unique libraries\n",
      "Library sizes:\n",
      "libraries\n",
      "Library10 Library11  Library2  Library4  Library5  Library7  Library9 \n",
      "       26         6         7         6         7        23         8 \n"
     ]
    }
   ],
   "source": [
    "# We'll handle batch and library as technical variables rather than doing batch adjustment\n",
    "cat(\"Handling sequencingBatch and Library as technical variables\\n\")\n",
    "\n",
    "# Check batch information\n",
    "batches <- dge$samples$sequencingBatch\n",
    "cat(\"Found\", length(unique(batches)), \"unique sequencing batches\\n\")\n",
    "\n",
    "# Check batch size\n",
    "batch_counts <- table(batches)\n",
    "cat(\"Batch sizes:\\n\")\n",
    "print(batch_counts)\n",
    "\n",
    "# Convert sequencingBatch to factor with at least 2 levels\n",
    "if (length(unique(batches)) < 2) {\n",
    "  cat(\"Only one sequencing batch found. Adding dummy batch for model compatibility.\\n\")\n",
    "  # Create a dummy batch factor to avoid model errors\n",
    "  dge$samples$sequencingBatch_factor <- factor(rep(\"batch1\", ncol(dge)))\n",
    "} else {\n",
    "  # Use the existing batch information\n",
    "  dge$samples$sequencingBatch_factor <- factor(dge$samples$sequencingBatch)\n",
    "}\n",
    "\n",
    "# Check library information\n",
    "libraries <- dge$samples$Library\n",
    "cat(\"Found\", length(unique(libraries)), \"unique libraries\\n\")\n",
    "\n",
    "# Check library size\n",
    "library_counts <- table(libraries)\n",
    "cat(\"Library sizes:\\n\")\n",
    "print(library_counts)\n",
    "\n",
    "# Convert Library to factor with at least 2 levels\n",
    "if (length(unique(libraries)) < 2) {\n",
    "  cat(\"Only one library found. Adding dummy library for model compatibility.\\n\")\n",
    "  # Create a dummy library factor to avoid model errors\n",
    "  dge$samples$Library_factor <- factor(rep(\"lib1\", ncol(dge)))\n",
    "} else {\n",
    "  # Use the existing library information\n",
    "  dge$samples$Library_factor <- factor(dge$samples$Library)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8dda3-89ae-47ad-8785-e393695061dd",
   "metadata": {},
   "source": [
    "####  Create model and run voom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e7f374-b7a5-4666-ac99-191807b7e8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model with technical covariates plus pmi and study\n",
      "Model formula: ~log_n_nuclei + med_nucleosome_signal + med_tss_enrich + log_med_n_tot_fragment +      log_total_unique_peaks + sequencingBatch_factor + Library_factor +      pmi + study \n",
      "Warning: Factor variable group has only one level. Converting to character.\n",
      "Successfully created design matrix with 15 columns\n",
      "Design matrix is not full rank. Adjusting...\n",
      "Adjusted design matrix columns: 14 \n",
      "Calculating offsets and residuals...\n"
     ]
    }
   ],
   "source": [
    "# Define the model based on available covariates - using log-transformed variables\n",
    "# Removed msex and age_death from the model\n",
    "if (\"study\" %in% colnames(dge$samples) && \"pmi\" %in% colnames(dge$samples)) {\n",
    "  # Technical model with pmi and study\n",
    "  cat(\"Using model with technical covariates plus pmi and study\\n\")\n",
    "  model <- ~ log_n_nuclei + med_nucleosome_signal + med_tss_enrich + log_med_n_tot_fragment  +\n",
    "    log_total_unique_peaks  + sequencingBatch_factor + Library_factor + pmi + study\n",
    "} else if (\"pmi\" %in% colnames(dge$samples)) {\n",
    "  # Technical model with pmi only\n",
    "  cat(\"Using model with technical covariates and pmi\\n\")\n",
    "  model <- ~ log_n_nuclei + med_nucleosome_signal + med_tss_enrich + log_med_n_tot_fragment  +\n",
    "    log_total_unique_peaks  + sequencingBatch_factor + Library_factor + pmi\n",
    "} else {\n",
    "  # Technical variables only model\n",
    "  cat(\"Using model with technical covariates only\\n\")\n",
    "  model <- ~ log_n_nuclei + med_nucleosome_signal + med_tss_enrich + log_med_n_tot_fragment  +\n",
    "    log_total_unique_peaks  + sequencingBatch_factor + Library_factor\n",
    "}\n",
    "\n",
    "# Print the model formula\n",
    "cat(\"Model formula:\", deparse(model), \"\\n\")\n",
    "\n",
    "# Check for factor variables with only one level\n",
    "for (col in colnames(dge$samples)) {\n",
    "  if (is.factor(dge$samples[[col]]) && nlevels(dge$samples[[col]]) < 2) {\n",
    "    cat(\"Warning: Factor variable\", col, \"has only one level. Converting to character.\\n\")\n",
    "    dge$samples[[col]] <- as.character(dge$samples[[col]])\n",
    "  }\n",
    "}\n",
    "\n",
    "# Create design matrix with error checking\n",
    "tryCatch({\n",
    "  design <- model.matrix(model, data=dge$samples)\n",
    "  cat(\"Successfully created design matrix with\", ncol(design), \"columns\\n\")\n",
    "}, error = function(e) {\n",
    "  cat(\"Error in creating design matrix:\", e$message, \"\\n\")\n",
    "  cat(\"Attempting to fix model formula...\\n\")\n",
    "  \n",
    "  # Check each term in the model\n",
    "  all_terms <- all.vars(model)\n",
    "  valid_terms <- character(0)\n",
    "  \n",
    "  for (term in all_terms) {\n",
    "    if (term %in% colnames(dge$samples)) {\n",
    "      # Check if it's a factor with at least 2 levels\n",
    "      if (is.factor(dge$samples[[term]])) {\n",
    "        if (nlevels(dge$samples[[term]]) >= 2) {\n",
    "          valid_terms <- c(valid_terms, term)\n",
    "        } else {\n",
    "          cat(\"Skipping factor\", term, \"with only\", nlevels(dge$samples[[term]]), \"level\\n\")\n",
    "        }\n",
    "      } else {\n",
    "        # Non-factor variables are fine\n",
    "        valid_terms <- c(valid_terms, term)\n",
    "      }\n",
    "    } else {\n",
    "      cat(\"Variable\", term, \"not found in sample data\\n\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Create a simplified model with valid terms\n",
    "  if (length(valid_terms) > 0) {\n",
    "    model_str <- paste(\"~\", paste(valid_terms, collapse = \" + \"))\n",
    "    model <- as.formula(model_str)\n",
    "    cat(\"New model formula:\", model_str, \"\\n\")\n",
    "    design <- model.matrix(model, data=dge$samples)\n",
    "    cat(\"Successfully created design matrix with\", ncol(design), \"columns\\n\")\n",
    "  } else {\n",
    "    stop(\"Could not create a valid model with the available variables\")\n",
    "  }\n",
    "})\n",
    "\n",
    "# Check if the design matrix is full rank\n",
    "if (!is.fullrank(design)) {\n",
    "  cat(\"Design matrix is not full rank. Adjusting...\\n\")\n",
    "  # Find and remove the problematic columns\n",
    "  qr_res <- qr(design)\n",
    "  design <- design[, qr_res$pivot[1:qr_res$rank]]\n",
    "  cat(\"Adjusted design matrix columns:\", ncol(design), \"\\n\")\n",
    "}\n",
    "\n",
    "# Run voom and fit model\n",
    "v <- voom(dge, design, plot=FALSE) #logCPM\n",
    "fit <- lmFit(v, design)\n",
    "fit <- eBayes(fit)\n",
    "\n",
    "# Calculate offset and residuals\n",
    "cat(\"Calculating offsets and residuals...\\n\")\n",
    "offset <- predictOffset(fit)\n",
    "resids <- residuals(fit, y=v)\n",
    "\n",
    "# Verify dimensions\n",
    "stopifnot(all(rownames(offset) == rownames(resids)) &\n",
    "          all(colnames(offset) == colnames(resids)))\n",
    "\n",
    "# Final adjusted data\n",
    "stopifnot(all(dim(offset) == dim(resids)))\n",
    "stopifnot(all(colnames(offset) == colnames(resids)))\n",
    "\n",
    "final_data <- offset + resids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac57ec8-7559-4c60-94f4-73d190a2f11a",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f3dbbf0-acdf-4257-8dad-529727dac1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed. Results and documentation saved to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/2_residuals/Oligo/ \n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "saveRDS(list(\n",
    "  dge = dge,\n",
    "  offset = offset,\n",
    "  residuals = resids,\n",
    "  final_data = final_data,\n",
    "  valid_samples = colnames(dge),\n",
    "  design = design,\n",
    "  fit = fit,\n",
    "  model = model\n",
    "), file = paste0(output_dir, \"/2_residuals/\", celltype, \"/\", celltype, \"_results.rds\"))\n",
    "\n",
    "# Write final residual data to file\n",
    "write.table(final_data,\n",
    "            file = paste0(output_dir, \"/2_residuals/\", celltype,  \"/\", celltype, \"_residuals.txt\"), \n",
    "            quote=FALSE, sep=\"\\t\", row.names=TRUE, col.names=TRUE)\n",
    "\n",
    "# Write summary statistics\n",
    "sink(file = paste0(output_dir, \"/2_residuals/\", celltype,  \"/\", celltype, \"_summary.txt\"))\n",
    "cat(\"*** Processing Summary for\", celltype, \"***\\n\\n\")\n",
    "cat(\"Original peak count:\", length(peak_id), \"\\n\")\n",
    "cat(\"Peaks after blacklist filtering:\", nrow(filtered_peak), \"\\n\")\n",
    "cat(\"Peaks after expression filtering:\", nrow(dge), \"\\n\\n\")\n",
    "cat(\"Number of samples:\", ncol(dge), \"\\n\")\n",
    "cat(\"Number of samples after nuclei (>20) filtering:\", ncol(peak_matrix), \"\\n\")\n",
    "cat(\"\\nTechnical Variables Used:\\n\")\n",
    "cat(\"- log_n_nuclei: Log-transformed number of nuclei per sample\\n\")\n",
    "cat(\"- med_nucleosome_signal: Median nucleosome signal\\n\")\n",
    "cat(\"- med_tss_enrich: Median TSS enrichment\\n\")\n",
    "cat(\"- log_med_n_tot_fragment: Log-transformed median number of total fragments\\n\")\n",
    "cat(\"- log_total_unique_peaks: Log-transformed count of unique peaks per sample\\n\")\n",
    "cat(\"- sequencingBatch_factor: Sequencing batch ID\\n\")\n",
    "cat(\"- Library_factor: Library ID\\n\")\n",
    "cat(\"\\nOther Variables Used:\\n\")\n",
    "cat(\"- pmi: Post-mortem interval\\n\")\n",
    "cat(\"- study: Study cohort\\n\")\n",
    "sink()\n",
    "\n",
    "# Write an additional explanation file about the variables and log transformation\n",
    "sink(file = paste0(output_dir, \"/2_residuals/\", celltype,  \"/\", celltype, \"_variable_explanation.txt\"))\n",
    "cat(\"# ATAC-seq Technical Variables Explanation\\n\\n\")\n",
    "\n",
    "\n",
    "cat(\"## Why Log Transformation?\\n\")\n",
    "cat(\"Log transformation is applied to certain variables for several reasons:\\n\")\n",
    "cat(\"1. To make the distribution more symmetric and closer to normal\\n\")\n",
    "cat(\"2. To stabilize variance across the range of values\\n\")\n",
    "cat(\"3. To match the scale of voom-transformed peak counts, which are on log2-CPM scale\\n\")\n",
    "cat(\"4. To be consistent with the approach used in related studies like haQTL\\n\\n\")\n",
    "\n",
    "cat(\"## Variables and Their Meanings\\n\\n\")\n",
    "\n",
    "cat(\"### Technical Variables\\n\")\n",
    "cat(\"- n_nuclei: Number of nuclei that contributed to this pseudobulk sample\\n\")\n",
    "cat(\"  * Filtered to include only samples with >20 nuclei\\n\")\n",
    "cat(\"  * Log-transformed because count data typically has a right-skewed distribution\\n\\n\")\n",
    "\n",
    "cat(\"- med_n_tot_fragment: Median number of total fragments per cell\\n\")\n",
    "cat(\"  * Represents sequencing depth\\n\")\n",
    "cat(\"  * Log-transformed because sequencing depth typically has exponential effects\\n\\n\")\n",
    "\n",
    "cat(\"- total_unique_peaks: Number of unique peaks detected in each sample\\n\")\n",
    "cat(\"  * Log-transformed similar to 'TotalNumPeaks' in haQTL pipeline\\n\\n\")\n",
    "\n",
    "cat(\"- med_nucleosome_signal: Median nucleosome signal\\n\")\n",
    "cat(\"  * Measures the degree of nucleosome positioning\\n\")\n",
    "cat(\"  * Not log-transformed as it's already a ratio/normalized metric\\n\\n\")\n",
    "\n",
    "cat(\"- med_tss_enrich: Median transcription start site enrichment score\\n\")\n",
    "cat(\"  * Indicates the quality of the ATAC-seq data\\n\")\n",
    "cat(\"  * Not log-transformed as it's already a ratio/normalized metric\\n\\n\")\n",
    "\n",
    "\n",
    "cat(\"- sequencingBatch: Batch ID for the sequencing run\\n\")\n",
    "cat(\"  * Treated as a factor to account for batch effects\\n\\n\")\n",
    "\n",
    "cat(\"- Library: Library preparation batch ID\\n\")\n",
    "cat(\"  * Treated as a factor to account for library preparation effects\\n\\n\")\n",
    "\n",
    "cat(\"### Other Variables\\n\")\n",
    "cat(\"- pmi: Post-mortem interval (time between death and tissue collection)\\n\")\n",
    "cat(\"- study: Study cohort (ROSMAP, MAP, ROS)\\n\\n\")\n",
    "\n",
    "cat(\"## Relationship to voom Transformation\\n\")\n",
    "cat(\"The voom transformation converts count data to log2-CPM (counts per million) values \")\n",
    "cat(\"and estimates the mean-variance relationship. By log-transforming certain technical \")\n",
    "cat(\"covariates, we ensure they're on a similar scale to the transformed expression data, \")\n",
    "cat(\"which can improve the fit of the linear model used for removing unwanted variation.\\n\")\n",
    "sink()\n",
    "\n",
    "cat(\"Processing completed. Results and documentation saved to:\", paste0(output_dir, \"/2_residuals/\", celltype,  \"/\"), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f20f0-2d2e-4674-9894-15434681504d",
   "metadata": {},
   "source": [
    "## Step 2: Phenotype Reformat\n",
    "**Purpose:** Converts covariate-adjusted residuals from Step 1 into genome-wide BED format suitable for QTL mapping tools (FastQTL, TensorQTL, MatrixEQTL).\n",
    "\n",
    "---\n",
    "\n",
    "#### Input:\n",
    "\n",
    "**From Step 1 (required):**\n",
    "- `{celltype}_residuals.txt` (in `output/2_residuals/{celltype}/`)\n",
    "\n",
    "**Cell Types:**\n",
    "- `Mic` (Microglia)\n",
    "- `Astro` (Astrocytes)\n",
    "- `Oligo` (Oligodendrocytes)\n",
    "- `Ex` (Excitatory neurons)\n",
    "- `In` (Inhibitory neurons)\n",
    "- `OPC` (Oligodendrocyte precursor cells)\n",
    "\n",
    "\n",
    "#### Process:\n",
    "\n",
    "1. Set Cell Type and Paths\n",
    "2. Load residuals file\n",
    "3. Extract and parse peak IDs\n",
    "4. Convert to Midpoint Coordinates\n",
    "5. Create BED format\n",
    "6. Sort by genomic position\n",
    "7. Write BED file\n",
    "8. Compress with bgzip\n",
    "\n",
    "#### Output:\n",
    "Output Directory: `output/3_phenotype_reformatting/{celltype}/`\n",
    "\n",
    "Output File: `{celltype}_kellis_xiong_snatac_phenotype.bed.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55814e99-5baf-4a24-b185-7ecfd2327ed8",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "858a632a-3ba8-4791-a2d5-b92110dc8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(data.table)\n",
    "library(stringr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf1b630b-20f7-43f2-973a-28dbee1acc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names from first line: SM-CTECR, SM-CJK5G, SM-CJEKQ, SM-CJGGY, SM-CJK3S, SM-CTEGU ...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env Rscript\n",
    "\n",
    "# Script to reformat ATAC-seq residuals into BED format and compress with bgzip\n",
    "# Usage: Rscript reformat_residuals.R [celltype]\n",
    "\n",
    "# Get command line arguments\n",
    "#args <- commandArgs(trailingOnly = TRUE)\n",
    "#if (length(args) < 1) {\n",
    "#  celltype <- \"Ex\"  # Default cell type\n",
    "#  cat(\"No cell type specified, using default:\", celltype, \"\\n\")\n",
    "#} else {\n",
    "#  celltype <- args[1]\n",
    "#  cat(\"Processing cell type:\", celltype, \"\\n\")\n",
    "#}\n",
    "\n",
    "# Define input and output paths\n",
    "#input_dir <- \"/home/al4225/project/kellis_snatac/output/xiong/2_residuals\"\n",
    "#output_dir <- \"/home/al4225/project/kellis_snatac/output/3_phenotype_processing\"\n",
    "pheno_reformat_output_dir <- paste0(output_dir, \"/3_phenotype_reformatting/\", celltype)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "dir.create(pheno_reformat_output_dir, recursive = TRUE, showWarnings = FALSE)\n",
    "\n",
    "# Check if input directory exists\n",
    "celltype_dir <- paste0(output_dir,\"/2_residuals/\", celltype)\n",
    "if (!dir.exists(celltype_dir)) {\n",
    "  cat(\"Cell type directory not found:\", celltype_dir, \"\\n\")\n",
    "  cat(\"Using backup directory...\\n\")\n",
    "  celltype_dir <- file.path(output_dir,paste0(\"2_residuals/backup/\", celltype))\n",
    "  if (!dir.exists(celltype_dir)) {\n",
    "    dir.create(celltype_dir, recursive = TRUE)\n",
    "    stop(\"Backup directory not found either: \", celltype_dir)\n",
    "  }\n",
    "}\n",
    "\n",
    "input_file <- file.path(celltype_dir, paste0(celltype, \"_residuals.txt\"))\n",
    "output_bed <- file.path(output_dir, paste0(\"3_phenotype_reformatting/\",celltype ,\"/\", celltype,\"_kellis_xiong_snatac_phenotype.bed\"))\n",
    "\n",
    "# Check if input file exists\n",
    "if (!file.exists(input_file)) {\n",
    "  stop(\"Input file not found: \", input_file)\n",
    "}\n",
    "\n",
    "# Read the first line manually to get the column names\n",
    "first_line <- readLines(input_file, n = 1)\n",
    "col_names <- unlist(strsplit(first_line, split = \"\\t\"))\n",
    "cat(\"Column names from first line:\", paste(head(col_names), collapse = \", \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282f3a2-650f-4a61-abd1-5038b324cfea",
   "metadata": {},
   "source": [
    "#### Load input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95e4ee18-4411-4bd6-9b33-6bc426d9742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading residuals file: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/2_residuals/Oligo/Oligo_residuals.txt \n"
     ]
    }
   ],
   "source": [
    "cat(\"Reading residuals file:\", input_file, \"\\n\")\n",
    "first_line <- readLines(input_file, n = 1)\n",
    "col_names  <- unlist(strsplit(first_line, split = \"\\t\"))\n",
    "\n",
    "residuals <- fread(input_file, header = FALSE, skip = 1)\n",
    "\n",
    "# Logic to handle row names/peak IDs\n",
    "if (ncol(residuals) > length(col_names)) {\n",
    "  peak_ids  <- residuals[[1]]\n",
    "  residuals <- residuals[, -1, with = FALSE]\n",
    "  setnames(residuals, col_names)\n",
    "} else {\n",
    "  peak_ids  <- residuals[[1]]\n",
    "  residuals <- residuals[, -1, with = FALSE]\n",
    "  setnames(residuals, col_names[-1]) # Adjusting for leading empty/ID column\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe44c41-8fde-4107-80fa-de5823e3f0ab",
   "metadata": {},
   "source": [
    "#### Coordinate Parsing (BED format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34037cc5-ad0e-48c7-b528-f67ecbc0bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing peak IDs into BED format with midpoint coordinates\n"
     ]
    }
   ],
   "source": [
    "cat(\"Parsing peak IDs into BED format with midpoint coordinates\\n\")\n",
    "parts <- strsplit(peak_ids, \"-\")\n",
    "chrs  <- sapply(parts, `[`, 1)\n",
    "starts_raw <- as.numeric(sapply(parts, `[`, 2))\n",
    "ends_raw   <- as.numeric(sapply(parts, `[`, 3))\n",
    "\n",
    "# Calculate midpoints for a 1bp window (Standard for QTLtools)\n",
    "# This centers the peak signal on a single genomic coordinate\n",
    "mids <- as.integer((starts_raw + ends_raw) / 2)\n",
    "\n",
    "parsed_peaks <- data.table(\n",
    "  '#chr' = chrs,\n",
    "  start = mids,\n",
    "  end   = mids + 1,\n",
    "  ID    = peak_ids\n",
    ")\n",
    "\n",
    "# Combine and Sort\n",
    "bed_data <- cbind(parsed_peaks, residuals)\n",
    "setorder(bed_data, '#chr', start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39221488-f744-402e-97c6-ef6f98c310e6",
   "metadata": {},
   "source": [
    "#### Save and compress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e09883c1-9d6e-4447-ae27-e3d668c33ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing BED file to: /restricted/projectnb/xqtl/jaempawi/atac_seq/output/xiong/3_phenotype_reformatting/Oligo/Oligo_kellis_xiong_snatac_phenotype.bed \n",
      "Compressing with bgzip...\n",
      "Process completed for Oligo \n"
     ]
    }
   ],
   "source": [
    "cat(\"Writing BED file to:\", output_bed, \"\\n\")\n",
    "fwrite(bed_data, output_bed, sep = \"\\t\", col.names = TRUE, quote = FALSE)\n",
    "\n",
    "cat(\"Compressing with bgzip...\\n\")\n",
    "system(paste(\"bgzip -f\", output_bed))\n",
    "\n",
    "# Highly recommended: Index for tabix\n",
    "system(paste(\"tabix -p bed\", paste0(output_bed, \".gz\")))\n",
    "\n",
    "cat(\"Process completed for\", celltype, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
