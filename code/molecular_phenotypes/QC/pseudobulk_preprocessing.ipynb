{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a44eb1-acb9-40f5-bcdb-7ede63d5db5e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Single-nuclei Pseudobulk Preprocessing (RNA-seq and ATAC-seq) Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline preprocesses single-nuclei pseudobulk count data (snATAC-seq or snRNA-seq)\n",
    "for downstream QTL analysis and region-specific studies.\n",
    "\n",
    "**Goals:**\n",
    "- Transform raw pseudobulk counts into analysis-ready formats\n",
    "- Remove technical confounders while preserving biological covariates (sex, age)\n",
    "- Generate QTL-ready phenotype files or region-specific datasets\n",
    "\n",
    "## Pipeline Structure\n",
    "\n",
    "```\n",
    "Step 0: Sample ID Mapping        [sampleid_mapping]\n",
    "         ↓\n",
    "Step 1: Pseudobulk QC            [pseudobulk_qc]\n",
    "         noBIOvar: regress out technical covariates only\n",
    "         (msex and age_death deliberately preserved)\n",
    "         ↓ (optional)\n",
    "         Batch Correction (ComBat-seq or limma::removeBatchEffect)\n",
    "         ↓ (optional)\n",
    "         Quantile Normalization\n",
    "         ↓\n",
    "Step 2: Format Output\n",
    "         ├── Phenotype Reformatting → BED    [phenotype_formatting]  (genome-wide QTL mapping, snATAC-seq only, locus-specific)\n",
    "         └── Region Peak Filtering  → TSV    [region_filtering]  (gene filtering for snRNA-seq)\n",
    "```\n",
    "\n",
    "## Modality Support\n",
    "\n",
    "| Feature | snATAC-seq | snRNA-seq |\n",
    "|---------|-----------|-----------|\n",
    "| Count file auto-detected | ✓ | ✓ |\n",
    "| Default `tech_vars` | `log_n_nuclei`, `med_nucleosome_signal`, `med_tss_enrich`, `log_med_n_tot_fragment`, `log_total_unique_peaks` | custom via `--tech_vars` |\n",
    "| Blacklist filtering | ✓ | — |\n",
    "| `region_filtering` step | ✓ | — |\n",
    "| `phenotype_formatting` step | ✓ | ✓ |\n",
    "\n",
    "For snRNA-seq, override `tech_vars` to match your metadata columns, e.g.:\n",
    "```bash\n",
    "--tech_vars log_n_nuclei percent_mito log_n_genes\n",
    "```\n",
    "\n",
    "Any `tech_var` starting with `log_` is automatically derived via `log1p()` from the\n",
    "raw column of the same name with `log_` stripped. No code changes needed across modalities.\n",
    "\n",
    "## Input Files\n",
    "\n",
    "All input files required to run this pipeline can be downloaded\n",
    "[here](https://drive.google.com/drive/folders/1UzJuHN8SotMn-PJTBp9uGShD25YxapKr?usp=drive_link).\n",
    "\n",
    "| File | Used in |\n",
    "|------|---------|\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` *(snATAC-seq)* | Step 0, Step 1 |\n",
    "| `pseudobulk_counts_{celltype}.csv.gz` *(snRNA-seq)* | Step 0, Step 1 |\n",
    "| `metadata_{celltype}.csv` | Step 0, Step 1 |\n",
    "| `rosmap_sample_mapping_data.csv` | Step 0 |\n",
    "| `rosmap_cov.txt` | Step 1 |\n",
    "| `hg38-blacklist.v2.bed.gz` | Step 1 (snATAC-seq only) |\n",
    "\n",
    "Count files are **auto-detected** from `input_dir` — no prefix parameter needed.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "### `sampleid_mapping`\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `map_file` | *required* | CSV with `individualID` → `sampleid` mapping |\n",
    "| `input_dir` | *required* | Directory with raw metadata and count files |\n",
    "| `output_dir` | *required* | Parent output directory; writes to `{output_dir}/1_files_with_sampleid/` |\n",
    "| `celltype` | `['Ast','Ex','In','Microglia','Oligo','OPC']` | Cell types to process |\n",
    "| `suffix` | `''` | Optional filename suffix (e.g. `_50nuc`) |\n",
    "\n",
    "### `pseudobulk_qc`\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `input_dir` | *required* | Directory with remapped metadata and count files |\n",
    "| `output_dir` | *required* | Parent output directory; writes to `{output_dir}/2_residuals/{ct}/` |\n",
    "| `covariates_file` | *required* | Covariate file with `pmi` and `study` columns |\n",
    "| `blacklist_file` | `''` | Genomic blacklist BED file (snATAC-seq only) |\n",
    "| `sample_list` | `''` | Optional file with one sample ID per line to subset |\n",
    "| `tech_vars` | `['log_n_nuclei','med_nucleosome_signal','med_tss_enrich','log_med_n_tot_fragment','log_total_unique_peaks']` | Technical covariates for the model |\n",
    "| `batch_correction` | `FALSE` | Apply batch correction (`TRUE`/`FALSE`) |\n",
    "| `batch_method` | `limma` | Batch correction method (`limma` or `combat`) |\n",
    "| `quant_norm` | `FALSE` | Apply quantile normalization after residuals |\n",
    "| `min_count` | `5` | Min reads in at least one sample |\n",
    "| `min_total_count` | `15` | Min total reads across all samples |\n",
    "| `min_prop` | `0.1` | Min proportion of samples with expression |\n",
    "| `min_nuclei` | `20` | Min nuclei per sample |\n",
    "| `celltype` | `['Ast','Ex','In','Microglia','Oligo','OPC']` | Cell types to process |\n",
    "| `suffix` | `''` | Optional filename suffix |\n",
    "\n",
    "### `phenotype_formatting`\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `input_dir` | *required* | Directory containing `{ct}/{ct}_residuals.txt` |\n",
    "| `output_dir` | *required* | Parent output directory; writes to `{output_dir}/3_pheno_reformat/` |\n",
    "| `modality` | `snatac` | Modality label used in output filename (`snatac` or `snrna`) |\n",
    "| `celltype` | `['Ast','Ex','In','Mic','Oligo','OPC']` | Cell types to process |\n",
    "\n",
    "### `region_filtering` *(snATAC-seq only)*\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `input_dir` | *required* | Directory containing `{ct}/{ct}_filtered_raw_counts.txt` |\n",
    "| `output_dir` | *required* | Parent output directory; writes to `{output_dir}/3_region_filter/` |\n",
    "| `regions` | `chr7:28000000-28300000,...` | Comma-separated genomic regions of interest |\n",
    "| `celltype` | `['Ast','Ex','In','Mic','Oligo','OPC']` | Cell types to process |\n",
    "\n",
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e13a3f-ab64-4bd1-b47c-acca8d58a8b9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 0: Sample ID Mapping\n",
    "\n",
    "Maps original sample identifiers (`individualID`) to standardized sample IDs (`sampleid`)\n",
    "across metadata and count matrix files.\n",
    "\n",
    "### Input\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `rosmap_sample_mapping_data.csv` | Mapping reference: `individualID → sampleid` |\n",
    "| `metadata_{celltype}.csv` × 6 | Per-cell-type sample metadata |\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` × 6 *(snATAC-seq)* | Per-cell-type peak count matrices |\n",
    "| `pseudobulk_counts_{celltype}.csv.gz` × 6 *(snRNA-seq)* | Per-cell-type gene count matrices |\n",
    "\n",
    "Cell types: `Ast`, `Ex`, `In`, `Microglia`, `Oligo`, `OPC`\n",
    "\n",
    "Count files are **auto-detected** from `input_dir` — any `.csv.gz` file ending with\n",
    "`{celltype}{suffix}` will be found regardless of prefix (`pseudobulk_peaks_counts_`,\n",
    "`pseudobulk_counts_`, etc.).\n",
    "\n",
    "### Process\n",
    "\n",
    "**Part 1 — Metadata files**\n",
    "\n",
    "For each `metadata_{celltype}.csv`:\n",
    "1. Look up each `individualID` in the mapping reference\n",
    "2. Assign `sampleid` — falls back to `individualID` if no mapping found\n",
    "3. Reorder columns: `sampleid` first, then `individualID`, then the rest\n",
    "4. Save updated file\n",
    "\n",
    "**Part 2 — Count matrix files**\n",
    "\n",
    "For each count file detected in `input_dir`:\n",
    "1. Auto-detect filename by scanning for `.csv.gz` files matching `{celltype}{suffix}`\n",
    "2. Extract the header row (column names only)\n",
    "3. Keep the first column (peak or gene IDs) unchanged\n",
    "4. Map remaining column names (`individualID` → `sampleid`) where mapping exists,\n",
    "   otherwise keep original\n",
    "5. Write new header and stream data rows unchanged\n",
    "6. Recompress with gzip\n",
    "\n",
    "### Output\n",
    "\n",
    "Output directory: `{output_dir}/1_files_with_sampleid/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `metadata_{celltype}.csv` × 6 | Metadata with `sampleid` column prepended |\n",
    "| `{detected_count_filename}` × 6 | Count matrices with mapped column headers |\n",
    "\n",
    "**Timing:** < 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7cbe0-bf5e-4d7e-8a2b-216915dea78e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/pseudobulk_preprocessing.ipynb sampleid_mapping \\\n",
    "    --map-file data/atac_seq/rosmap_sample_mapping_data.csv \\\n",
    "    --input-dir data/atac_seq/1_files_with_sampleid \\\n",
    "    --output-dir output/atac_seq \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540a4da-843a-4789-8123-47911cf519c5",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 1: Pseudobulk QC\n",
    "\n",
    "Regresses out technical covariates while preserving biological variation (sex, age) for\n",
    "downstream QTL analysis. Works for both snATAC-seq and snRNA-seq.\n",
    "\n",
    "### Input\n",
    "\n",
    "| File | Location |\n",
    "|------|----------|\n",
    "| `pseudobulk_*counts_{celltype}.csv.gz` *(auto-detected)* | `1_files_with_sampleid/` |\n",
    "| `metadata_{celltype}.csv` | `1_files_with_sampleid/` |\n",
    "| `rosmap_cov.txt` | `data/` |\n",
    "| `hg38-blacklist.v2.bed.gz` *(snATAC-seq, optional)* | `data/` |\n",
    "\n",
    "### Process\n",
    "\n",
    "1. Load metadata per cell type; auto-detect and load count matrix from `input_dir`\n",
    "2. Standardize metadata column names across datasets\n",
    "3. Filter samples with fewer than `min_nuclei` nuclei (default: 20)\n",
    "4. *(Optional)* Subset to samples listed in `sample_list` file\n",
    "5. Align samples between metadata and count matrix\n",
    "6. *(Optional)* Filter blacklisted genomic regions (`blacklist_file`)\n",
    "7. Merge with demographic covariates (`pmi`, `study`) from `covariates_file`\n",
    "8. Impute missing `pmi` values with median\n",
    "9. Load `tech_vars` from parameter — any variable prefixed with `log_` is automatically\n",
    "   derived via `log1p()` from the raw column of the same name:\n",
    "   - e.g. `log_n_nuclei` ← `log1p(n_nuclei)`\n",
    "   - e.g. `log_total_unique_peaks` ← `log1p(colSums(counts > 0))`\n",
    "   - Works for both snATAC-seq and snRNA-seq without code changes\n",
    "10. Build model variable list — `msex` and `age_death` are **deliberately excluded**\n",
    "11. Drop samples with NA in any model variable\n",
    "12. Apply expression filtering (`filterByExpr`):\n",
    "    - `min_count = 5`: minimum reads in at least one sample\n",
    "    - `min_total_count = 15`: minimum total reads across all samples\n",
    "    - `min_prop = 0.1`: feature expressed in ≥10% of samples\n",
    "13. TMM normalization\n",
    "14. *(Optional)* Batch correction (`sequencingBatch` and/or `Library`):\n",
    "    - `limma::removeBatchEffect` (default)\n",
    "    - `ComBat-seq`\n",
    "15. Add `sequencingBatch` and `Library` to model if multi-level\n",
    "16. Fit linear model (`voom` + `lmFit` + `eBayes`)\n",
    "\n",
    "**Model formula (default snATAC-seq):**\n",
    "```\n",
    "~ log_n_nuclei + med_nucleosome_signal + med_tss_enrich +\n",
    "  log_med_n_tot_fragment + log_total_unique_peaks +\n",
    "  [sequencingBatch] + [Library] + pmi + study\n",
    "```\n",
    "\n",
    "> `sequencingBatch` and `Library` are included only if present in metadata and have\n",
    "> more than one level. If batch correction was applied, they are removed from the model.\n",
    "\n",
    "17. Compute `offset + residuals` as final adjusted values:\n",
    "    - `offset`: predicted value at median/reference covariate levels\n",
    "    - `residuals`: unexplained variation after removing all covariate effects\n",
    "18. *(Optional)* Quantile normalization of final values\n",
    "19. Save outputs\n",
    "\n",
    "### Output\n",
    "\n",
    "Output directory: `{output_dir}/2_residuals/{celltype}/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_residuals.txt` | Covariate-adjusted values (log2-CPM) |\n",
    "| `{celltype}_results.rds` | Full results: DGEList, fit, offset, residuals, design, parameters |\n",
    "| `{celltype}_filtered_raw_counts.txt` | Filtered raw counts before normalization |\n",
    "\n",
    "**Variables deliberately NOT regressed out:**\n",
    "- Sex (`msex`)\n",
    "- Age at death (`age_death`)\n",
    "\n",
    "**Timing:** < 5 min per cell type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cbd39c-60f8-4e21-9915-14b30ebc02cd",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Pseudobulk QC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741ac2e-91b3-49e7-906a-2fd6b8d8d137",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# snATAC-seq\n",
    "sos run pipeline/pseudobulk_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --input-dir output/atac_seq/1_files_with_sampleid \\\n",
    "    --output-dir output/atac_seq \\\n",
    "    --blacklist-file data/atac_seq/hg38-blacklist.v2.bed.gz \\\n",
    "    --covariates-file data/atac_seq/rosmap_cov.txt \\\n",
    "    --batch-correction FALSE \\\n",
    "    --min-count 5 \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC\n",
    "\n",
    "# snRNA-seq\n",
    "sos run pipeline/pseudobulk_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --input-dir output/snrna_seq/1_files_with_sampleid \\\n",
    "    --output-dir output/snrna_seq \\\n",
    "    --covariates-file data/snrna_seq/covariates.txt \\\n",
    "    --min-count 5 \\\n",
    "    --batch-correction FALSE \\\n",
    "    --quant-norm TRUE \\      # add this if you want quantile normalized output\n",
    "    --celltype Ast Ex In Microglia Oligo OPC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e96ad2-1b75-43d0-978e-0757bc11f135",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Batch Correction (Optional)\n",
    "\n",
    "Runs between TMM normalization (step 15) and model fitting (step 18).\n",
    "Use when batch effects are severe (e.g., visible batch clusters in PCA, multiple sequencing runs).\n",
    "\n",
    "> When batch correction is applied, `sequencingBatch` and `Library` are **removed** from\n",
    "> the model formula since their variance has already been removed from the counts.\n",
    "\n",
    "**Method comparison:**\n",
    "\n",
    "| | ComBat-seq | limma `removeBatchEffect` |\n",
    "|---|---|---|\n",
    "| **Operates on** | Raw integer counts | log-CPM values |\n",
    "| **Mean-variance modelling** | Yes | No |\n",
    "| **Best for** | Large, balanced batches | Small or fragmented batches |\n",
    "| **Robustness** | May fail with many small batches | More robust to unbalanced designs |\n",
    "\n",
    "**ComBat-seq:**\n",
    "```r\n",
    "dge$counts <- ComBat_seq(as.matrix(dge$counts), batch = batches)\n",
    "```\n",
    "\n",
    "**limma `removeBatchEffect`:**\n",
    "```r\n",
    "logCPM     <- cpm(dge, log = TRUE, prior.count = 1)\n",
    "logCPM     <- removeBatchEffect(logCPM, batch = factor(batches))\n",
    "dge$counts <- round(pmax(2^logCPM, 0))\n",
    "```\n",
    "\n",
    "**Additional filtering applied before correction:**\n",
    "- Singleton batches (only 1 sample in a batch) are removed prior to correction\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `batch_correction` | `FALSE` | Enable batch correction |\n",
    "| `batch_method` | `limma` | Method to use (`limma` or `combat`) |\n",
    "\n",
    "**Command:**\n",
    "```bash\n",
    "sos run pipeline/pseudobulk_preprocessing.ipynb pseudobulk_qc \\\n",
    "    ... \\\n",
    "    --batch_correction TRUE \\\n",
    "    --batch_method limma\n",
    "```\n",
    "\n",
    "**Effect on RDS output:**\n",
    "\n",
    "The `{celltype}_results.rds` file will include:\n",
    "- `batch_correction = TRUE`\n",
    "- `batch_method = \"limma\"` or `\"combat\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad900d-768d-45ee-815a-6847e8eba32e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Pseudobulk QC with batch correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0faa-4dd9-431d-a5cf-3e92d7256a3b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/pseudobulk_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --input-dir output/atac_seq/1_files_with_sampleid \\\n",
    "    --output-dir output/atac_seq \\\n",
    "    --blacklist-file data/atac_seq/hg38-blacklist.v2.bed.gz \\\n",
    "    --covariates-file data/atac_seq/rosmap_cov.txt \\\n",
    "    --batch-correction TRUE \\\n",
    "    --batch-method limma \\\n",
    "    --min-count 5 \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ea976-ca30-48e4-811b-eaa0b5f246ed",
   "metadata": {},
   "source": [
    "### Additional parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b41a7f-1d08-4174-858b-a0593aaadcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All available pseudobulk_qc parameters with defaults\n",
    "--min-count 5\n",
    "--min-total-count 15\n",
    "--min-prop 0.1\n",
    "--min-nuclei 20\n",
    "--sample-list '' # path to file with one sample ID per line\n",
    "--tech-vars log_n_nuclei med_nucleosome_signal med_tss_enrich log_med_n_tot_fragment log_total_unique_peaks# snATAC-seq defaults; for snRNA-seq use e.g.: log_n_nuclei percent_mito log_n_genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d5b04-ec21-4c0c-8879-d78563d5ed96",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 2: Format Output\n",
    "\n",
    "### Phenotype Reformatting (exclusively for snATAC-seq)\n",
    "\n",
    "Converts residuals into a QTL-ready BED format for genome-wide QTL mapping.\n",
    "Works for both snATAC-seq and snRNA-seq.\n",
    "\n",
    "**Input:**\n",
    "\n",
    "| File | Location |\n",
    "|------|----------|\n",
    "| `{celltype}_residuals.txt` | `{output_dir}/2_residuals/{celltype}/` |\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Read residuals file with proper handling of feature IDs and sample columns\n",
    "2. Parse peak coordinates from peak IDs (`chr-start-end` format)\n",
    "3. Convert to midpoint coordinates (standard for QTLtools):\n",
    "```\n",
    "start = floor((peak_start + peak_end) / 2)\n",
    "end   = start + 1\n",
    "```\n",
    "4. Build BED format: `#chr`, `start`, `end`, `ID` followed by per-sample values\n",
    "5. Sort by chromosome and position\n",
    "6. Compress with `bgzip` and index with `tabix`\n",
    "\n",
    "**Output:** `{output_dir}/3_pheno_reformat/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_{modality}_phenotype.bed.gz` | bgzip-compressed BED with midpoint coordinates |\n",
    "| `{celltype}_{modality}_phenotype.bed.gz.tbi` | tabix index for random-access queries |\n",
    "\n",
    "**Use case:** Standard QTL mapping to identify genetic variants affecting chromatin\n",
    "accessibility (caQTL) or gene expression (eQTL), with biological variation preserved.\n",
    "Compatible with FastQTL, TensorQTL, and QTLtools.\n",
    "\n",
    "**Timing:** < 1 min per cell type\n",
    "\n",
    "**Note** For snRNA-seq, please follow this [pipeline](https://github.com/StatFunGen/xqtl-protocol/blob/main/code/data_preprocessing/phenotype/phenotype_formatting.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf9d56-b53b-4a6a-b0af-b5a5fb98907b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/pseudobulk_preprocessing.ipynb phenotype_formatting \\\n",
    "    --input-dir output/atac_seq/2_residuals \\\n",
    "    --output-dir output/atac_seq \\\n",
    "    --celltype Ast Ex In Mic Oligo OPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c874b17-9a77-4e7d-a0a3-3605f7005148",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Region Filtering\n",
    "\n",
    "Filters peak counts to specific genomic regions of interest for locus-specific analysis.\n",
    "\n",
    "**Input:**\n",
    "\n",
    "| File | Location |\n",
    "|------|----------|\n",
    "| `{celltype}_filtered_raw_counts.txt` | `{output_dir}/2_residuals/{celltype}/` |\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Read filtered raw counts per cell type\n",
    "2. Parse peak coordinates from peak IDs (`chr-start-end` format)\n",
    "3. Calculate per-peak metrics:\n",
    "   - `peakwidth`: `end - start`\n",
    "   - `midpoint`: `(start + end) / 2`\n",
    "4. Filter peaks overlapping any target region — includes peaks that start, end, or span region boundaries\n",
    "5. Calculate summary statistics per peak:\n",
    "   - `total_count`: sum of counts across all samples\n",
    "   - `weighted_count`: `total_count / peakwidth` (normalizes for peak size)\n",
    "\n",
    "**Output:** `{output_dir}/3_region_filter/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_filtered_regions_of_interest.txt` | Full count matrix for peaks in target regions |\n",
    "| `{celltype}_filtered_regions_of_interest_summary.txt` | Peak metadata with coordinates and count statistics |\n",
    "\n",
    "**Use case:** Hypothesis-driven analysis of specific genomic loci (e.g., AD risk loci such as\n",
    "the APOE or TREM2 regions) where biological variation is preserved for downstream interpretation.\n",
    "\n",
    "**Timing:** < 1 min per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944afdd-fffc-4b56-863f-eee89408cfa1",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#snATAC-seq \n",
    "sos run pipeline/pseudobulk_preprocessing.ipynb region_filtering \\\n",
    "    --input-dir output/atac_seq/2_residuals \\\n",
    "    --output-dir output/atac_seq \\\n",
    "    --celltype Ast Ex In Mic Oligo OPC \\\n",
    "    --regions \"chr7:28000000-28300000,chr11:85050000-86200000\"\n",
    "\n",
    "#snRNA-seq\n",
    "sos run pipeline/pseudobulk_preprocessing.ipynb region_filtering \\\n",
    "    --input-dir output/snrna_seq/2_residuals \\\n",
    "    --output-dir output/snrna_seq \\\n",
    "    --celltype MIC \\\n",
    "    --gene-list \"ENSG00000000010\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a676801-6845-4ca5-944b-7978a5ecbb1f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486664a9-55c2-4738-91a0-b63ffdcd6cfa",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/pseudobulk_preprocessing.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17a301-cca9-49a1-843b-4248546f1f79",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57fe47-f2ca-4a6e-8789-f7dbe3a9fad2",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "parameter: job_size = 1\n",
    "parameter: walltime = \"5h\"\n",
    "parameter: mem = \"16G\"\n",
    "parameter: numThreads = 8\n",
    "parameter: container = \"\"\n",
    "\n",
    "import re\n",
    "from sos.utils import expand_size\n",
    "\n",
    "entrypoint = (\n",
    "    'micromamba run -a \"\" -n' + ' ' +\n",
    "    re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])\n",
    ") if container else \"\"\n",
    "\n",
    "cwd = path(f'{cwd:a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee58015-c8e2-4697-bdae-58d7e494640d",
   "metadata": {},
   "source": [
    "```\n",
    "  usage: sos run pipeline/pseudobulk_preprocessing.ipynb\n",
    "               [workflow_name | -t targets] [options] [workflow_options]\n",
    "  workflow_name:        Single or combined workflows defined in this script\n",
    "  targets:              One or more targets to generate\n",
    "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
    "  workflow_options:     Double-hyphen workflow-specific parameters\n",
    "Workflows:\n",
    "  sampleid_mapping\n",
    "  pseudobulk_qc\n",
    "  phenotype_formatting\n",
    "  region_filtering\n",
    "Global Workflow Options:\n",
    "  --cwd output (as path)\n",
    "  --job-size 1 (as int)\n",
    "  --walltime 5h\n",
    "  --mem 16G\n",
    "  --numThreads 8 (as int)\n",
    "  --container ''\n",
    "Sections\n",
    "  sampleid_mapping:\n",
    "    Workflow Options:\n",
    "      --map-file VAL (as str, required)\n",
    "      --input-dir VAL (as str, required)\n",
    "      --output-dir VAL (as str, required)\n",
    "      --celltype Ast Ex In Microglia Oligo OPC (as list)\n",
    "      --suffix ''\n",
    "  pseudobulk_qc:\n",
    "    Workflow Options:\n",
    "      --celltype Ast Ex In Microglia Oligo OPC (as list)\n",
    "      --input-dir VAL (as str, required)\n",
    "      --output-dir VAL (as str, required)\n",
    "      --covariates-file VAL (as str, required)\n",
    "      --blacklist-file ''\n",
    "      --sample-list ''\n",
    "      --tech-vars log_n_nuclei med_nucleosome_signal med_tss_enrich log_med_n_tot_fragment log_total_unique_peaks (as list)\n",
    "      --batch-correction FALSE\n",
    "      --batch-method limma\n",
    "      --quant-norm FALSE\n",
    "      --min-count 5 (as int)\n",
    "      --min-total-count 15 (as int)\n",
    "      --min-prop 0.1 (as float)\n",
    "      --min-nuclei 20 (as int)\n",
    "      --suffix ''\n",
    "  phenotype_formatting:\n",
    "    Workflow Options:\n",
    "      --celltype Ast Ex In Mic Oligo OPC (as list)\n",
    "      --input-dir VAL (as str, required)\n",
    "      --output-dir VAL (as str, required)\n",
    "  region_filtering:\n",
    "    Workflow Options:\n",
    "      --celltype Ast Ex In Mic Oligo OPC (as list)\n",
    "                        Parameters\n",
    "      --input-dir VAL (as str, required)\n",
    "      --output-dir VAL (as str, required)\n",
    "      --regions ''\n",
    "      --gene-list ''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6024cd-28be-4fb0-994e-0460e3a3beae",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `sampleid_mapping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1b7c0-2819-45d1-b2ce-8d117a6cc9eb",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sampleid_mapping]\n",
    "parameter: map_file   = str\n",
    "parameter: input_dir  = str\n",
    "parameter: output_dir = str\n",
    "parameter: celltype   = ['Ast', 'Ex', 'In', 'Microglia', 'Oligo', 'OPC']\n",
    "parameter: suffix     = ''\n",
    "\n",
    "input:  [f'{input_dir}/metadata_{ct}{suffix}.csv' for ct in celltype]\n",
    "output: [f'{output_dir}/1_files_with_sampleid/metadata_{ct}{suffix}.csv' for ct in celltype]\n",
    "\n",
    "python: expand = \"${ }\"\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "map_df = pd.read_csv(\"${map_file}\")\n",
    "id_map = dict(zip(map_df[\"individualID\"], map_df[\"sampleid\"]))\n",
    "\n",
    "celltype   = ${celltype}\n",
    "input_dir  = \"${input_dir}\"\n",
    "output_dir = \"${output_dir}/1_files_with_sampleid\"\n",
    "suffix     = \"${suffix}\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def map_id(ind_id):\n",
    "    return id_map.get(ind_id, ind_id)\n",
    "\n",
    "def format_value(val):\n",
    "    if pd.isna(val):\n",
    "        return ''\n",
    "    if isinstance(val, (int, np.integer)):\n",
    "        return str(val)\n",
    "    if isinstance(val, (float, np.floating)):\n",
    "        if val == int(val):\n",
    "            return str(int(val))\n",
    "        else:\n",
    "            return str(val)\n",
    "    return str(val)\n",
    "\n",
    "def find_count_file(input_dir, ct, suffix):\n",
    "    candidates = [\n",
    "        f for f in os.listdir(input_dir)\n",
    "        if f.endswith(f\"{ct}{suffix}.csv.gz\") or f.endswith(f\"_{ct}{suffix}.csv.gz\")\n",
    "    ]\n",
    "    if not candidates:\n",
    "        return None, None\n",
    "    preferred = [f for f in candidates if f.endswith(f\"_{ct}{suffix}.csv.gz\")]\n",
    "    fname = preferred[0] if preferred else candidates[0]\n",
    "    return os.path.join(input_dir, fname), fname\n",
    "\n",
    "# ── Process metadata ───────────────────────────────────────────────────────\n",
    "for ct in celltype:\n",
    "    fname    = f\"metadata_{ct}{suffix}.csv\"\n",
    "    in_path  = os.path.join(input_dir, fname)\n",
    "    out_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    if not os.path.exists(in_path):\n",
    "        print(f\"Skipping metadata (not found): {fname}\")\n",
    "        continue\n",
    "\n",
    "    meta = pd.read_csv(in_path)\n",
    "\n",
    "    if \"individualID\" not in meta.columns:\n",
    "        print(f\"Warning: individualID column not found in {fname}\")\n",
    "        continue\n",
    "\n",
    "    meta[\"sampleid\"] = meta[\"individualID\"].map(map_id)\n",
    "    cols = meta.columns.tolist()\n",
    "    cols.remove(\"sampleid\")\n",
    "    cols.remove(\"individualID\")\n",
    "    meta = meta[[\"sampleid\", \"individualID\"] + cols]\n",
    "\n",
    "    with open(out_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(meta.columns)\n",
    "        for _, row in meta.iterrows():\n",
    "            writer.writerow([format_value(val) for val in row])\n",
    "\n",
    "    print(f\"Processed metadata: {fname}\")\n",
    "\n",
    "# ── Process count files ────────────────────────────────────────────────────\n",
    "for ct in celltype:\n",
    "    in_path, fname = find_count_file(input_dir, ct, suffix)\n",
    "\n",
    "    if in_path is None:\n",
    "        print(f\"Skipping counts (not found) for celltype: {ct}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Detected count file: {fname}\")\n",
    "    out_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    with gzip.open(in_path, \"rt\") as fh:\n",
    "        header_line = fh.readline().rstrip(\"\\n\")\n",
    "\n",
    "    col_names       = header_line.split(\",\")\n",
    "    peak_id_col     = col_names[0]\n",
    "    new_sample_cols = [map_id(s) for s in col_names[1:]]\n",
    "    new_header      = \",\".join([peak_id_col] + new_sample_cols)\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')\n",
    "    tmp.write(new_header + \"\\n\")\n",
    "    tmp.close()\n",
    "\n",
    "    cmd = f\"zcat {in_path} | tail -n +2 | cat {tmp.name} - | gzip -6 > {out_path}\"\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "    os.unlink(tmp.name)\n",
    "\n",
    "    print(f\"Processed counts: {fname}\")\n",
    "\n",
    "print(\"\\nSample ID mapping completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0884ae7-a851-425a-86dd-b606768a012e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `pseudobulk_qc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46328b-c3d8-46f8-8c71-bad27820438e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[pseudobulk_qc]\n",
    "parameter: celltype         = ['Ast','Ex','In','Microglia','Oligo','OPC']\n",
    "parameter: input_dir        = str\n",
    "parameter: output_dir       = str\n",
    "parameter: covariates_file  = str\n",
    "parameter: blacklist_file   = ''\n",
    "parameter: sample_list      = ''\n",
    "parameter: tech_vars        = ['log_n_nuclei','med_nucleosome_signal','med_tss_enrich','log_med_n_tot_fragment','log_total_unique_peaks']\n",
    "parameter: batch_correction = \"FALSE\"\n",
    "parameter: batch_method     = \"limma\"\n",
    "parameter: quant_norm       = \"FALSE\"\n",
    "parameter: min_count        = 5\n",
    "parameter: min_total_count  = 15\n",
    "parameter: min_prop         = 0.1\n",
    "parameter: min_nuclei       = 20\n",
    "parameter: suffix           = ''\n",
    "\n",
    "input:  [f'{input_dir}/metadata_{ct}{suffix}.csv' for ct in celltype]\n",
    "output: [f'{output_dir}/2_residuals/{ct}/{ct}_residuals.txt' for ct in celltype]\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '6:00:00', mem = '64G', cores = 4\n",
    "\n",
    "cts_str = \"c(\" + \", \".join([f\"'{x}'\" for x in celltype]) + \")\"\n",
    "tvs_str = \"c(\" + \", \".join([f\"'{x}'\" for x in tech_vars]) + \")\"\n",
    "\n",
    "R: expand = \"${ }\", stdout = f'{_output[0]:n}.stdout', stderr = f'{_output[0]:n}.stderr'\n",
    "\n",
    "    library(edgeR)\n",
    "    library(limma)\n",
    "    library(data.table)\n",
    "    library(GenomicRanges)\n",
    "    if (as.logical(\"${batch_correction}\") && \"${batch_method}\" == \"combat\") library(sva)\n",
    "\n",
    "    rename_if_found <- function(dt, target, candidates) {\n",
    "        found <- intersect(candidates, colnames(dt))[1]\n",
    "        if (!is.na(found) && found != target) setnames(dt, found, target)\n",
    "    }\n",
    "\n",
    "    standardize_meta <- function(meta) {\n",
    "        rename_if_found(meta, \"n_nuclei\",              c(\"n.nuclei\",\"nNuclei\",\"nuclei_count\"))\n",
    "        rename_if_found(meta, \"med_nucleosome_signal\", c(\"med.nucleosome_signal.ct\",\"NucleosomeRatio\",\"med_nucleosome_signal.ct\"))\n",
    "        rename_if_found(meta, \"med_tss_enrich\",        c(\"med.tss.enrich.ct\",\"TSSEnrichment\",\"med_tss_enrich.ct\"))\n",
    "        rename_if_found(meta, \"med_n_tot_fragment\",    c(\"med.n_tot_fragment.ct\",\"med_n_tot_fragment.ct\"))\n",
    "        return(meta)\n",
    "    }\n",
    "\n",
    "    find_count_file <- function(input_dir, ct, suffix) {\n",
    "        all_files  <- list.files(input_dir, pattern=\"\\\\.csv\\\\.gz$\", full.names=FALSE)\n",
    "        pattern    <- paste0(ct, suffix, \"\\\\.csv\\\\.gz$\")\n",
    "        candidates <- all_files[grepl(pattern, all_files)]\n",
    "        if (length(candidates) == 0) return(NULL)\n",
    "        preferred <- candidates[grepl(paste0(\"_\", ct, suffix, \"\\\\.csv\\\\.gz$\"), candidates)]\n",
    "        if (length(preferred) > 0) return(file.path(input_dir, preferred[1]))\n",
    "        return(file.path(input_dir, candidates[1]))\n",
    "    }\n",
    "\n",
    "    filter_blacklist <- function(mat, bed, feat_label) {\n",
    "        peaks <- data.table(id = rownames(mat))\n",
    "        peaks[, c(\"chr\",\"start\",\"end\") := tstrsplit(gsub(\"_\",\"-\",id), \"-\")]\n",
    "        peaks[, `:=`(start = as.numeric(start), end = as.numeric(end))]\n",
    "        bl <- fread(bed)[, 1:3]\n",
    "        setnames(bl, c(\"chr\",\"start\",\"end\"))\n",
    "        bl[, `:=`(start = as.numeric(start), end = as.numeric(end))]\n",
    "        gr1 <- GRanges(peaks$chr, IRanges(peaks$start, peaks$end))\n",
    "        gr2 <- GRanges(bl$chr,    IRanges(bl$start,    bl$end))\n",
    "        blacklisted <- unique(queryHits(findOverlaps(gr1, gr2)))\n",
    "        if (length(blacklisted) > 0) {\n",
    "            message(\"Blacklisted \", feat_label, \" removed: \", length(blacklisted))\n",
    "            return(mat[-blacklisted, , drop=FALSE])\n",
    "        }\n",
    "        return(mat)\n",
    "    }\n",
    "\n",
    "    predictOffset <- function(fit) {\n",
    "        D  <- fit$design\n",
    "        Dm <- D\n",
    "        for (col in colnames(D)) {\n",
    "            if (col == \"(Intercept)\") next\n",
    "            if (is.numeric(D[, col]) && !all(D[, col] %in% c(0, 1)))\n",
    "                Dm[, col] <- median(D[, col], na.rm=TRUE)\n",
    "            else\n",
    "                Dm[, col] <- 0\n",
    "        }\n",
    "        B <- fit$coefficients\n",
    "        B[is.na(B)] <- 0\n",
    "        B %*% t(Dm)\n",
    "    }\n",
    "\n",
    "    cts       <- ${cts_str}\n",
    "    tech_vars <- ${tvs_str}\n",
    "\n",
    "    for (ct in cts) {\n",
    "        message(\"\\n\", paste(rep(\"=\", 40), collapse=\"\"))\n",
    "        message(\"Processing: \", ct)\n",
    "        message(\"Batch correction: \", ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\"))\n",
    "        message(\"Quantile normalization: \", ifelse(as.logical(\"${quant_norm}\"), \"TRUE\", \"FALSE\"))\n",
    "        message(paste(rep(\"=\", 40), collapse=\"\"))\n",
    "\n",
    "        outdir <- file.path(\"${output_dir}/2_residuals\", ct)\n",
    "        dir.create(outdir, recursive=TRUE, showWarnings=FALSE)\n",
    "\n",
    "        # ── 1. Load data ───────────────────────────────────────────────────\n",
    "        meta <- fread(sprintf(\"${input_dir}/metadata_%s${suffix}.csv\", ct))\n",
    "\n",
    "        counts_file <- find_count_file(\"${input_dir}\", ct, \"${suffix}\")\n",
    "        if (is.null(counts_file)) stop(\"No count file found for celltype: \", ct)\n",
    "        message(\"Detected count file: \", basename(counts_file))\n",
    "\n",
    "        counts_raw <- fread(counts_file)\n",
    "        counts <- as.matrix(counts_raw[, -1, with=FALSE])\n",
    "        rownames(counts) <- counts_raw[[1]]\n",
    "        rm(counts_raw)\n",
    "\n",
    "        # ── Auto-detect modality ───────────────────────────────────────────\n",
    "        is_atac    <- grepl(\"^chr.*-[0-9]+-[0-9]+$\", rownames(counts)[1])\n",
    "        feat_label <- ifelse(is_atac, \"peaks\", \"genes\")\n",
    "        message(\"Detected modality: \", ifelse(is_atac, \"snATAC-seq\", \"snRNA-seq\"))\n",
    "        message(\"Loaded: \", nrow(counts), \" \", feat_label, \" x \", ncol(counts), \" samples\")\n",
    "\n",
    "        # ── 2. Standardize metadata ────────────────────────────────────────\n",
    "        meta <- standardize_meta(meta)\n",
    "\n",
    "        # ── 3. Sample ID column ───────────────────────────────────────────\n",
    "        idcol <- intersect(c(\"sampleid\",\"sampleID\",\"individualID\",\"projid\"), colnames(meta))[1]\n",
    "        if (is.na(idcol)) stop(\"Cannot find sample ID column in metadata.\")\n",
    "\n",
    "        # ── 4. Nuclei filter ──────────────────────────────────────────────\n",
    "        if (\"n_nuclei\" %in% colnames(meta)) {\n",
    "            meta <- meta[meta$n_nuclei > ${min_nuclei}]\n",
    "            message(\"Samples after nuclei (>${min_nuclei}) filter: \", nrow(meta))\n",
    "        }\n",
    "\n",
    "        # ── 5. Optional sample list filter ────────────────────────────────\n",
    "        if (\"${sample_list}\" != \"\" && file.exists(\"${sample_list}\")) {\n",
    "            keep_ids <- fread(\"${sample_list}\", header=FALSE)[[1]]\n",
    "            meta     <- meta[meta[[idcol]] %in% keep_ids]\n",
    "            message(\"Samples after sample_list filter: \", nrow(meta))\n",
    "        }\n",
    "\n",
    "        # ── 6. Align samples ──────────────────────────────────────────────\n",
    "        common <- intersect(meta[[idcol]], colnames(counts))\n",
    "        if (length(common) == 0) stop(\"Zero sample overlap between metadata and count matrix.\")\n",
    "        meta   <- meta[match(common, meta[[idcol]])]\n",
    "        counts <- counts[, common, drop=FALSE]\n",
    "        message(\"Samples after alignment: \", length(common))\n",
    "\n",
    "        # ── 7. Blacklist filtering ─────────────────────────────────────────\n",
    "        if (\"${blacklist_file}\" != \"\" && file.exists(\"${blacklist_file}\")) {\n",
    "            counts <- filter_blacklist(counts, \"${blacklist_file}\", feat_label)\n",
    "            message(feat_label, \" after blacklist filter: \", nrow(counts))\n",
    "        } else {\n",
    "            message(\"No blacklist file provided - skipping.\")\n",
    "        }\n",
    "\n",
    "        # ── 8. Load and merge covariates ──────────────────────────────────\n",
    "        covs      <- fread(\"${covariates_file}\")\n",
    "        id2       <- intersect(c(\"#id\",\"id\",\"projid\",\"individualID\"), colnames(covs))[1]\n",
    "        keep_cols <- c(id2, intersect(c(\"pmi\",\"study\"), colnames(covs)))\n",
    "        covs      <- covs[, ..keep_cols]\n",
    "        meta      <- merge(meta, covs, by.x=idcol, by.y=id2, all.x=TRUE)\n",
    "        meta      <- meta[match(common, meta[[idcol]])]\n",
    "\n",
    "        # ── 9. Impute missing PMI ─────────────────────────────────────────\n",
    "        if (\"pmi\" %in% colnames(meta) && any(is.na(meta$pmi))) {\n",
    "            message(\"Imputing missing values for: pmi\")\n",
    "            meta$pmi[is.na(meta$pmi)] <- median(meta$pmi, na.rm=TRUE)\n",
    "        }\n",
    "\n",
    "        # ── 10. Tech vars ─────────────────────────────────────────────────\n",
    "        message(\"Tech vars: \", paste(tech_vars, collapse=\", \"))\n",
    "\n",
    "        # ── 11. Compute derived log metrics ───────────────────────────────\n",
    "        for (tv in tech_vars[startsWith(tech_vars, \"log_\")]) {\n",
    "            if (tv %in% colnames(meta)) next\n",
    "            if (tv == \"log_total_unique_peaks\") {\n",
    "                meta$log_total_unique_peaks <- log1p(colSums(counts > 0))\n",
    "            } else {\n",
    "                raw_col <- sub(\"^log_\", \"\", tv)\n",
    "                if (raw_col %in% colnames(meta)) {\n",
    "                    meta[[tv]] <- log1p(meta[[raw_col]])\n",
    "                } else {\n",
    "                    message(\"Warning: cannot compute \", tv, \" - '\", raw_col, \"' not in metadata\")\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # ── 12. Select model variables ────────────────────────────────────\n",
    "        all_vars <- c(intersect(tech_vars, colnames(meta)), \"pmi\", \"study\")\n",
    "        all_vars <- intersect(all_vars, colnames(meta))\n",
    "        message(\"Model terms: \", paste(all_vars, collapse=\", \"))\n",
    "\n",
    "        # ── 13. Drop samples with NA in model variables ───────────────────\n",
    "        keep_rows <- complete.cases(meta[, ..all_vars])\n",
    "        meta      <- meta[keep_rows]\n",
    "        counts    <- counts[, meta[[idcol]], drop=FALSE]\n",
    "        message(\"Valid samples for modelling: \", nrow(meta))\n",
    "\n",
    "        # ── 14. Expression filtering ──────────────────────────────────────\n",
    "        dge <- DGEList(counts=counts, samples=meta)\n",
    "        dge$samples$group <- factor(rep(\"all\", ncol(dge)))\n",
    "        message(feat_label, \" before expression filter: \", nrow(dge))\n",
    "\n",
    "        keep <- filterByExpr(dge, group=dge$samples$group,\n",
    "                             min.count=${min_count},\n",
    "                             min.total.count=${min_total_count},\n",
    "                             min.prop=${min_prop})\n",
    "        dge <- dge[keep,, keep.lib.sizes=FALSE]\n",
    "        message(feat_label, \" after expression filter: \", nrow(dge))\n",
    "\n",
    "        # ── Save filtered raw counts ──────────────────────────────────────\n",
    "        write.table(dge$counts,\n",
    "                    file.path(outdir, paste0(ct, \"_filtered_raw_counts.txt\")),\n",
    "                    sep=\"\\t\", quote=FALSE, col.names=NA)\n",
    "\n",
    "        # ── 15. TMM normalization ─────────────────────────────────────────\n",
    "        dge <- calcNormFactors(dge, method=\"TMM\")\n",
    "\n",
    "        # ── 16. Optional batch correction ─────────────────────────────────\n",
    "        if (as.logical(\"${batch_correction}\") && \"sequencingBatch\" %in% colnames(dge$samples)) {\n",
    "            batches       <- dge$samples$sequencingBatch\n",
    "            batch_counts  <- table(batches)\n",
    "            valid_batches <- names(batch_counts[batch_counts > 1])\n",
    "            keep_bc       <- batches %in% valid_batches\n",
    "            dge           <- dge[, keep_bc, keep.lib.sizes=FALSE]\n",
    "            batches       <- batches[keep_bc]\n",
    "            message(\"Samples after singleton batch removal: \", ncol(dge))\n",
    "\n",
    "            if (\"${batch_method}\" == \"combat\") {\n",
    "                dge$counts <- ComBat_seq(as.matrix(dge$counts), batch=batches)\n",
    "                message(\"ComBat-seq batch correction applied.\")\n",
    "            } else {\n",
    "                logCPM     <- cpm(dge, log=TRUE, prior.count=1)\n",
    "                logCPM     <- removeBatchEffect(logCPM, batch=factor(batches))\n",
    "                dge$counts <- round(pmax(2^logCPM, 0))\n",
    "                message(\"limma removeBatchEffect applied.\")\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # ── 17. Add batch vars to model if multi-level ────────────────────\n",
    "        other_vars <- setdiff(all_vars, tech_vars)\n",
    "        batch_vars <- c()\n",
    "        if (\"sequencingBatch\" %in% colnames(dge$samples) &&\n",
    "            length(unique(dge$samples$sequencingBatch)) > 1) {\n",
    "            dge$samples$sequencingBatch_factor <- factor(dge$samples$sequencingBatch)\n",
    "            batch_vars <- c(batch_vars, \"sequencingBatch_factor\")\n",
    "        }\n",
    "        if (\"Library\" %in% colnames(dge$samples) &&\n",
    "            length(unique(dge$samples$Library)) > 1) {\n",
    "            dge$samples$Library_factor <- factor(dge$samples$Library)\n",
    "            batch_vars <- c(batch_vars, \"Library_factor\")\n",
    "        }\n",
    "        all_vars <- intersect(c(tech_vars, batch_vars, other_vars),\n",
    "                              c(colnames(dge$samples), colnames(meta)))\n",
    "\n",
    "        # ── 18. Build design matrix ───────────────────────────────────────\n",
    "        form   <- as.formula(paste(\"~\", paste(all_vars, collapse=\" + \")))\n",
    "        design <- model.matrix(form, data=dge$samples)\n",
    "        message(\"Formula: \", deparse(form))\n",
    "\n",
    "        if (!is.fullrank(design)) {\n",
    "            message(\"Design not full rank - trimming.\")\n",
    "            qr_d   <- qr(design)\n",
    "            design <- design[, qr_d$pivot[seq_len(qr_d$rank)], drop=FALSE]\n",
    "        }\n",
    "        message(\"Design matrix: \", nrow(design), \" x \", ncol(design))\n",
    "\n",
    "        # ── 19. Voom + lmFit + eBayes ────────────────────────────────────\n",
    "        v   <- voom(dge, design, plot=FALSE)\n",
    "        fit <- lmFit(v, design)\n",
    "        fit <- eBayes(fit)\n",
    "\n",
    "        # ── 20. Offset + residuals ────────────────────────────────────────\n",
    "        off   <- predictOffset(fit)\n",
    "        res   <- residuals(fit, v$E)\n",
    "        final <- off + res\n",
    "\n",
    "        # ── 21. Save residuals ────────────────────────────────────────────\n",
    "        out_file <- file.path(outdir, paste0(ct, \"_residuals.txt\"))\n",
    "\n",
    "        write.table(final,\n",
    "            out_file,\n",
    "            sep=\"\\t\", quote=FALSE, col.names=NA)\n",
    "\n",
    "        feat_label <- if (is_atac) \"Peaks\" else \"Genes\"\n",
    "\n",
    "        message(\"Saved: \", out_file)\n",
    "        message(\"  \", feat_label, \": \", nrow(final), \" | Samples: \", ncol(final))\n",
    "\n",
    "        # ── 22. Optional Quantile Normalization ───────────────────────────\n",
    "        if (as.logical(\"${quant_norm}\")) {\n",
    "            message(\"\\n\", paste(rep(\"=\", 40), collapse=\"\"))\n",
    "            message(\"Applying quantile normalization...\")\n",
    "            message(paste(rep(\"=\", 40), collapse=\"\"))\n",
    "            \n",
    "            final_qn <- t(apply(final, 1, rank, ties.method = \"average\"))\n",
    "            final_qn <- stats::qnorm(final_qn / (ncol(final_qn) + 1))\n",
    "            \n",
    "            qn_file <- file.path(outdir, paste0(ct, \"_residuals_qn.txt\"))\n",
    "            write.table(final_qn,\n",
    "                qn_file,\n",
    "                sep=\"\\t\", quote=FALSE, col.names=NA)\n",
    "            \n",
    "            message(\"Saved QN: \", qn_file)\n",
    "            message(\"  \", feat_label, \": \", nrow(final_qn), \" | Samples: \", ncol(final_qn))\n",
    "            \n",
    "            # Save RDS with QN\n",
    "            saveRDS(list(\n",
    "                dge              = dge,\n",
    "                offset           = off,\n",
    "                residuals        = res,\n",
    "                final_data       = final,\n",
    "                final_data_qn    = final_qn,\n",
    "                valid_samples    = colnames(dge),\n",
    "                design           = design,\n",
    "                fit              = fit,\n",
    "                model            = form,\n",
    "                mode             = \"noBIOvar\",\n",
    "                batch_correction = as.logical(\"${batch_correction}\"),\n",
    "                batch_method     = ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\"),\n",
    "                quant_norm       = TRUE,\n",
    "                modality         = ifelse(is_atac, \"snATAC-seq\", \"snRNA-seq\")\n",
    "            ), file.path(outdir, paste0(ct, \"_results_qn.rds\")))\n",
    "        } else {\n",
    "            # Save RDS without QN\n",
    "            saveRDS(list(\n",
    "                dge              = dge,\n",
    "                offset           = off,\n",
    "                residuals        = res,\n",
    "                final_data       = final,\n",
    "                valid_samples    = colnames(dge),\n",
    "                design           = design,\n",
    "                fit              = fit,\n",
    "                model            = form,\n",
    "                mode             = \"noBIOvar\",\n",
    "                batch_correction = as.logical(\"${batch_correction}\"),\n",
    "                batch_method     = ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\"),\n",
    "                quant_norm       = FALSE,\n",
    "                modality         = ifelse(is_atac, \"snATAC-seq\", \"snRNA-seq\")\n",
    "            ), file.path(outdir, paste0(ct, \"_results.rds\")))\n",
    "        }\n",
    "\n",
    "        message(\"Completed: \", ct, \" -> \", outdir)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56ffb1-6c07-47ac-9a1a-abbd37f253c9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `phenotype_reformatting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef18e3e-fe77-486f-89e6-e724b7126b73",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[phenotype_formatting]\n",
    "parameter: celltype   = ['Ast','Ex','In','Mic','Oligo','OPC']\n",
    "parameter: input_dir  = str\n",
    "parameter: output_dir = str\n",
    "\n",
    "input:  [f'{input_dir}/{ct}/{ct}_residuals.txt' for ct in celltype]\n",
    "output: [f'{output_dir}/3_pheno_reformat/{ct}_phenotype.bed.gz' for ct in celltype]\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '2:00:00', mem = '16G', cores = 2\n",
    "\n",
    "python: expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "\n",
    "    import os\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "\n",
    "    celltypes  = ${celltype}\n",
    "    input_dir  = \"${input_dir}\"\n",
    "    output_dir = \"${output_dir}\"\n",
    "\n",
    "    def read_residuals(path):\n",
    "        first_line = open(path).readline().rstrip(\"\\n\")\n",
    "        col_names  = first_line.split(\"\\t\")\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, skiprows=1)\n",
    "        if df.shape[1] > len(col_names):\n",
    "            peak_ids   = df.iloc[:, 0].values\n",
    "            df         = df.iloc[:, 1:]\n",
    "            df.columns = col_names\n",
    "        else:\n",
    "            peak_ids   = df.iloc[:, 0].values\n",
    "            df         = df.iloc[:, 1:]\n",
    "            df.columns = col_names[1:]\n",
    "        return peak_ids, df\n",
    "\n",
    "    def to_midpoint_bed(peak_ids, residuals):\n",
    "        \"\"\"Convert snATAC-seq peak IDs (chr-start-end) to midpoint BED format.\"\"\"\n",
    "        parts  = pd.Series(peak_ids).str.split(\"-\", expand=True)\n",
    "        chrs   = parts[0].values\n",
    "        starts = parts[1].astype(int).values\n",
    "        ends   = parts[2].astype(int).values\n",
    "        mids   = ((starts + ends) // 2).astype(int)\n",
    "        bed = pd.DataFrame({\n",
    "            \"#chr\":  chrs,\n",
    "            \"start\": mids,\n",
    "            \"end\":   mids + 1,\n",
    "            \"ID\":    peak_ids\n",
    "        })\n",
    "        bed = pd.concat([bed, residuals.reset_index(drop=True)], axis=1)\n",
    "        return bed.sort_values([\"#chr\", \"start\"]).reset_index(drop=True)\n",
    "\n",
    "    def run_cmd(cmd, label):\n",
    "        r = subprocess.run(cmd, capture_output=True)\n",
    "        if r.returncode != 0:\n",
    "            print(f\"WARNING: {label} failed: {r.stderr.decode()}\")\n",
    "        else:\n",
    "            print(f\"{label}: OK\")\n",
    "\n",
    "    for ct in celltypes:\n",
    "        print(f\"\\n{'='*40}\\nPhenotype Formatting: {ct}\\n{'='*40}\")\n",
    "\n",
    "        out_dir = os.path.join(output_dir, \"3_pheno_reformat\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        res_path = os.path.join(input_dir, ct, f\"{ct}_residuals.txt\")\n",
    "        if not os.path.exists(res_path):\n",
    "            print(f\"WARNING: {res_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        peak_ids, residuals = read_residuals(res_path)\n",
    "        print(f\"Loaded {len(peak_ids)} peaks x {residuals.shape[1]} samples\")\n",
    "\n",
    "        bed     = to_midpoint_bed(peak_ids, residuals)\n",
    "        out_bed = os.path.join(out_dir, f\"{ct}_phenotype.bed\")\n",
    "        bed.to_csv(out_bed, sep=\"\\t\", index=False, float_format=\"%.15f\")\n",
    "        print(f\"Written: {out_bed}\")\n",
    "\n",
    "        run_cmd([\"bgzip\", \"-f\", out_bed],                \"bgzip\")\n",
    "        run_cmd([\"tabix\", \"-p\", \"bed\", f\"{out_bed}.gz\"], \"tabix\")\n",
    "        print(f\"Completed: {ct} -> {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bc2ab-c412-40ef-a9b6-f5dddf5292ee",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `region_filtering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd13567e-2d4c-48f6-83d7-62ab252421bf",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[region_filtering]\n",
    "# Parameters\n",
    "parameter: celltype   = ['Ast','Ex','In','Mic','Oligo','OPC']\n",
    "parameter: input_dir  = str\n",
    "parameter: output_dir = str\n",
    "parameter: regions    = \"\"\n",
    "parameter: gene_list  = \"\"  # Note: Use --gene_list in command line\n",
    "\n",
    "# SoS Input/Output logic\n",
    "input:  [f'{input_dir}/{ct}/{ct}_filtered_raw_counts.txt' for ct in (celltype if isinstance(celltype, list) else [celltype])]\n",
    "output: [f'{output_dir}/3_region_filter/{ct}_filtered_regions_of_interest.txt' for ct in (celltype if isinstance(celltype, list) else [celltype])]\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '1:00:00', mem = '16G', cores = 2\n",
    "\n",
    "python: expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    # Handle SoS passing single strings vs lists\n",
    "    raw_ct = ${celltype!r}\n",
    "    celltypes = [raw_ct] if isinstance(raw_ct, str) else raw_ct\n",
    "    \n",
    "    input_dir  = \"${input_dir}\"\n",
    "    output_dir = \"${output_dir}\"\n",
    "    regions_str = \"${regions}\"\n",
    "    gene_list_str = \"${gene_list}\"\n",
    "\n",
    "    def parse_regions(region_str):\n",
    "        if not region_str or region_str.strip() == \"\":\n",
    "            return []\n",
    "        result = []\n",
    "        for r in region_str.split(\",\"):\n",
    "            chrom, coords = r.strip().split(\":\")\n",
    "            start, end    = coords.split(\"-\")\n",
    "            result.append({\"chr\": chrom, \"start\": int(start), \"end\": int(end)})\n",
    "        return result\n",
    "\n",
    "    def parse_peak_ids(peak_ids):\n",
    "        parts = pd.Series(peak_ids).str.split(\"-\", expand=True)\n",
    "        return pd.DataFrame({\n",
    "            \"chr\":   parts[0].values,\n",
    "            \"start\": parts[1].astype(int).values,\n",
    "            \"end\":   parts[2].astype(int).values\n",
    "        })\n",
    "\n",
    "    def overlaps_region(chr_col, start_col, end_col, reg):\n",
    "        return (\n",
    "            (chr_col   == reg[\"chr\"]) &\n",
    "            (start_col <   reg[\"end\"]) &\n",
    "            (end_col   >   reg[\"start\"])\n",
    "        )\n",
    "\n",
    "    regions = parse_regions(regions_str)\n",
    "    \n",
    "    genes_to_filter = None\n",
    "    if gene_list_str and gene_list_str.strip():\n",
    "        genes_to_filter = set([g.strip() for g in gene_list_str.split(\",\")])\n",
    "\n",
    "    for ct in celltypes:\n",
    "        reg_dir = os.path.join(output_dir, \"3_region_filter\")\n",
    "        os.makedirs(reg_dir, exist_ok=True)\n",
    "\n",
    "        counts_path = os.path.join(input_dir, ct, f\"{ct}_filtered_raw_counts.txt\")\n",
    "        if not os.path.exists(counts_path):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(counts_path, sep=\"\\t\", index_col=0)\n",
    "        first_id = df.index[0]\n",
    "        is_atac = \"-\" in str(first_id) and str(first_id).count(\"-\") >= 2\n",
    "        \n",
    "        # Consistent output name to match SoS 'output' definition\n",
    "        full_out = os.path.join(reg_dir, f\"{ct}_filtered_regions_of_interest.txt\")\n",
    "\n",
    "        if is_atac:\n",
    "            if not regions: continue\n",
    "            df.index.name = \"peak_id\"\n",
    "            df = df.reset_index()\n",
    "            coords = parse_peak_ids(df[\"peak_id\"].values)\n",
    "            df[\"chr\"], df[\"start\"], df[\"end\"] = coords[\"chr\"], coords[\"start\"], coords[\"end\"]\n",
    "            df[\"peakwidth\"] = df[\"end\"] - df[\"start\"]\n",
    "            \n",
    "            mask = pd.Series(False, index=df.index)\n",
    "            for reg in regions:\n",
    "                mask |= overlaps_region(df[\"chr\"], df[\"start\"], df[\"end\"], reg)\n",
    "\n",
    "            region_df = df[mask].copy()\n",
    "            region_df.to_csv(full_out, sep=\"\\t\", index=False)\n",
    "        \n",
    "        else:\n",
    "            if not genes_to_filter: continue\n",
    "            df.index.name = \"gene_name\"\n",
    "            genes_present = set(df.index) & genes_to_filter\n",
    "            if not genes_present: continue\n",
    "            \n",
    "            region_df = df.loc[list(genes_present)].copy()\n",
    "            # FIX: Use the same filename as defined in the SoS 'output' block\n",
    "            region_df.to_csv(full_out, sep=\"\\t\")\n",
    "\n",
    "        print(f\"Completed: {ct}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "sos",
     "",
     ""
    ]
   ],
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
