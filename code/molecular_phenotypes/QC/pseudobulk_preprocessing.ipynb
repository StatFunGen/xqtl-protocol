{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a44eb1-acb9-40f5-bcdb-7ede63d5db5e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#  Single-nuclei Pseudobulk Preprocessing (RNA-seq and ATAC-seq) Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline preprocesses single-nuclei pseudobulk **count** data (snATAC-seq or snRNA-seq) for downstream QTL analysis and region-specific studies.\n",
    "\n",
    "**Goals:**\n",
    "- Transform raw pseudobulk counts into analysis-ready formats\n",
    "- Remove technical confounders\n",
    "- Generate QTL-ready phenotype files or region-specific datasets\n",
    "\n",
    "## Pipeline Structure\n",
    "\n",
    "```\n",
    "Step 0: Sample ID Mapping                                 [sampleid_mapping]\n",
    "                    ↓\n",
    "Step 1: Pseudobulk QC                                     [pseudobulk_qc]\n",
    "        (optional) Region Peak/Gene Filtering          \n",
    "        (optional) Batch Correction (ComBat or limma)\n",
    "        (optional) Quantile Normalization\n",
    "                    ↓\n",
    "Step 2: Phenotype Reformatting  → BED                     [phenotype_formatting]\n",
    "        (genome-wide QTL mapping, snATAC-seq only)  \n",
    "```\n",
    "\n",
    "## Modality Support\n",
    "\n",
    "| Feature | snATAC-seq | snRNA-seq |\n",
    "|---------|-----------|-----------|\n",
    "| Sample ID mapping | ✓ | ✓ |\n",
    "| Region/gene filtering | ✓ (`--regions`) | ✓ (`--gene-list`) |\n",
    "| Blacklist filtering | ✓ | — |\n",
    "| `pseudobulk_qc` step | ✓ | ✓ |\n",
    "| `phenotype_formatting` step | ✓ | — (refer to this [pipeline](https://github.com/StatFunGen/xqtl-protocol/blob/main/code/data_preprocessing/phenotype/phenotype_formatting.ipynb)) |\n",
    "\n",
    "## Input Files\n",
    "\n",
    "All toy input files required to run this pipeline can be downloaded\n",
    "[here](https://drive.google.com/drive/folders/13ORslmqWTpICMIufhj_mrdL1KxQsG4lH?usp=drive_link).\n",
    "\n",
    "| File | Used in |\n",
    "|------|---------|\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` *(snATAC-seq)* | Step 0, Step 1 |\n",
    "| `pseudobulk_counts_{celltype}.csv.gz` *(snRNA-seq)* | Step 0, Step 1 |\n",
    "| `metadata_{celltype}.csv` | Step 0, Step 1 |\n",
    "| `rosmap_sample_mapping_data.csv` | Step 0 |\n",
    "| `tech_vars_{celltype}.csv` | Step 1 |\n",
    "| `hg38-blacklist.v2.bed.gz` | Step 1 (snATAC-seq only) |\n",
    "\n",
    "\n",
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e13a3f-ab64-4bd1-b47c-acca8d58a8b9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 0: Sample ID Mapping\n",
    "\n",
    "Maps original sample identifiers (`individualID`) to standardized sample IDs (`sampleid`)\n",
    "across metadata and count matrix files.\n",
    "\n",
    "### Input\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `rosmap_sample_mapping_data.csv` | Mapping reference: `individualID → sampleid` |\n",
    "| `metadata_{celltype}.csv` | Per-cell-type sample metadata |\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` *(snATAC-seq)* | Per-cell-type peak count matrices |\n",
    "| `pseudobulk_counts_{celltype}.csv.gz` *(snRNA-seq)* | Per-cell-type gene count matrices |\n",
    "\n",
    "### Process\n",
    "\n",
    "**Part 1 — Metadata files**\n",
    "\n",
    "For each metadata file:\n",
    "1. Look up each `individualID` in the mapping reference\n",
    "2. Assign `sampleid` — falls back to `individualID` if no mapping found\n",
    "3. Reorder columns: `sampleid` first, then `individualID`, then the rest\n",
    "4. Save updated file\n",
    "\n",
    "**Part 2 — Count matrix files**\n",
    "\n",
    "For each count file:\n",
    "1. Extract the header row (column names only)\n",
    "2. Keep the first column (peak or gene IDs) unchanged\n",
    "3. Map remaining column names (`individualID` → `sampleid`) where mapping exists, otherwise keep original\n",
    "4. Write new header and stream data rows unchanged\n",
    "5. Recompress with gzip\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `map_file` | *required* | CSV with `individualID` → `sampleid` mapping |\n",
    "| `meta_files` | *required* | Metadata CSV files to remap |\n",
    "| `count_files` | *required* | Count CSV.gz files to remap |\n",
    "| `output_dir` | *required* | Parent output directory; writes to `{output_dir}/1_files_with_sampleid/` |\n",
    "\n",
    "### Output\n",
    "\n",
    "Output directory: `{output_dir}/1_files_with_sampleid/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `metadata_{celltype}.csv` | Metadata with `sampleid` column prepended |\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` *(snATAC-seq)* | Count matrices with mapped column headers |\n",
    "| `pseudobulk_counts_{celltype}.csv.gz` *(snRNA-seq)* | Count matrices with mapped column headers |\n",
    "\n",
    "\n",
    "**Timing:** < 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f7cbe0-bf5e-4d7e-8a2b-216915dea78e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = input): <text>:1:5: unexpected symbol\n1: sos run\n        ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = input): <text>:1:5: unexpected symbol\n1: sos run\n        ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "sos run pipeline/pseudobulk_preprocessing.ipynb sampleid_mapping \\\n",
    "    --output-dir output/snatac_seq \\\n",
    "    --map-file data/rosmap_sample_mapping_data.csv \\\n",
    "    --meta-files data/snatac_seq/metadata_Mic_50nuc.csv \\\n",
    "    --count-files data/snatac_seq/pseudobulk_peaks_counts_Mic_50nuc.csv.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540a4da-843a-4789-8123-47911cf519c5",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 1: Pseudobulk QC\n",
    "\n",
    "Regresses out technical covariates for downstream QTL analysis. Works for both snATAC-seq and snRNA-seq.\n",
    "\n",
    "### Input\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `metadata_{celltype}.csv` | Sample-level metadata (nuclei counts, batch info) |\n",
    "| `pseudobulk_*counts_{celltype}.csv.gz` | Pseudobulk count matrix |\n",
    "| `tech_vars.csv` | Technical covariates (sampleid + tech var columns, pre-processed) |\n",
    "| `hg38-blacklist.v2.bed.gz` *(snATAC-seq, optional)* | Blacklisted genomic regions |\n",
    "\n",
    "### Process\n",
    "\n",
    "1. Load count matrix and auto-detect modality (snATAC-seq vs snRNA-seq)\n",
    "2. ***(Optional)*** Filter to specific genomic regions (snATAC-seq) or gene list (snRNA-seq)\n",
    "3. Load metadata; filter samples with fewer than `min_nuclei` nuclei (default: 20)\n",
    "4. Align samples between metadata and count matrix\n",
    "5. ***(Optional)*** Filter blacklisted genomic regions (snATAC-seq only)\n",
    "6. Merge tech vars from `tech_vars_file` by `sampleid` \n",
    "7. Drop samples with NA in any tech var\n",
    "8. Apply expression filtering (`filterByExpr`):\n",
    "   - `min_count = 5`: minimum reads in at least one sample\n",
    "   - `min_total_count = 15`: minimum total reads across all samples\n",
    "   - `min_prop = 0.1`: feature expressed in ≥10% of samples\n",
    "9. TMM normalization\n",
    "10. ***(Optional)*** Batch correction on `sequencingBatch`:\n",
    "    - `limma::removeBatchEffect` (default)\n",
    "    - `ComBat` (on log-CPM)\n",
    "11. Add `sequencingBatch` and `Library` to model if present and multi-level\n",
    "12. Fit linear model (`voom` + `lmFit` + `eBayes`) with **tech vars + batch vars only** \n",
    "13. Compute `offset + residuals` as final adjusted values:\n",
    "    - `offset`: intercept + batch effects at reference level\n",
    "    - `residuals`: variation after removing technical effects; biological signal retained\n",
    "14. ***(Optional)*** Quantile normalization of final values\n",
    "\n",
    "**Model formula:**\n",
    "```\n",
    "~ {tech_vars} + [sequencingBatch] + [Library]\n",
    "```\n",
    "> `sequencingBatch` and `Library` included only if present and have more than one level.\n",
    "> Biological variables (`pmi`, `study`, `msex`, `age_death` etc.) are **not** included — they should not be regressed out as they may be associated with genotype.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `meta_files` | *required* | Metadata CSV files (one per cell type) |\n",
    "| `count_files` | *required* | Count CSV.gz files (one per cell type, same order as `meta_files`) |\n",
    "| `output_dir` | *required* | Parent output directory; writes to `{output_dir}/2_residuals/{ct}/` |\n",
    "| `tech_vars_file` | *required* | CSV with `sampleid` + tech var columns |\n",
    "| `blacklist_file` | `''` | Genomic blacklist BED file (snATAC-seq only) |\n",
    "| `regions` | `''` | Comma-separated genomic regions e.g. `chr7:28000000-28300000` (snATAC-seq) |\n",
    "| `gene_list` | `''` | Comma-separated gene IDs e.g. `ENSG00000000010` (snRNA-seq) |\n",
    "| `batch_correction` | `FALSE` | Apply batch correction (`TRUE`/`FALSE`) |\n",
    "| `batch_method` | `limma` | Batch correction method (`limma` or `combat`) |\n",
    "| `quant_norm` | `FALSE` | Apply quantile normalization after residuals |\n",
    "| `min_count` | `5` | Min reads in at least one sample |\n",
    "| `min_total_count` | `15` | Min total reads across all samples |\n",
    "| `min_prop` | `0.1` | Min proportion of samples with expression |\n",
    "| `min_nuclei` | `20` | Min nuclei per sample |\n",
    "\n",
    "### Output\n",
    "\n",
    "Output directory: `{output_dir}/2_residuals/{celltype}/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_residuals.txt` | Tech-covariate-adjusted values (log2-CPM) |\n",
    "| `{celltype}_residuals_qn.txt` | Quantile-normalized adjusted values *(if `quant_norm=TRUE`)* |\n",
    "| `{celltype}_results.rds` | Full results: DGEList, fit, offset, residuals, design, parameters |\n",
    "| `{celltype}_filtered_raw_counts.txt` | Filtered raw counts before normalization |\n",
    "\n",
    "**Timing:** < 5 min per cell type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cbd39c-60f8-4e21-9915-14b30ebc02cd",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Pseudobulk QC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741ac2e-91b3-49e7-906a-2fd6b8d8d137",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/pseudobulk_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --meta-files output/snatac_seq/1_files_with_sampleid/metadata_Mic_50nuc.csv \\\n",
    "    --count-files output/snatac_seq/1_files_with_sampleid/pseudobulk_peaks_counts_Mic_50nuc.csv.gz  \\\n",
    "    --output-dir output/snatac_seq \\\n",
    "    --tech-vars-file data/snatac_seq/tech_vars_MIC.csv \\\n",
    "    --blacklist-file data/hg38-blacklist.v2.bed.gz #only for snATAC-seq\n",
    "\n",
    "sos run pipeline/pseudobulk_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --meta-files output/snrna_seq/1_files_with_sampleid/metadata_MIC.csv \\\n",
    "    --count-files output/snrna_seq/1_files_with_sampleid/pseudobulk_counts_MIC.csv.gz  \\\n",
    "    --output-dir output/snrna_seq \\\n",
    "    --tech-vars-file data/snrna_seq/tech_vars_MIC.csv \\\n",
    "    --gene-list ENSG00000000010,ENSG00000000020 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ea976-ca30-48e4-811b-eaa0b5f246ed",
   "metadata": {},
   "source": [
    "### Additional parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b41a7f-1d08-4174-858b-a0593aaadcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "--min-count 5\n",
    "--min-total-count 15\n",
    "--min-prop 0.1\n",
    "--min-nuclei 20\n",
    "--quant-norm TRUE\n",
    "--batch-correction TRUE \n",
    "--batch-method combat # or limma\n",
    "--gene-list ENSG00000000010,ENSG00000000020  # for snRNA-seq\n",
    "--regions chr7:28000000-28300000  # for snATAC-seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d5b04-ec21-4c0c-8879-d78563d5ed96",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 2: Phenotype Reformatting (snATAC-seq only)\n",
    "\n",
    "Converts residuals into a QTL-ready BED format for genome-wide caQTL mapping.\n",
    "\n",
    "> For snRNA-seq, please follow this [pipeline](https://github.com/StatFunGen/xqtl-protocol/blob/main/code/data_preprocessing/phenotype/phenotype_formatting.ipynb).\n",
    "\n",
    "### Input\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_residuals.txt` | Residuals from `pseudobulk_qc` |\n",
    "\n",
    "### Process\n",
    "\n",
    "1. Read residuals file with proper handling of feature IDs and sample columns\n",
    "2. Parse peak coordinates from peak IDs (`chr-start-end` format)\n",
    "3. Convert to midpoint coordinates (standard for QTLtools):\n",
    "```\n",
    "start = floor((peak_start + peak_end) / 2)\n",
    "end   = start + 1\n",
    "```\n",
    "4. Build BED format: `#chr`, `start`, `end`, `ID` followed by per-sample values\n",
    "5. Sort by chromosome and position\n",
    "6. Compress with `bgzip` and index with `tabix`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `residual_files` | *required* | Residual txt files from `pseudobulk_qc` |\n",
    "| `output_dir` | *required* | Parent output directory; writes to `{output_dir}/3_pheno_reformat/` |\n",
    "\n",
    "### Output\n",
    "\n",
    "Output directory: `{output_dir}/3_pheno_reformat/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_phenotype.bed.gz` | bgzip-compressed BED with midpoint coordinates |\n",
    "| `{celltype}_phenotype.bed.gz.tbi` | tabix index for random-access queries |\n",
    "\n",
    "Compatible with FastQTL, TensorQTL, and QTLtools.\n",
    "\n",
    "**Timing:** < 1 min per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaf9d56-b53b-4a6a-b0af-b5a5fb98907b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = input): <text>:1:5: unexpected symbol\n1: sos run\n        ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = input): <text>:1:5: unexpected symbol\n1: sos run\n        ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "sos run pipeline/pseudobulk_preprocessing.ipynb phenotype_formatting \\\n",
    "    --residual-files output/snatac_seq/2_residuals/Mic_50nuc/Mic_50nuc_residuals.txt \\\n",
    "    --output-dir output/snatac_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a676801-6845-4ca5-944b-7978a5ecbb1f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486664a9-55c2-4738-91a0-b63ffdcd6cfa",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/pseudobulk_preprocessing.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17a301-cca9-49a1-843b-4248546f1f79",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57fe47-f2ca-4a6e-8789-f7dbe3a9fad2",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "parameter: job_size = 1\n",
    "parameter: walltime = \"5h\"\n",
    "parameter: mem = \"16G\"\n",
    "parameter: numThreads = 8\n",
    "parameter: container = \"\"\n",
    "\n",
    "import re\n",
    "from sos.utils import expand_size\n",
    "\n",
    "entrypoint = (\n",
    "    'micromamba run -a \"\" -n' + ' ' +\n",
    "    re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])\n",
    ") if container else \"\"\n",
    "\n",
    "cwd = path(f'{cwd:a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee58015-c8e2-4697-bdae-58d7e494640d",
   "metadata": {},
   "source": [
    "```\n",
    "usage: sos run pipeline/pseudobulk_preprocessing.ipynb\n",
    "               [workflow_name | -t targets] [options] [workflow_options]\n",
    "  workflow_name:        Single or combined workflows defined in this script\n",
    "  targets:              One or more targets to generate\n",
    "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
    "  workflow_options:     Double-hyphen workflow-specific parameters\n",
    "Workflows:\n",
    "  sampleid_mapping\n",
    "  pseudobulk_qc\n",
    "  phenotype_formatting\n",
    "Global Workflow Options:\n",
    "  --cwd output (as path)\n",
    "  --job-size 1 (as int)\n",
    "  --walltime 5h\n",
    "  --mem 16G\n",
    "  --numThreads 8 (as int)\n",
    "  --container ''\n",
    "Sections\n",
    "  sampleid_mapping:\n",
    "    Workflow Options:\n",
    "      --map-file VAL (as str, required)\n",
    "      --output-dir VAL (as str, required)\n",
    "      --meta-files  (as list)\n",
    "      --count-files  (as list)\n",
    "  pseudobulk_qc:\n",
    "    Workflow Options:\n",
    "      --meta-files  (as list)\n",
    "      --count-files  (as list)\n",
    "      --output-dir VAL (as str, required)\n",
    "      --tech-vars-file VAL (as str, required)\n",
    "      --blacklist-file ''\n",
    "      --batch-correction FALSE\n",
    "      --batch-method limma\n",
    "      --quant-norm FALSE\n",
    "      --min-count 5 (as int)\n",
    "      --min-total-count 15 (as int)\n",
    "      --min-prop 0.1 (as float)\n",
    "      --min-nuclei 20 (as int)\n",
    "      --regions ''\n",
    "      --gene-list ''\n",
    "  phenotype_formatting:\n",
    "    Workflow Options:\n",
    "      --residual-files  (as list)\n",
    "      --output-dir VAL (as str, required)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6024cd-28be-4fb0-994e-0460e3a3beae",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `sampleid_mapping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1b7c0-2819-45d1-b2ce-8d117a6cc9eb",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sampleid_mapping]\n",
    "parameter: map_file    = str\n",
    "parameter: output_dir  = str\n",
    "parameter: meta_files  = []\n",
    "parameter: count_files = []\n",
    "\n",
    "import os\n",
    "\n",
    "input:  meta_files + count_files\n",
    "output: [f'{output_dir}/1_files_with_sampleid/{os.path.basename(f)}' for f in meta_files + count_files]\n",
    "         \n",
    "python: expand = \"${ }\"\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "map_df = pd.read_csv(\"${map_file}\")\n",
    "id_map = dict(zip(map_df[\"individualID\"], map_df[\"sampleid\"]))\n",
    "output_dir  = \"${output_dir}/1_files_with_sampleid\"\n",
    "meta_files  = ${meta_files}\n",
    "count_files = ${count_files}\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def map_id(ind_id):\n",
    "    return id_map.get(ind_id, ind_id)\n",
    "\n",
    "def format_value(val):\n",
    "    if pd.isna(val):\n",
    "        return ''\n",
    "    if isinstance(val, (int, np.integer)):\n",
    "        return str(val)\n",
    "    if isinstance(val, (float, np.floating)):\n",
    "        if val == int(val):\n",
    "            return str(int(val))\n",
    "        else:\n",
    "            return str(val)\n",
    "    return str(val)\n",
    "\n",
    "# ── Process metadata ───────────────────────────────────────────────────────\n",
    "for in_path in meta_files:\n",
    "    fname    = os.path.basename(in_path)\n",
    "    out_path = os.path.join(output_dir, fname)\n",
    "    meta = pd.read_csv(in_path)\n",
    "    if \"individualID\" not in meta.columns:\n",
    "        print(f\"Warning: individualID column not found in {fname}, skipping.\")\n",
    "        continue\n",
    "    meta[\"sampleid\"] = meta[\"individualID\"].map(map_id)\n",
    "    cols = meta.columns.tolist()\n",
    "    cols.remove(\"sampleid\")\n",
    "    cols.remove(\"individualID\")\n",
    "    meta = meta[[\"sampleid\", \"individualID\"] + cols]\n",
    "    with open(out_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(meta.columns)\n",
    "        for _, row in meta.iterrows():\n",
    "            writer.writerow([format_value(val) for val in row])\n",
    "\n",
    "# ── Process count files ────────────────────────────────────────────────────\n",
    "for in_path in count_files:\n",
    "    fname    = os.path.basename(in_path)\n",
    "    out_path = os.path.join(output_dir, fname)\n",
    "    with gzip.open(in_path, \"rt\") as fh:\n",
    "        header_line = fh.readline().rstrip(\"\\n\")\n",
    "    col_names       = header_line.split(\",\")\n",
    "    peak_id_col     = col_names[0]\n",
    "    new_sample_cols = [map_id(s) for s in col_names[1:]]\n",
    "    new_header      = \",\".join([peak_id_col] + new_sample_cols)\n",
    "    tmp = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')\n",
    "    tmp.write(new_header + \"\\n\")\n",
    "    tmp.close()\n",
    "    cmd = f\"zcat {in_path} | tail -n +2 | cat {tmp.name} - | gzip -6 > {out_path}\"\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "    os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0884ae7-a851-425a-86dd-b606768a012e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `pseudobulk_qc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46328b-c3d8-46f8-8c71-bad27820438e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[pseudobulk_qc]\n",
    "parameter: meta_files       = []\n",
    "parameter: count_files      = []\n",
    "parameter: output_dir       = str\n",
    "parameter: tech_vars_file   = str\n",
    "parameter: blacklist_file   = ''\n",
    "parameter: batch_correction = \"FALSE\"\n",
    "parameter: batch_method     = \"limma\"\n",
    "parameter: quant_norm       = \"FALSE\"\n",
    "parameter: min_count        = 5\n",
    "parameter: min_total_count  = 15\n",
    "parameter: min_prop         = 0.1\n",
    "parameter: min_nuclei       = 20\n",
    "parameter: regions          = ''\n",
    "parameter: gene_list        = ''\n",
    "\n",
    "import os\n",
    "\n",
    "_cts = [os.path.basename(f).replace('metadata_','').replace('.csv','') for f in meta_files]\n",
    "\n",
    "input:  meta_files + count_files\n",
    "output: [f'{output_dir}/2_residuals/{ct}/{ct}_residuals.txt' for ct in _cts]\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '6:00:00', mem = '64G', cores = 4\n",
    "\n",
    "R: expand = \"${ }\", stdout = f'{_output[0]:n}.stdout', stderr = f'{_output[0]:n}.stderr'\n",
    "\n",
    "    library(edgeR)\n",
    "    library(limma)\n",
    "    library(data.table)\n",
    "    library(GenomicRanges)\n",
    "    if (as.logical(\"${batch_correction}\") && \"${batch_method}\" == \"combat\") library(sva)\n",
    "\n",
    "    # ── predictOffset ──────────────────────────────────────────────────────\n",
    "    predictOffset <- function(fit, tech_vars) {\n",
    "        D  <- fit$design\n",
    "        Dm <- D\n",
    "        for (col in colnames(D)) {\n",
    "            if (col == \"(Intercept)\") next\n",
    "            is_tech <- any(sapply(tech_vars, function(v) grepl(paste0(\"^\", v), col)))\n",
    "            if (is_tech) {\n",
    "                if (is.numeric(D[, col]) && !all(D[, col] %in% c(0, 1)))\n",
    "                    Dm[, col] <- median(D[, col], na.rm=TRUE)\n",
    "                else\n",
    "                    Dm[, col] <- 0\n",
    "            } else {\n",
    "                Dm[, col] <- 0\n",
    "            }\n",
    "        }\n",
    "        B <- fit$coefficients\n",
    "        B[is.na(B)] <- 0\n",
    "        off <- B %*% t(Dm)\n",
    "        colnames(off) <- rownames(fit$design)\n",
    "        return(off)\n",
    "    }\n",
    "\n",
    "    filter_blacklist <- function(mat, bed, feat_label) {\n",
    "        peaks <- data.table(id = rownames(mat))\n",
    "        peaks[, c(\"chr\",\"start\",\"end\") := tstrsplit(gsub(\"_\",\"-\",id), \"-\")]\n",
    "        peaks[, `:=`(start = as.numeric(start), end = as.numeric(end))]\n",
    "        bl <- fread(bed)[, 1:3]\n",
    "        setnames(bl, c(\"chr\",\"start\",\"end\"))\n",
    "        bl[, `:=`(start = as.numeric(start), end = as.numeric(end))]\n",
    "        gr1 <- GRanges(peaks$chr, IRanges(peaks$start, peaks$end))\n",
    "        gr2 <- GRanges(bl$chr,    IRanges(bl$start,    bl$end))\n",
    "        blacklisted <- unique(queryHits(findOverlaps(gr1, gr2)))\n",
    "        if (length(blacklisted) > 0) {\n",
    "            message(\"Blacklisted \", feat_label, \" removed: \", length(blacklisted))\n",
    "            return(mat[-blacklisted, , drop=FALSE])\n",
    "        }\n",
    "        return(mat)\n",
    "    }\n",
    "\n",
    "    parse_regions <- function(region_str) {\n",
    "        if (is.null(region_str) || region_str == \"\") return(NULL)\n",
    "        lapply(strsplit(region_str, \",\")[[1]], function(r) {\n",
    "            parts <- strsplit(trimws(r), \":|−|-\")[[1]]\n",
    "            list(chr=parts[1], start=as.integer(parts[2]), end=as.integer(parts[3]))\n",
    "        })\n",
    "    }\n",
    "\n",
    "    filter_regions <- function(mat, regions) {\n",
    "        peaks <- data.table(id = rownames(mat))\n",
    "        peaks[, c(\"chr\",\"start\",\"end\") := tstrsplit(gsub(\"_\",\"-\",id), \"-\")]\n",
    "        peaks[, `:=`(start = as.integer(start), end = as.integer(end))]\n",
    "        gr_peaks <- GRanges(peaks$chr, IRanges(peaks$start, peaks$end))\n",
    "        gr_regions <- GRanges(\n",
    "            sapply(regions, `[[`, \"chr\"),\n",
    "            IRanges(sapply(regions, `[[`, \"start\"), sapply(regions, `[[`, \"end\"))\n",
    "        )\n",
    "        keep <- unique(queryHits(findOverlaps(gr_peaks, gr_regions)))\n",
    "        if (length(keep) == 0) stop(\"No peaks overlap the specified regions.\")\n",
    "        message(\"Peaks after region filter: \", length(keep))\n",
    "        mat[keep, , drop=FALSE]\n",
    "    }\n",
    "\n",
    "    meta_files  <- c(${','.join([f'\"{f}\"' for f in meta_files])})\n",
    "    count_files <- c(${','.join([f'\"{f}\"' for f in count_files])})\n",
    "\n",
    "    if (length(meta_files) != length(count_files))\n",
    "        stop(\"meta_files and count_files must have the same length and order.\")\n",
    "\n",
    "    # ── Load tech vars from file ───────────────────────────────────────────\n",
    "    tech_df   <- fread(\"${tech_vars_file}\")\n",
    "    tech_vars <- setdiff(colnames(tech_df), \"sampleid\")\n",
    "    message(\"Tech vars: \", paste(tech_vars, collapse=\", \"))\n",
    "\n",
    "    regions   <- parse_regions(\"${regions}\")\n",
    "    gene_list <- trimws(strsplit(\"${gene_list}\", \",\")[[1]])\n",
    "    gene_list <- gene_list[gene_list != \"\"]\n",
    "\n",
    "    for (i in seq_along(meta_files)) {\n",
    "        meta_file   <- meta_files[i]\n",
    "        counts_file <- count_files[i]\n",
    "        ct          <- sub(\"\\\\.csv$\", \"\", sub(\"^metadata_\", \"\", basename(meta_file)))\n",
    "\n",
    "        message(\"\\n\", paste(rep(\"=\", 40), collapse=\"\"))\n",
    "        message(\"Processing: \", ct)\n",
    "        message(\"Batch correction: \", ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\"))\n",
    "        message(\"Quantile normalization: \", as.logical(\"${quant_norm}\"))\n",
    "        message(paste(rep(\"=\", 40), collapse=\"\"))\n",
    "\n",
    "        outdir <- file.path(\"${output_dir}/2_residuals\", ct)\n",
    "        dir.create(outdir, recursive=TRUE, showWarnings=FALSE)\n",
    "\n",
    "        # ── 1. Load counts ─────────────────────────────────────────────────\n",
    "        counts_raw       <- fread(counts_file)\n",
    "        counts           <- as.matrix(counts_raw[, -1, with=FALSE])\n",
    "        rownames(counts) <- counts_raw[[1]]\n",
    "        rm(counts_raw)\n",
    "\n",
    "        # ── Auto-detect modality ───────────────────────────────────────────\n",
    "        is_atac    <- grepl(\"^chr.*-[0-9]+-[0-9]+$\", rownames(counts)[1])\n",
    "        feat_label <- ifelse(is_atac, \"peaks\", \"genes\")\n",
    "        message(\"Modality: \", ifelse(is_atac, \"snATAC-seq\", \"snRNA-seq\"))\n",
    "        message(\"Loaded: \", nrow(counts), \" \", feat_label, \" x \", ncol(counts), \" samples\")\n",
    "\n",
    "        # ── 1b. Region/gene filtering (optional) ──────────────────────────\n",
    "        if (is_atac && !is.null(regions)) {\n",
    "            message(\"Filtering peaks to specified regions...\")\n",
    "            counts <- filter_regions(counts, regions)\n",
    "        } else if (!is_atac && length(gene_list) > 0) {\n",
    "            genes_present <- intersect(rownames(counts), gene_list)\n",
    "            if (length(genes_present) == 0) stop(\"No matching genes found in count matrix.\")\n",
    "            message(\"Genes after gene_list filter: \", length(genes_present))\n",
    "            counts <- counts[genes_present, , drop=FALSE]\n",
    "        }\n",
    "\n",
    "        # ── 2. Load metadata ───────────────────────────────────────────────\n",
    "        meta  <- fread(meta_file)\n",
    "        idcol <- intersect(c(\"sampleid\",\"sampleID\",\"individualID\",\"projid\"), colnames(meta))[1]\n",
    "        if (is.na(idcol)) stop(\"Cannot find sample ID column in metadata.\")\n",
    "\n",
    "        # ── 3. Nuclei filter ──────────────────────────────────────────────\n",
    "        n_nuclei_col <- intersect(c(\"n_nuclei\",\"n.nuclei\",\"nNuclei\",\"nuclei_count\"), colnames(meta))[1]\n",
    "        if (!is.na(n_nuclei_col)) {\n",
    "            meta <- meta[meta[[n_nuclei_col]] > ${min_nuclei}]\n",
    "            message(\"Samples after nuclei (>${min_nuclei}) filter: \", nrow(meta))\n",
    "        }\n",
    "\n",
    "        # ── 4. Align samples ──────────────────────────────────────────────\n",
    "        common <- intersect(meta[[idcol]], colnames(counts))\n",
    "        if (length(common) == 0) stop(\"Zero sample overlap between metadata and count matrix.\")\n",
    "        counts <- counts[, common, drop=FALSE]\n",
    "        message(\"Samples after alignment: \", length(common))\n",
    "\n",
    "        # ── 5. Blacklist filtering ─────────────────────────────────────────\n",
    "        if (\"${blacklist_file}\" != \"\" && file.exists(\"${blacklist_file}\")) {\n",
    "            counts <- filter_blacklist(counts, \"${blacklist_file}\", feat_label)\n",
    "            message(feat_label, \" after blacklist filter: \", nrow(counts))\n",
    "        } else {\n",
    "            message(\"No blacklist file - skipping.\")\n",
    "        }\n",
    "\n",
    "        # ── 6. Merge tech vars by sampleid ────────────────────────────────\n",
    "        tech_sub <- tech_df[tech_df$sampleid %in% common]\n",
    "        tech_sub <- tech_sub[match(common, tech_sub$sampleid)]\n",
    "\n",
    "        # ── 7. Drop samples with NA in tech vars ──────────────────────────\n",
    "        keep_rows <- complete.cases(tech_sub[, ..tech_vars])\n",
    "        tech_sub  <- tech_sub[keep_rows]\n",
    "        counts    <- counts[, tech_sub$sampleid, drop=FALSE]\n",
    "        message(\"Valid samples for modelling: \", nrow(tech_sub))\n",
    "\n",
    "        # ── 8. Expression filtering ────────────────────────────────────────\n",
    "        dge <- DGEList(counts=counts, samples=tech_sub)\n",
    "        dge$samples$group <- factor(rep(\"all\", ncol(dge)))\n",
    "        message(feat_label, \" before filter: \", nrow(dge))\n",
    "\n",
    "        keep <- filterByExpr(dge, group=dge$samples$group,\n",
    "                             min.count=${min_count},\n",
    "                             min.total.count=${min_total_count},\n",
    "                             min.prop=${min_prop})\n",
    "        dge <- dge[keep,, keep.lib.sizes=FALSE]\n",
    "        message(feat_label, \" after filter: \", nrow(dge))\n",
    "\n",
    "        write.table(dge$counts,\n",
    "                    file.path(outdir, paste0(ct, \"_filtered_raw_counts.txt\")),\n",
    "                    sep=\"\\t\", quote=FALSE, col.names=NA)\n",
    "\n",
    "        # ── 9. TMM normalization ───────────────────────────────────────────\n",
    "        dge <- calcNormFactors(dge, method=\"TMM\")\n",
    "\n",
    "        # ── 10. Optional batch correction ──────────────────────────────────\n",
    "        if (as.logical(\"${batch_correction}\") && \"sequencingBatch\" %in% colnames(dge$samples)) {\n",
    "            batches       <- dge$samples$sequencingBatch\n",
    "            batch_counts  <- table(batches)\n",
    "            valid_batches <- names(batch_counts[batch_counts > 1])\n",
    "            keep_bc       <- batches %in% valid_batches\n",
    "            dge           <- dge[, keep_bc, keep.lib.sizes=FALSE]\n",
    "            batches       <- batches[keep_bc]\n",
    "            message(\"Samples after singleton batch removal: \", ncol(dge))\n",
    "\n",
    "            if (\"${batch_method}\" == \"combat\") {\n",
    "                logCPM     <- cpm(dge, log=TRUE, prior.count=1)\n",
    "                logCPM     <- ComBat(dat=logCPM, batch=factor(batches))\n",
    "                dge$counts <- round(pmax(2^logCPM, 0))\n",
    "                message(\"ComBat applied on log-CPM.\")\n",
    "            } else {\n",
    "                logCPM     <- cpm(dge, log=TRUE, prior.count=1)\n",
    "                logCPM     <- removeBatchEffect(logCPM, batch=factor(batches))\n",
    "                dge$counts <- round(pmax(2^logCPM, 0))\n",
    "                message(\"limma removeBatchEffect applied.\")\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # ── 11. Add batch vars to model if multi-level ────────────────────\n",
    "        batch_vars <- c()\n",
    "        if (\"sequencingBatch\" %in% colnames(dge$samples) &&\n",
    "            length(unique(dge$samples$sequencingBatch)) > 1) {\n",
    "            dge$samples$sequencingBatch_factor <- factor(dge$samples$sequencingBatch)\n",
    "            batch_vars <- c(batch_vars, \"sequencingBatch_factor\")\n",
    "        }\n",
    "        if (\"Library\" %in% colnames(dge$samples) &&\n",
    "            length(unique(dge$samples$Library)) > 1) {\n",
    "            dge$samples$Library_factor <- factor(dge$samples$Library)\n",
    "            batch_vars <- c(batch_vars, \"Library_factor\")\n",
    "        }\n",
    "\n",
    "        # ── 12. Build design matrix ────────────────────────────────────────\n",
    "        all_model_vars <- intersect(c(tech_vars, batch_vars), colnames(dge$samples))\n",
    "        form           <- as.formula(paste(\"~\", paste(all_model_vars, collapse=\" + \")))\n",
    "        design         <- model.matrix(form, data=dge$samples)\n",
    "        message(\"Formula: \", deparse(form))\n",
    "\n",
    "        if (!is.fullrank(design)) {\n",
    "            message(\"Design not full rank - trimming.\")\n",
    "            qr_d   <- qr(design)\n",
    "            design <- design[, qr_d$pivot[seq_len(qr_d$rank)], drop=FALSE]\n",
    "        }\n",
    "        message(\"Design matrix: \", nrow(design), \" x \", ncol(design))\n",
    "\n",
    "        # ── 13. Voom + lmFit + eBayes ─────────────────────────────────────\n",
    "        v   <- voom(dge, design, plot=FALSE)\n",
    "        fit <- lmFit(v, design)\n",
    "        fit <- eBayes(fit)\n",
    "\n",
    "        # ── 14. Offset + residuals ─────────────────────────────────────────\n",
    "        off   <- predictOffset(fit, tech_vars=tech_vars)\n",
    "        res   <- residuals(fit, v$E)\n",
    "        final <- off + res\n",
    "\n",
    "        # ── 15. Save residuals ─────────────────────────────────────────────\n",
    "        out_file <- file.path(outdir, paste0(ct, \"_residuals.txt\"))\n",
    "        write.table(final, out_file, sep=\"\\t\", quote=FALSE, col.names=NA)\n",
    "        message(\"Saved: \", out_file)\n",
    "        message(\"  \", ifelse(is_atac,\"Peaks\",\"Genes\"), \": \", nrow(final), \" | Samples: \", ncol(final))\n",
    "\n",
    "        # ── 16. Optional quantile normalization ───────────────────────────\n",
    "        if (as.logical(\"${quant_norm}\")) {\n",
    "            final_qn <- t(apply(final, 1, rank, ties.method=\"average\"))\n",
    "            final_qn <- stats::qnorm(final_qn / (ncol(final_qn) + 1))\n",
    "            qn_file  <- file.path(outdir, paste0(ct, \"_residuals_qn.txt\"))\n",
    "            write.table(final_qn, qn_file, sep=\"\\t\", quote=FALSE, col.names=NA)\n",
    "            message(\"Saved QN: \", qn_file)\n",
    "\n",
    "            saveRDS(list(\n",
    "                dge=dge, offset=off, residuals=res,\n",
    "                final_data=final, final_data_qn=final_qn,\n",
    "                valid_samples=colnames(dge), design=design, fit=fit, model=form,\n",
    "                tech_vars=tech_vars, batch_vars=batch_vars,\n",
    "                batch_correction=as.logical(\"${batch_correction}\"),\n",
    "                batch_method=ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\"),\n",
    "                quant_norm=TRUE,\n",
    "                modality=ifelse(is_atac, \"snATAC-seq\", \"snRNA-seq\")\n",
    "            ), file.path(outdir, paste0(ct, \"_results_qn.rds\")))\n",
    "        } else {\n",
    "            saveRDS(list(\n",
    "                dge=dge, offset=off, residuals=res,\n",
    "                final_data=final,\n",
    "                valid_samples=colnames(dge), design=design, fit=fit, model=form,\n",
    "                tech_vars=tech_vars, batch_vars=batch_vars,\n",
    "                batch_correction=as.logical(\"${batch_correction}\"),\n",
    "                batch_method=ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\"),\n",
    "                quant_norm=FALSE,\n",
    "                modality=ifelse(is_atac, \"snATAC-seq\", \"snRNA-seq\")\n",
    "            ), file.path(outdir, paste0(ct, \"_results.rds\")))\n",
    "        }\n",
    "\n",
    "        message(\"Completed: \", ct, \" -> \", outdir)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56ffb1-6c07-47ac-9a1a-abbd37f253c9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `phenotype_reformatting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef18e3e-fe77-486f-89e6-e724b7126b73",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = input): <text>:1:1: unexpected '['\n1: [\n    ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = input): <text>:1:1: unexpected '['\n1: [\n    ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "[phenotype_formatting]\n",
    "parameter: residual_files = []\n",
    "parameter: output_dir     = str\n",
    "\n",
    "import os\n",
    "\n",
    "_cts = [os.path.basename(os.path.dirname(f)) for f in residual_files]\n",
    "\n",
    "input:  residual_files\n",
    "output: [f'{output_dir}/3_pheno_reformat/{ct}_phenotype.bed.gz' for ct in _cts]\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '2:00:00', mem = '16G', cores = 2\n",
    "\n",
    "python: expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    import os\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "\n",
    "    residual_files = ${residual_files}\n",
    "    output_dir     = \"${output_dir}\"\n",
    "\n",
    "    def read_residuals(path):\n",
    "        first_line = open(path).readline().rstrip(\"\\n\")\n",
    "        col_names  = first_line.split(\"\\t\")\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, skiprows=1)\n",
    "        if df.shape[1] > len(col_names):\n",
    "            peak_ids   = df.iloc[:, 0].values\n",
    "            df         = df.iloc[:, 1:]\n",
    "            df.columns = col_names\n",
    "        else:\n",
    "            peak_ids   = df.iloc[:, 0].values\n",
    "            df         = df.iloc[:, 1:]\n",
    "            df.columns = col_names[1:]\n",
    "        return peak_ids, df\n",
    "\n",
    "    def to_midpoint_bed(peak_ids, residuals):\n",
    "        parts  = pd.Series(peak_ids).str.split(\"-\", expand=True)\n",
    "        chrs   = parts[0].values\n",
    "        starts = parts[1].astype(int).values\n",
    "        ends   = parts[2].astype(int).values\n",
    "        mids   = ((starts + ends) // 2).astype(int)\n",
    "        bed = pd.DataFrame({\n",
    "            \"#chr\":  chrs,\n",
    "            \"start\": mids,\n",
    "            \"end\":   mids + 1,\n",
    "            \"ID\":    peak_ids\n",
    "        })\n",
    "        bed = pd.concat([bed, residuals.reset_index(drop=True)], axis=1)\n",
    "        return bed.sort_values([\"#chr\", \"start\"]).reset_index(drop=True)\n",
    "\n",
    "    def run_cmd(cmd, label):\n",
    "        r = subprocess.run(cmd, capture_output=True)\n",
    "        if r.returncode != 0:\n",
    "            print(f\"WARNING: {label} failed: {r.stderr.decode()}\")\n",
    "        else:\n",
    "            print(f\"{label}: OK\")\n",
    "\n",
    "    out_dir = os.path.join(output_dir, \"3_pheno_reformat\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for res_path in residual_files:\n",
    "        ct = os.path.basename(os.path.dirname(res_path))\n",
    "\n",
    "        print(f\"\\n{'='*40}\\nPhenotype Formatting: {ct}\\n{'='*40}\")\n",
    "\n",
    "        if not os.path.exists(res_path):\n",
    "            print(f\"WARNING: {res_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        peak_ids, residuals = read_residuals(res_path)\n",
    "        print(f\"Loaded {len(peak_ids)} peaks x {residuals.shape[1]} samples\")\n",
    "\n",
    "        bed     = to_midpoint_bed(peak_ids, residuals)\n",
    "        out_bed = os.path.join(out_dir, f\"{ct}_phenotype.bed\")\n",
    "        bed.to_csv(out_bed, sep=\"\\t\", index=False, float_format=\"%.15f\")\n",
    "        print(f\"Written: {out_bed}\")\n",
    "\n",
    "        run_cmd([\"bgzip\", \"-f\", out_bed],                \"bgzip\")\n",
    "        run_cmd([\"tabix\", \"-p\", \"bed\", f\"{out_bed}.gz\"], \"tabix\")\n",
    "        print(f\"Completed: {ct} -> {out_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "sos",
     "",
     ""
    ]
   ],
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
