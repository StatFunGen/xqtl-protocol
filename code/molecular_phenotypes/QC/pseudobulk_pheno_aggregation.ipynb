{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Pseudobulk eqtl phenotype QC and normalization\n",
    "It is based on Nick's code. Should be optimized for general use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "The input is pseudo bulk eqtl phenotype data of seurat rds object. In this notebook, we use the following files as input:\n",
    "phenotype original file:\n",
    "- Ast: `/with_projids/Astrocytes.rds`\n",
    "- Immune_cells: `/with_projids/Immune_cells.rds`\n",
    "- Exc: `/with_projids/Excitatory_neurons_set1.rds`\n",
    "    `/with_projids/Excitatory_neurons_set2.rds`\n",
    "    `/with_projids/Excitatory_neurons_set3.rds`\n",
    "- Inh: `/with_projids/Inhibitory_neurons.rds`\n",
    "- Oli: `/with_projids/Oligodendrocytes.rds`\n",
    "- OPC: `/with_projids/OPCs.rds`\n",
    "\n",
    "For Ast, Inh, Oli, OPC, the input is separate seurat objects, each of a specific celltype. So we list the celltype name as 1st col, rds path as the 2nd col in a txt file as the input. It should use The first version--`seuratagg` workflow.\n",
    "    \n",
    "For Immune_cells and some celltypes, it is a combined rds objest with multiple celltypes or subtypes. We want to get one or some of the celltypes(subtypes) from the seurat object. So we list the celltypes(subtypes) name that we need as 1st col, rds path as the 2nd col in a txt file as the input. It should use The Second version--`subtypeagg` workflow.\n",
    "\n",
    "For Exc, it was split into multiple seurat objects, so handled separately. It should use The Third version--`neuronsagg` workflow.\n",
    "\n",
    "`FIXME: All of the sos workflow are based on projid, and in R code, the column name with pure number will be add a prefix 'g' to the projid, we should then delete the prefix 'g' to the projid. Need optimize the code for considering sampleid to avoid this step. `\n",
    "\n",
    "## Steps:\n",
    "-- Count Cells by Sample: Calculate the number of cells for each sample using metadata. This helps in filtering samples based on cell count.\n",
    "-- Aggregate Expression Data: Create pseudobulk data by aggregating raw count data per sample, enhancing signal-to-noise ratio for downstream analysis.\n",
    "-- Filter Samples: Exclude samples with fewer than 10 cells to ensure sufficient data quality and representativeness.\n",
    "-- Gene Filtering: Use `filterByExpr()` to retain genes with sufficient expression across samples, improving the reliability of statistical tests.\n",
    "-- Normalization: Apply TMM normalization to adjust for composition effects, making counts between samples comparable.\n",
    "-- Voom Transformation: Transform count data to log2-counts per million (logCPM), stabilizing variance across genes.\n",
    "-- Filter by Expression: Remove genes with mean log2CPM < 2.0 to focus on genes with significant expression levels.\n",
    "-- Quantile normalization: Apply quantile normalization to ensure that the distribution of expression values is consistent across samples.\n",
    "\n",
    "## Output\n",
    "\n",
    "The output is a normalized.log2cpm.tsv file, with 1st column id as gene name, then the projids as following columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Global parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sos"
    }
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = str\n",
    "parameter: cwd = path(\"output\")\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 5\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"20h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sos run pipeline/pseudobulk_pheno_aggregation.ipynb seuratagg \\\n",
    "    --name snuc_pseudo_bulk \\\n",
    "    --seurat_rds /home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/celltype.txt \\\n",
    "    --cwd /home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/data_after_aggregation/ \\\n",
    "    --container /home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/container/seurat.sif \\\n",
    "    --mem 40G -J 50 -c /home/al4225/project/quantile_qtl/csg.yml -q csg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Seurat rds aggregation \n",
    "### First version: for each seurat object with just one cell type.\n",
    "The input is a txt file with the first column as the cell type name and the second column as the seurat rds file path. The output is a normalized aggregated rds file for each cell type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sos"
    }
   },
   "outputs": [],
   "source": [
    "[seuratagg]\n",
    "import pandas as pd\n",
    "# load seurat_rds rds output file\n",
    "parameter: seurat_rds = path()\n",
    "\n",
    "\n",
    "#for each tissue.\n",
    "rds_result = pd.read_csv(seurat_rds, sep = \"\\t\", header=None)\n",
    "print(rds_result)\n",
    "input_inv = rds_result.values.tolist()\n",
    "tissue_id_inv = [x[0] for x in input_inv]\n",
    "file_inv = [x[1] for x in input_inv]\n",
    "print(\"\\ntissue ID List:\")\n",
    "print(tissue_id_inv)\n",
    "print(\"\\nFile List:\")\n",
    "print(file_inv)\n",
    "print(\"Length of tissue_id_inv:\", len(tissue_id_inv))\n",
    "print(\"Length of file_inv:\", len(file_inv))\n",
    "\n",
    "input: file_inv, group_by = 1, group_with = \"tissue_id_inv\"\n",
    "output: normalized_log2cpm = f'{cwd:a}/{name}.{_tissue_id_inv}.normalized.log2cpm.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "library(Seurat)\n",
    "library(edgeR)\n",
    "library(limma)\n",
    "\n",
    "#loading separate seurat objects, each of a specific celltype\n",
    "seu = readRDS(${_input:r})\n",
    "\n",
    "#keep cell counts for sample filtering, (sample must be in metadata under 'sample')\n",
    "cellcounts=table(seu@meta.data$projid)\n",
    "seu=SetIdent(seu,value=\"projid\")\n",
    "\n",
    "#creation of the raw count pseudobulk\n",
    "expr=AggregateExpression(seu,group.by=\"projid\",slot=\"counts\")$RNA\n",
    "\n",
    "# delete the g prefix of colname: only for projids version.\n",
    "colnames(expr) <- gsub(\"^g\", \"\", colnames(expr))\n",
    "\n",
    "#filtering out samples with fewer than 10 cells in a celltype\n",
    "sampnames=names(cellcounts[cellcounts>9])\n",
    "expr=expr[,sampnames]\n",
    "\n",
    "\n",
    "#filter low expression genes\n",
    "y <- DGEList(counts = expr)\n",
    "keep <- filterByExpr(y)\n",
    "y <- y[keep,,keep.lib.sizes=F]\n",
    "\n",
    "\n",
    "#counts per million\n",
    "y <- calcNormFactors(y, method = \"TMM\")\n",
    "v <- voom(y, plot=F)\n",
    "logcpm <- v$E\n",
    "\n",
    "# remove genes if mean log2CPM < 2.0\n",
    "mean_logcpm <- apply(logcpm, 1, mean)\n",
    "logcpm <- logcpm[mean_logcpm > 2.0,]\n",
    "\n",
    "logcpm <- as.data.frame(logcpm)\n",
    "logcpm$id <- rownames(logcpm)\n",
    "rownames(logcpm) <- NULL  #the rownames are now in the id column\n",
    "logcpm <- logcpm[, c(\"id\", setdiff(names(logcpm), \"id\"))]\n",
    "\n",
    "# convert log2CPM to matrix\n",
    "logcpm_id <- logcpm$id\n",
    "logcpm <- as.matrix(logcpm[, colnames(logcpm) != \"id\"])\n",
    "rownames(logcpm) <- logcpm_id\n",
    "\n",
    "# quantile normalizarion\n",
    "logcpm <- t(apply(logcpm, 1, rank, ties.method = \"average\"))\n",
    "logcpm <- qnorm(logcpm / (ncol(logcpm) + 1))\n",
    "\n",
    "# export\n",
    "df <- data.frame(id = rownames(logcpm), logcpm, check.names = F)\n",
    "write.table(df, file=\"${_output['normalized_log2cpm']}\", sep=\"\\t\", quote = F, row.names = F)\n",
    "cat(\"the normalized aggregated pseudo_bulk_eqtl tsv are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second version for a seurat object with multiple celltypes or subtypes.\n",
    "If you are loading a seurat object with multiple celltypes or subtypes (in metadata) to run pseudobulk, you can use this command to subtract the celltype of interest from the rest of the cells.\n",
    "\n",
    "You should check the colnames of the metadata of the seurat object to make sure the celltype column name is correct. In this code, it uses `predicted.id` col as the subtypes name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example:\n",
    "sos run pipeline/pseudobulk_pheno_aggregation.ipynb subtypeagg \\\n",
    "    --name snuc_pseudo_bulk \\\n",
    "    --seurat_rds /mnt/vast/hpc/homes/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/data_before_aggregate/subtype/mic_subtypes.txt \\\n",
    "    --cwd /mnt/vast/hpc/homes/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/phenodata_quantnorm_nofill0/subtype/ \\\n",
    "    --container /home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/container/seurat.sif \\\n",
    "    --mem 60G -J 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sos"
    }
   },
   "outputs": [],
   "source": [
    "[subtypeagg]\n",
    "import pandas as pd\n",
    "# load seurat rds output file\n",
    "parameter: seurat_rds = path()\n",
    "\n",
    "\n",
    "#for each tissue.\n",
    "rds_result = pd.read_csv(seurat_rds, sep = \"\\t\", header=None)\n",
    "print(rds_result)\n",
    "input_inv = rds_result.values.tolist()\n",
    "tissue_id_inv = [x[0] for x in input_inv]\n",
    "file_inv = [x[1] for x in input_inv]\n",
    "print(\"\\ntissue ID List:\")\n",
    "print(tissue_id_inv)\n",
    "print(\"\\nFile List:\")\n",
    "print(file_inv)\n",
    "print(\"Length of tissue_id_inv:\", len(tissue_id_inv))\n",
    "print(\"Length of file_inv:\", len(file_inv))\n",
    "\n",
    "input: file_inv, group_by = 1, group_with = \"tissue_id_inv\"\n",
    "output: normalized_log2cpm = f'{cwd:a}/{name}.{_tissue_id_inv}.normalized.log2cpm.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "library(Seurat)\n",
    "library(edgeR)\n",
    "library(limma)\n",
    "\n",
    "seu_all=readRDS(${_input:r})\n",
    "seu_all=SetIdent(seu_all,value=\"predicted.id\") #this is the subtype/celltype column name.\n",
    "ct='${_tissue_id_inv}'\n",
    "seu=subset(seu_all,idents=ct)\n",
    "\n",
    "#keep cell counts for sample filtering, (sample must be in metadata under 'sample')\n",
    "cellcounts=table(seu@meta.data$projid)\n",
    "seu=SetIdent(seu,value=\"projid\")\n",
    "\n",
    "#creation of the raw count pseudobulk\n",
    "expr=AggregateExpression(seu,group.by=\"projid\",slot=\"counts\")$RNA\n",
    "\n",
    "# delete the g prefix of colname\n",
    "colnames(expr) <- gsub(\"^g\", \"\", colnames(expr))\n",
    "\n",
    "#filtering out samples with fewer than 10 cells in a celltype\n",
    "sampnames=names(cellcounts[cellcounts>9])\n",
    "expr=expr[,sampnames]\n",
    "\n",
    "#filter low expression genes\n",
    "y <- DGEList(counts = expr)\n",
    "keep <- filterByExpr(y)\n",
    "y <- y[keep,,keep.lib.sizes=F]\n",
    "\n",
    "#counts per million\n",
    "y <- calcNormFactors(y, method = \"TMM\")\n",
    "\n",
    "v <- voom(y, plot=F)\n",
    "logcpm <- v$E\n",
    "\n",
    "# remove genes if mean log2CPM < 2.0\n",
    "mean_logcpm <- apply(logcpm, 1, mean)\n",
    "logcpm <- logcpm[mean_logcpm > 2.0,]\n",
    "\n",
    "logcpm <- as.data.frame(logcpm)\n",
    "logcpm$id <- rownames(logcpm)\n",
    "rownames(logcpm) <- NULL\n",
    "logcpm <- logcpm[, c(\"id\", setdiff(names(logcpm), \"id\"))]\n",
    "\n",
    "# convert log2CPM to matrix\n",
    "logcpm_id <- logcpm$id\n",
    "logcpm <- as.matrix(logcpm[, colnames(logcpm) != \"id\"])\n",
    "rownames(logcpm) <- logcpm_id\n",
    "\n",
    "# quantile normalizarion\n",
    "logcpm <- t(apply(logcpm, 1, rank, ties.method = \"average\"))\n",
    "logcpm <- qnorm(logcpm / (ncol(logcpm) + 1))\n",
    "\n",
    "# export\n",
    "df <- data.frame(id = rownames(logcpm), logcpm, check.names = F)\n",
    "write.table(df, file=\"${_output['normalized_log2cpm']}\", sep=\"\\t\", quote = F, row.names = F)\n",
    "\n",
    "cat(\"the normalized aggregated pseudo_bulk_eqtl tsv are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sos run pipeline/pseudobulk_pheno_aggregation.ipynb neuronsagg \\\n",
    "    --name snuc_pseudo_bulk \\\n",
    "    --cwd /home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/data_after_aggregation/exc/ \\\n",
    "    --container /home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/container/seurat.sif \\\n",
    "    --mem 100 -J 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The third version for a Celltype with multiple seurat object. \n",
    "e.g.neurons were split into two or more files, so handled separately. This is an example of how to aggregate 3 Exc files together.\n",
    "\n",
    "`FIXME: The sos workflow is not generalized and you should optimizing the code.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sos"
    }
   },
   "outputs": [],
   "source": [
    "[neuronsagg]\n",
    "import pandas as pd\n",
    "output: normalized_log2cpm = f'{cwd:a}/{name}.{_tissue_id_inv}.normalized.log2cpm.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "library(Seurat)\n",
    "library(edgeR)\n",
    "library(limma)\n",
    "\n",
    "# Neurons were split into three files, so handled separately\n",
    "seu1 = readRDS(\"/home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/data_before_aggregate/Excitatory_neurons_set1.rds\")\n",
    "cellcounts1 = table(seu1@meta.data$projid)\n",
    "seu1 = SetIdent(seu1, value = \"projid\")\n",
    "expr1 = AggregateExpression(seu1, group.by = \"projid\", slot = \"counts\")$RNA\n",
    "\n",
    "seu2 = readRDS(\"/home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/data_before_aggregate/Excitatory_neurons_set2.rds\")\n",
    "cellcounts2 = table(seu2@meta.data$projid)\n",
    "seu2 = SetIdent(seu2, value = \"projid\")\n",
    "expr2 = AggregateExpression(seu2, group.by = \"projid\", slot = \"counts\")$RNA\n",
    "\n",
    "seu3 = readRDS(\"/home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/data_before_aggregate/Excitatory_neurons_set3.rds\")\n",
    "cellcounts3 = table(seu3@meta.data$projid)\n",
    "seu3 = SetIdent(seu3, value = \"projid\")\n",
    "expr3 = AggregateExpression(seu3, group.by = \"projid\", slot = \"counts\")$RNA\n",
    "\n",
    "ct = \"neuron\"\n",
    "genes1 = rownames(expr1)\n",
    "genes2 = rownames(expr2)\n",
    "genes3 = rownames(expr3)\n",
    "\n",
    "# Find common genes among all three sets\n",
    "common_genes = Reduce(intersect, list(genes1, genes2, genes3))\n",
    "\n",
    "# Filter the expression matrices to keep only common genes\n",
    "expr1 = expr1[common_genes, ]\n",
    "expr2 = expr2[common_genes, ]\n",
    "expr3 = expr3[common_genes, ]\n",
    "\n",
    "# Combine the expression matrices horizontally (by columns)\n",
    "expr = cbind(expr1, expr2, expr3)\n",
    "\n",
    "# delete the g prefix of colname\n",
    "colnames(expr) <- gsub(\"^g\", \"\", colnames(expr))\n",
    "\n",
    "# Remove unnecessary objects from memory\n",
    "rm(expr1, expr2, expr3, seu1, seu2, seu3)\n",
    "\n",
    "# Combine cell counts from all three sets\n",
    "cellcounts = c(cellcounts1, cellcounts2, cellcounts3)\n",
    "\n",
    "#filtering out samples with fewer than 10 cells in a celltype\n",
    "sampnames=names(cellcounts[cellcounts>9])\n",
    "expr=expr[,sampnames]\n",
    "\n",
    "#filter low expression genes\n",
    "y <- DGEList(counts = expr)\n",
    "keep <- filterByExpr(y)\n",
    "y <- y[keep,,keep.lib.sizes=F]\n",
    "\n",
    "\n",
    "#counts per million\n",
    "y <- calcNormFactors(y, method = \"TMM\")\n",
    "v <- voom(y, plot=F)\n",
    "logcpm <- v$E\n",
    "\n",
    "# remove genes if mean log2CPM < 2.0\n",
    "mean_logcpm <- apply(logcpm, 1, mean)\n",
    "logcpm <- logcpm[mean_logcpm > 2.0,]\n",
    "\n",
    "logcpm <- as.data.frame(logcpm)\n",
    "logcpm$id <- rownames(logcpm)\n",
    "rownames(logcpm) <- NULL\n",
    "logcpm <- logcpm[, c(\"id\", setdiff(names(logcpm), \"id\"))]\n",
    "\n",
    "# convert log2CPM to matrix\n",
    "logcpm_id <- logcpm$id\n",
    "logcpm <- as.matrix(logcpm[, colnames(logcpm) != \"id\"])\n",
    "rownames(logcpm) <- logcpm_id\n",
    "\n",
    "# quantile normalizarion\n",
    "logcpm <- t(apply(logcpm, 1, rank, ties.method = \"average\"))\n",
    "logcpm <- qnorm(logcpm / (ncol(logcpm) + 1))\n",
    "\n",
    "# export\n",
    "df <- data.frame(id = rownames(logcpm), logcpm, check.names = F)\n",
    "write.table(df, file=\"${_output['normalized_log2cpm']}\", sep=\"\\t\", quote = F, row.names = F)\n",
    "\n",
    "cat(\"the normalized aggregated pseudo_bulk_eqtl tsv are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplement: Code for fill projid into length 8 as prefix 0\n",
    "Python code. The example uses projid insdead of sampleid, and the length of projid number should be filled into 8 with 0 as the prefix to match the projid--sample list. So after aggregatiom, should processed the column name in the tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Python code. The example uses projid insdead of sampleid, and the length of projid number should be filled into 8 to match \n",
    "the projid--sample list. So after aggregatiom, should processed the column name in the tsv file.\n",
    "# the log2CPM tsv file: 1st col is id, the rest are projids.\n",
    "# fill projid to 8\n",
    "import pandas as pd\n",
    "\n",
    "def pad_column_names(df, pad_length=8):\n",
    "    new_cols = [df.columns[0]] + [col if len(col) == pad_length or not col.isdigit() else col.zfill(pad_length) for col in df.columns[1:]]\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "file_path = '/The input path/phenodata_quantnorm_nofill0/snuc_pseudo_bulk.Ast.normalized.log2cpm.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "df = pad_column_names(df)\n",
    "\n",
    "output_file_path = 'The output path/For your tsv data'\n",
    "df.to_csv(output_file_path, sep='\\t', index=False, quotechar='', quoting=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (my_kernel)",
   "language": "python",
   "name": "my_python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
