{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a44eb1-acb9-40f5-bcdb-7ede63d5db5e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Single-nucleus ATAC-seq Preprocessing Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline preprocesses single-nucleus ATAC-seq (snATAC-seq) pseudobulk peak count data\n",
    "for downstream chromatin accessibility QTL (caQTL) analysis and region-specific studies.\n",
    "\n",
    "**Goals:**\n",
    "- Transform raw pseudobulk peak counts into analysis-ready formats\n",
    "- Remove technical confounders while optionally preserving biological covariates\n",
    "- Generate QTL-ready phenotype files or region-specific datasets\n",
    "\n",
    "## Pipeline Structure\n",
    "```\n",
    "Step 0: Sample ID Mapping\n",
    "↓\n",
    "Step 1: Pseudobulk QC\n",
    "├── Option A: BIOvar (regress out technical + biological covariates)\n",
    "└── Option B: noBIOvar (regress out technical covariates only)\n",
    "↓ (optional)\n",
    "Batch Correction (ComBat-seq or limma::removeBatchEffect)\n",
    "↓\n",
    "Step 2: Format Output\n",
    "├── Format A: Phenotype Reformatting → BED (genome-wide caQTL mapping)\n",
    "└── Format B: Region Peak Filtering  → TSV (locus-specific analysis)\n",
    "\n",
    "```\n",
    "\n",
    "## Input Files\n",
    "\n",
    "All input files required to run this pipeline can be downloaded\n",
    "[here](https://drive.google.com/drive/folders/1UzJuHN8SotMn-PJTBp9uGShD25YxapKr?usp=drive_link).\n",
    "\n",
    "| File | Used in |\n",
    "|------|---------|\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` | Step 0, Step 1 |\n",
    "| `metadata_{celltype}.csv` | Step 0, Step 1 |\n",
    "| `rosmap_sample_mapping_data.csv` | Step 0 |\n",
    "| `rosmap_cov.txt` | Step 1 |\n",
    "| `hg38-blacklist.v2.bed.gz` | Step 1 |\n",
    "| `SampleSheet.csv` | Step 1 (batch correction only) |\n",
    "| `sampleSheetAfterQc.csv` | Step 1 (batch correction only) |\n",
    "\n",
    "\n",
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e13a3f-ab64-4bd1-b47c-acca8d58a8b9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 0: Sample ID Mapping\n",
    "\n",
    "Maps original sample identifiers (`individualID`) to standardized sample IDs (`sampleid`)\n",
    "across metadata and count matrix files.\n",
    "\n",
    "### Input\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `rosmap_sample_mapping_data.csv` | Mapping reference: `individualID → sampleid` |\n",
    "| `metadata_{celltype}.csv` × 6 | Per-cell-type sample metadata |\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` × 6 | Per-cell-type peak count matrices |\n",
    "\n",
    "Cell types: `Ast`, `Ex`, `In`, `Microglia`, `Oligo`, `OPC`\n",
    "\n",
    "### Process\n",
    "\n",
    "**Part 1 — Metadata files**\n",
    "\n",
    "For each `metadata_{celltype}.csv`:\n",
    "1. Look up each `individualID` in the mapping reference\n",
    "2. Assign `sampleid` — falls back to `individualID` if no mapping found\n",
    "3. Insert `sampleid` as the first column\n",
    "4. Save updated file\n",
    "\n",
    "**Part 2 — Count matrix files**\n",
    "\n",
    "For each `pseudobulk_peaks_counts_{celltype}.csv.gz`:\n",
    "1. Extract the header row (column names only)\n",
    "2. Keep `peak_id` (first column) unchanged\n",
    "3. Map remaining column names (`individualID` → `sampleid`) where mapping exists,\n",
    "   otherwise keep original\n",
    "4. Write new header and stream data rows unchanged\n",
    "5. Recompress with gzip\n",
    "\n",
    "### Output\n",
    "\n",
    "Output directory: `output/1_files_with_sampleid/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `metadata_{celltype}.csv` × 6 | Metadata with `sampleid` column prepended |\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` × 6 | Count matrices with mapped column headers |\n",
    "\n",
    "**Timing:** < 1 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7cbe0-bf5e-4d7e-8a2b-216915dea78e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/snatacseq_preprocessing.ipynb sampleid_mapping \\\n",
    "    --cwd output/atac_seq/1_files_with_sampleid \\\n",
    "    --map_file data/atac_seq/rosmap_sample_mapping_data.csv \\\n",
    "    --input_dir data/atac_seq/1_files_with_sampleid_xiong \\\n",
    "    --output_dir output/atac_seq/1_files_with_sampleid \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC\n",
    "\n",
    "\n",
    "# For MIT input data\n",
    "sos run pipeline/snatacseq_preprocessing.ipynb sampleid_mapping \\\n",
    "    --cwd output/atac_seq/1_files_with_sampleid  \\\n",
    "    --map_file data/atac_seq/rosmap_sample_mapping_data.csv \\\n",
    "    --input_dir data/atac_seq/1_files_with_sampleid_MIT \\\n",
    "    --output_dir output/atac_seq/1_files_with_sampleid \\\n",
    "    --celltype Astro Exc Inh Mic Oligo OPC \\\n",
    "    --suffix _50nuc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540a4da-843a-4789-8123-47911cf519c5",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 1: Pseudobulk QC\n",
    "\n",
    "Two approaches are available depending on whether biological covariates should be regressed out.\n",
    "Both options support an **optional batch correction** step after filtering and normalization.\n",
    "\n",
    "\n",
    "### Option A: With Biological Covariates (BIOvar)\n",
    "\n",
    "Use when residuals should be adjusted for all technical **and** biological covariates (sex, age, PMI).\n",
    "\n",
    "**Input:**\n",
    "\n",
    "| File | Location |\n",
    "|------|----------|\n",
    "| `pseudobulk_peaks_counts_{celltype}.csv.gz` | `1_files_with_sampleid/` |\n",
    "| `metadata_{celltype}.csv` | `1_files_with_sampleid/` |\n",
    "| `rosmap_cov.txt` | `data/` |\n",
    "| `hg38-blacklist.v2.bed.gz` | `data/` |\n",
    "| `SampleSheet.csv` *(batch correction only)* | `data/` |\n",
    "| `sampleSheetAfterQc.csv` *(batch correction only)* | `data/` |\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Load pseudobulk peak count matrix and metadata per cell type\n",
    "2. Filter samples with fewer than 20 nuclei\n",
    "3. Calculate technical QC metrics per sample:\n",
    "   - `log_n_nuclei`: log-transformed nuclei count\n",
    "   - `med_nucleosome_signal`: median nucleosome signal\n",
    "   - `med_tss_enrich`: median TSS enrichment score\n",
    "   - `log_med_n_tot_fragment`: log-transformed median total fragments\n",
    "   - `log_total_unique_peaks`: log-transformed unique peak count\n",
    "4. Filter blacklisted genomic regions\n",
    "5. Merge with demographic covariates (`msex`, `age_death`, `pmi`, `study`)\n",
    "6. Apply expression filtering (`filterByExpr`):\n",
    "   - `min_count = 5`: minimum reads in at least one sample\n",
    "   - `min_total_count = 15`: minimum total reads across all samples\n",
    "   - `min_prop = 0.1`: peak expressed in ≥10% of samples\n",
    "7. TMM normalization\n",
    "8. *(Optional)* Batch correction — see [Batch Correction](#batch-correction-optional) below\n",
    "9. Fit linear model (`voom` + `lmFit`):~ log_n_nuclei + med_nucleosome_signal + med_tss_enrich\n",
    "\n",
    "log_med_n_tot_fragment + log_total_unique_peaks\n",
    "sequencingBatch + msex + age_death + pmi + study\n",
    "\n",
    "   > If batch correction was applied, `sequencingBatch` is removed from the model.\n",
    "10. Compute residuals adjusted for all covariates\n",
    "11. Compute final adjusted values: `offset + residuals`\n",
    "    - `offset`: predicted expression at median/reference covariate values\n",
    "    - `residuals`: unexplained variation after removing all covariate effects\n",
    "\n",
    "**Output:** `output/2_residuals/{celltype}/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_residuals.txt` | Covariate-adjusted peak accessibility (log2-CPM) |\n",
    "| `{celltype}_results.rds` | Full results: DGEList, fit, offset, residuals, design |\n",
    "| `{celltype}_filtered_raw_counts.txt` | Filtered raw counts before normalization |\n",
    "| `{celltype}_summary.txt` | Filtering statistics and QC summary |\n",
    "\n",
    "**Covariates regressed out:**\n",
    "- Technical: sequencing depth, nuclei count, nucleosome signal, TSS enrichment, batch\n",
    "- Biological: sex (`msex`), age at death (`age_death`), post-mortem interval (`pmi`), study cohort\n",
    "\n",
    "### Option B: Without Biological Covariates (noBIOvar)\n",
    "\n",
    "Use when biological variation should be preserved (e.g., age/sex comparisons, region-specific analyses).\n",
    "\n",
    "**Input:** Same as Option A.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "Steps 1–8 are identical to Option A. Key differences at the modelling stage:\n",
    "- `msex` and `age_death` are **excluded** from the model\n",
    "- `med_peakwidth` (weighted median peak width per sample) is added as a technical covariate\n",
    "\n",
    "**Model formula:**\n",
    "```\n",
    "Model: ~ log_n_nuclei + med_nucleosome_signal + med_tss_enrich + log_med_n_tot_fragment + log_total_unique_peaks + med_peakwidth + sequencingBatch + pmi + study\n",
    "```\n",
    "\n",
    "**Output:** `output/2_residuals/{celltype}/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_residuals.txt` | Covariate-adjusted peak accessibility (log2-CPM) |\n",
    "| `{celltype}_results.rds` | Full results: DGEList, fit, offset, residuals, design |\n",
    "| `{celltype}_filtered_raw_counts.txt` | Filtered raw counts before normalization |\n",
    "| `{celltype}_summary.txt` | Filtering statistics and QC summary |\n",
    "\n",
    "**Variables deliberately NOT regressed out:**\n",
    "- Sex (`msex`)\n",
    "- Age at death (`age_death`)\n",
    "\n",
    "**Timing:** <5 min per celltype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f80085-6d2c-4e1c-af35-454382d94de1",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Pseudobulk QC with BIOVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569d816-d292-4512-85b6-fcd3ea1c9ba7",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/snatacseq_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --cwd output/atac_seq \\\n",
    "    --input_dir output/atac_seq/1_files_with_sampleid_xiong \\\n",
    "    --output_dir output/atac_seq/2_residuals \\\n",
    "    --blacklist_file data/atac_seq/hg38-blacklist.v2.bed.gz \\\n",
    "    --covariates_file data/atac_seq/rosmap_cov.txt \\\n",
    "    --include_bio TRUE \\\n",
    "    --batch_correction FALSE \\\n",
    "    --min_count 5 \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cbd39c-60f8-4e21-9915-14b30ebc02cd",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Pseudobulk QC noBIOvar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741ac2e-91b3-49e7-906a-2fd6b8d8d137",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/snatacseq_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --cwd output/atac_seq \\\n",
    "    --input_dir output/atac_seq/1_files_with_sampleid_xiong \\\n",
    "    --output_dir output/atac_seq/2_residuals \\\n",
    "    --blacklist_file data/atac_seq/hg38-blacklist.v2.bed.gz \\\n",
    "    --covariates_file data/atac_seq/rosmap_cov.txt \\\n",
    "    --include_bio FALSE \\\n",
    "    --batch_correction FALSE \\\n",
    "    --min_count 5 \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e96ad2-1b75-43d0-978e-0757bc11f135",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Batch Correction (Optional)\n",
    "\n",
    "Applies to both Option A and Option B. Runs between TMM normalization and model fitting.\n",
    "Use when batch effects are severe (e.g., visible batch clusters in PCA, multiple sequencing runs).\n",
    "\n",
    "> When batch correction is applied, `sequencingBatch` is **removed** from the model formula\n",
    "> since batch variance has already been removed from the counts.\n",
    "\n",
    "**Method comparison:**\n",
    "\n",
    "| | ComBat-seq | limma `removeBatchEffect` |\n",
    "|---|---|---|\n",
    "| **Operates on** | Raw integer counts | log-CPM values |\n",
    "| **Mean-variance modelling** | Yes | No |\n",
    "| **Best for** | Large, balanced batches | Small or fragmented batches |\n",
    "| **Robustness** | May fail with many small batches | More robust to unbalanced designs |\n",
    "\n",
    "**ComBat-seq:**\n",
    "```r\n",
    "adjusted_counts <- ComBat_seq(counts = dge$counts, batch = batches)\n",
    "```\n",
    "\n",
    "**limma `removeBatchEffect`:**\n",
    "```r\n",
    "logCPM <- cpm(dge, log = TRUE, prior.count = 1)\n",
    "adj_logCPM  <- removeBatchEffect(logCPM, batch = batches, design = model.matrix(~1, data = dge$samples))\n",
    "adjusted_counts <- round(pmax(2^adj_logCPM * mean(dge$samples$lib.size) / 1e6, 0))\n",
    "```\n",
    "\n",
    "**Additional filtering applied before correction:**\n",
    "- Singleton batches (only 1 sample) are removed\n",
    "- Samples absent from the batch sheet are dropped\n",
    "\n",
    "**Additional output when batch correction is enabled:**\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_results.rds` | Includes `batch_adjusted_counts` and `batch_method` fields |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d582c85-2265-46ee-8080-0ec5d8423a1d",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Pseudobulk QC with BIOvar & with batch correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3676870-496d-4379-8d6b-acec08f1c0d7",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = input): <text>:1:5: unexpected symbol\n1: sos run\n        ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = input): <text>:1:5: unexpected symbol\n1: sos run\n        ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "sos run pipeline/snatacseq_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --cwd output/atac_seq \\\n",
    "    --input_dir output/atac_seq/1_files_with_sampleid_xiong \\\n",
    "    --output_dir output/atac_seq/2_residuals \\\n",
    "    --blacklist_file data/atac_seq/hg38-blacklist.v2.bed.gz \\\n",
    "    --covariates_file data/atac_seq/rosmap_cov.txt \\\n",
    "    --include_bio TRUE \\\n",
    "    --batch_correction TRUE \\\n",
    "    --batch_method limma \\\n",
    "    --min_count 2\n",
    "    --celltype Ast Ex In Microglia Oligo OPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad900d-768d-45ee-815a-6847e8eba32e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Pseudobulk QC noBIOvar & with batch correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0faa-4dd9-431d-a5cf-3e92d7256a3b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/snatacseq_preprocessing.ipynb pseudobulk_qc \\\n",
    "    --cwd output/atac_seq \\\n",
    "    --input_dir output/atac_seq/1_files_with_sampleid_xiong \\\n",
    "    --output_dir output/atac_seq/2_residuals \\\n",
    "    --blacklist_file data/atac_seq/hg38-blacklist.v2.bed.gz \\\n",
    "    --covariates_file data/atac_seq/rosmap_cov.txt \\\n",
    "    --include_bio FALSE \\\n",
    "    --batch_correction TRUE \\\n",
    "    --batch_method limma \\\n",
    "    --min_count 5\n",
    "    --celltype Ast Ex In Microglia Oligo OPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f2b32-e80d-472b-9af8-5f3d4ebb9bf2",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**Note**\n",
    "For MIT data, add these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee860bb3-d628-4255-b222-f62b3c03a91a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "--celltype Astro Exc Inh Mic Oligo OPC \\\n",
    "--suffix _50nuc \\\n",
    "--input_dir output/1_files_with_sampleid_MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ea976-ca30-48e4-811b-eaa0b5f246ed",
   "metadata": {},
   "source": [
    "For additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b41a7f-1d08-4174-858b-a0593aaadcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "--min_count 5\n",
    "--min_total_count 15\n",
    "--min_prop 0.1\n",
    "--min_nuclei 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d5b04-ec21-4c0c-8879-d78563d5ed96",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 2: Format Output\n",
    "### Phenotype Reformatting\n",
    "\n",
    "Converts residuals into a QTL-ready BED format for genome-wide caQTL mapping.\n",
    "\n",
    "**Input:**\n",
    "\n",
    "| File | Location |\n",
    "|------|----------|\n",
    "| `{celltype}_residuals.txt` | `output/2_residuals/{celltype}/` |\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Read residuals file with proper handling of peak IDs and sample columns\n",
    "2. Parse peak coordinates from peak IDs (`chr-start-end` format)\n",
    "3. Convert to midpoint coordinates (standard for QTLtools):\n",
    "```\n",
    "   start = floor((peak_start + peak_end) / 2)\n",
    "   end = start + 1\n",
    "```\n",
    "4. Build BED format: `#chr`, `start`, `end`, `ID` followed by per-sample expression values\n",
    "5. Sort by chromosome and position\n",
    "6. Compress with `bgzip` and index with `tabix`\n",
    "\n",
    "**Output:** `output/3_phenotype_processing/phenotype/{celltype}_snatac_phenotype.bed.gz`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_snatac_phenotype.bed.gz` | bgzip-compressed BED with peak midpoint coordinates |\n",
    "| `{celltype}_snatac_phenotype.bed.gz.tbi` | tabix index for random-access queries |\n",
    "\n",
    "**Use case:** Standard caQTL mapping to identify genetic variants affecting chromatin\n",
    "accessibility independent of demographic factors. Compatible with FastQTL, TensorQTL, and QTLtools.\n",
    "\n",
    "**Timing:** <1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf9d56-b53b-4a6a-b0af-b5a5fb98907b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/snatacseq_preprocessing.ipynb phenotype_formatting \\\n",
    "    --cwd output/atac_seq \\\n",
    "    --input_dir output/atac_seq/2_residuals \\\n",
    "    --output_dir output/atac_seq/3_pheno_reformat \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c874b17-9a77-4e7d-a0a3-3605f7005148",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Region Peak Filtering\n",
    "\n",
    "Filters peak counts to specific genomic regions of interest for locus-specific analysis.\n",
    "\n",
    "**Input:**\n",
    "\n",
    "| File | Location |\n",
    "|------|----------|\n",
    "| `{celltype}_filtered_raw_counts.txt` | `output/2_residuals/{celltype}/` |\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Read filtered raw counts per cell type\n",
    "2. Parse peak coordinates from peak IDs (`chr-start-end` format)\n",
    "3. Calculate per-peak metrics:\n",
    "   - `peakwidth`: `end - start`\n",
    "   - `midpoint`: `(start + end) / 2`\n",
    "4. Filter peaks overlapping target regions (includes peaks that start, end, or span boundaries):\n",
    "\n",
    "   | Region | Coordinates | Size |\n",
    "   |--------|-------------|------|\n",
    "   | Chr7   | 28,000,000 – 28,300,000 bp | 300 kb |\n",
    "   | Chr11  | 85,050,000 – 86,200,000 bp | 1.15 Mb |\n",
    "\n",
    "5. Calculate summary statistics per peak:\n",
    "   - `total_count`: sum of counts across all samples\n",
    "   - `weighted_count`: `total_count / peakwidth` (normalizes for peak size)\n",
    "\n",
    "**Output:** `output/3_format_output/regions/{celltype}/`\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `{celltype}_filtered_regions.txt` | Full count matrix for peaks in target regions |\n",
    "| `{celltype}_filtered_regions_summary.txt` | Peak metadata with coordinates and count statistics |\n",
    "\n",
    "**Use case:** Hypothesis-driven analysis of specific genomic loci (e.g., AD risk loci such as\n",
    "the APOE or TREM2 regions) where biological variation is preserved for downstream interpretation.\n",
    "\n",
    "**Timing:** <1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944afdd-fffc-4b56-863f-eee89408cfa1",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/snatacseq_preprocessing.ipynb region_filtering \\\n",
    "    --cwd output/atac_seq \\\n",
    "    --input_dir output/atac_seq/2_residuals \\\n",
    "    --output_dir output/atac_seq/3_region_filter \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10440301-99c6-4f0e-b6ce-efe5ac9281fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom regions\n",
    "sos run pipeline/snatacseq_preprocessing.ipynb region_filtering \\\n",
    "    --cwd output/atac_seq \\\n",
    "    --input_dir output/atac_seq/2_residuals \\\n",
    "    --output_dir output/atac_seq \\\n",
    "    --celltype Ast Ex In Microglia Oligo OPC \\\n",
    "    --regions \"chr1:1000000-2000000,chr5:50000000-51000000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a676801-6845-4ca5-944b-7978a5ecbb1f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486664a9-55c2-4738-91a0-b63ffdcd6cfa",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/snatacseq_preprocessing.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17a301-cca9-49a1-843b-4248546f1f79",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57fe47-f2ca-4a6e-8789-f7dbe3a9fad2",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Output directory\n",
    "parameter: cwd = path(\"output\")\n",
    "# For cluster jobs, number of commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container\n",
    "parameter: container = \"\"\n",
    "\n",
    "import re\n",
    "parameter: entrypoint = (\n",
    "    'micromamba run -a \"\" -n' + ' ' +\n",
    "    re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])\n",
    ") if container else \"\"\n",
    "\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6024cd-28be-4fb0-994e-0460e3a3beae",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `sampleid_mapping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1b7c0-2819-45d1-b2ce-8d117a6cc9eb",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sampleid_mapping]\n",
    "parameter: map_file   = str\n",
    "parameter: input_dir  = str\n",
    "parameter: output_dir = str\n",
    "parameter: celltype   = ['Ast', 'Ex', 'In', 'Microglia', 'Oligo', 'OPC']\n",
    "parameter: suffix     = ''   # e.g. '' for Xiong, '_50nuc' for Kellis\n",
    "\n",
    "input:  [f'{input_dir}/metadata_{ct}{suffix}.csv' for ct in celltype]\n",
    "output: [f'{output_dir}/metadata_{ct}{suffix}.csv' for ct in celltype]\n",
    "\n",
    "python: expand = \"${ }\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import gzip\n",
    "    import os\n",
    "    import subprocess\n",
    "    import csv\n",
    "    import numpy as np\n",
    "\n",
    "    map_df = pd.read_csv(\"${map_file}\")\n",
    "    id_map = dict(zip(map_df[\"individualID\"], map_df[\"sampleid\"]))\n",
    "\n",
    "    celltype   = ${celltype}\n",
    "    input_dir  = \"${input_dir}\"\n",
    "    output_dir = \"${output_dir}/1_files_with_sampleid\"\n",
    "    suffix     = \"${suffix}\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def map_id(ind_id):\n",
    "        return id_map.get(ind_id, ind_id)\n",
    "    \n",
    "    def format_value(val):\n",
    "        \"\"\"Format numeric values: remove .0 from integers, keep decimals\"\"\"\n",
    "        if pd.isna(val):\n",
    "            return ''\n",
    "        if isinstance(val, (int, np.integer)):\n",
    "            return str(val)\n",
    "        if isinstance(val, (float, np.floating)):\n",
    "            if val == int(val):  # Check if it's a whole number\n",
    "                return str(int(val))\n",
    "            else:\n",
    "                return str(val)\n",
    "        return str(val)\n",
    "\n",
    "    # ── Process metadata CSV files ────────────────────────────────────────────\n",
    "    for ct in celltype:\n",
    "        fname    = f\"metadata_{ct}{suffix}.csv\"\n",
    "        in_path  = os.path.join(input_dir,  fname)\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "\n",
    "        if not os.path.exists(in_path):\n",
    "            print(f\"Warning: Metadata file not found: {in_path}\")\n",
    "            continue\n",
    "\n",
    "        meta = pd.read_csv(in_path)\n",
    "\n",
    "        if \"individualID\" not in meta.columns:\n",
    "            print(f\"Warning: individualID column not found in {fname}\")\n",
    "            continue\n",
    "\n",
    "        # Create or update sampleid column\n",
    "        meta[\"sampleid\"] = meta[\"individualID\"].map(map_id)\n",
    "        \n",
    "        # Always reorder: sampleid FIRST, then individualID, then rest\n",
    "        cols = meta.columns.tolist()\n",
    "        cols.remove(\"sampleid\")\n",
    "        cols.remove(\"individualID\")\n",
    "        new_cols = [\"sampleid\", \"individualID\"] + cols\n",
    "        meta = meta[new_cols]\n",
    "\n",
    "        # Write CSV with custom formatting\n",
    "        with open(out_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "            # Write header\n",
    "            writer.writerow(meta.columns)\n",
    "            # Write data rows with custom formatting\n",
    "            for _, row in meta.iterrows():\n",
    "                writer.writerow([format_value(val) for val in row])\n",
    "        \n",
    "        print(f\"Processed metadata: {fname}\")\n",
    "\n",
    "    # ── Process count matrix .csv.gz files ───────────────────────────────────\n",
    "    for ct in celltype:\n",
    "        # Try both naming patterns: with and without underscore\n",
    "        patterns = [\n",
    "            f\"pseudobulk_peaks_counts_{ct}{suffix}.csv.gz\",  # Xiong pattern\n",
    "            f\"pseudobulk_peaks_counts{ct}{suffix}.csv.gz\"    # Kellis pattern\n",
    "        ]\n",
    "        \n",
    "        in_path = None\n",
    "        for pattern in patterns:\n",
    "            test_path = os.path.join(input_dir, pattern)\n",
    "            if os.path.exists(test_path):\n",
    "                in_path = test_path\n",
    "                fname = pattern\n",
    "                break\n",
    "        \n",
    "        if in_path is None:\n",
    "            print(f\"Warning: Count file not found for celltype {ct}\")\n",
    "            continue\n",
    "        \n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "\n",
    "        with gzip.open(in_path, \"rt\") as fh:\n",
    "            header_line = fh.readline().rstrip(\"\\n\")\n",
    "\n",
    "        col_names       = header_line.split(\",\")\n",
    "        peak_id_col     = col_names[0]\n",
    "        sample_cols     = col_names[1:]\n",
    "        new_sample_cols = [map_id(s) for s in sample_cols]\n",
    "        new_header      = \",\".join([peak_id_col] + new_sample_cols)\n",
    "\n",
    "        import tempfile\n",
    "        temp_header = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')\n",
    "        temp_header.write(new_header + \"\\n\")\n",
    "        temp_header.close()\n",
    "        \n",
    "        cmd = f\"zcat {in_path} | tail -n +2 | cat {temp_header.name} - | gzip -6 > {out_path}\"\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "        \n",
    "        os.unlink(temp_header.name)\n",
    "        print(f\"Processed counts: {fname}\")\n",
    "\n",
    "    print(\"\\nSample ID mapping completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0884ae7-a851-425a-86dd-b606768a012e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `pseudobulk_qc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46328b-c3d8-46f8-8c71-bad27820438e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[pseudobulk_qc]\n",
    "parameter: celltype         = ['Ast','Ex','In','Microglia','Oligo','OPC']\n",
    "parameter: input_dir        = str\n",
    "parameter: output_dir       = str\n",
    "parameter: covariates_file  = str\n",
    "parameter: blacklist_file   = ''\n",
    "parameter: include_bio      = \"FALSE\"   # \"TRUE\" or \"FALSE\"\n",
    "parameter: batch_correction = \"FALSE\"   # \"TRUE\" or \"FALSE\"\n",
    "parameter: batch_method     = \"limma\"   # \"limma\" or \"combat\"\n",
    "parameter: min_count        = 5\n",
    "parameter: min_total_count  = 15\n",
    "parameter: min_prop         = 0.1\n",
    "parameter: min_nuclei       = 20\n",
    "parameter: suffix           = ''\n",
    "\n",
    "input:  [f'{input_dir}/metadata_{ct}{suffix}.csv'                   for ct in celltype], \\\n",
    "        [f'{input_dir}/pseudobulk_peaks_counts_{ct}{suffix}.csv.gz' for ct in celltype]\n",
    "output: [f'{output_dir}/{ct}/{ct}_residuals.txt'        for ct in celltype]\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '6:00:00', mem = '64G', cores = 4\n",
    "\n",
    "R: expand = \"${ }\", stdout = f'{_output[0]:n}.stdout', stderr = f'{_output[0]:n}.stderr'\n",
    "\n",
    "    library(edgeR)\n",
    "    library(limma)\n",
    "    library(data.table)\n",
    "    library(GenomicRanges)\n",
    "    if (as.logical(\"${batch_correction}\") && \"${batch_method}\" == \"combat\") library(sva)\n",
    "\n",
    "    # ── Helper: standardize metadata column names ─────────────────────────────\n",
    "    rename_if_found <- function(dt, target, candidates) {\n",
    "        found <- intersect(candidates, colnames(dt))[1]\n",
    "        if (!is.na(found) && found != target) setnames(dt, found, target)\n",
    "    }\n",
    "\n",
    "    standardize_meta <- function(meta) {\n",
    "        rename_if_found(meta, \"n_nuclei\",              c(\"n.nuclei\",\"nNuclei\",\"nuclei_count\"))\n",
    "        rename_if_found(meta, \"med_nucleosome_signal\", c(\"med.nucleosome_signal.ct\",\"NucleosomeRatio\",\"med_nucleosome_signal.ct\"))\n",
    "        rename_if_found(meta, \"med_tss_enrich\",        c(\"med.tss.enrich.ct\",\"TSSEnrichment\",\"med_tss_enrich.ct\"))\n",
    "        rename_if_found(meta, \"med_n_tot_fragment\",    c(\"med.n_tot_fragment.ct\",\"med_n_tot_fragment.ct\"))\n",
    "        return(meta)\n",
    "    }\n",
    "\n",
    "    # ── Helper: blacklist filtering ───────────────────────────────────────────\n",
    "    filter_blacklist <- function(mat, bed) {\n",
    "        peaks <- data.table(id = rownames(mat))\n",
    "        peaks[, c(\"chr\",\"start\",\"end\") := tstrsplit(gsub(\"_\",\"-\",id), \"-\")]\n",
    "        peaks[, `:=`(start = as.numeric(start), end = as.numeric(end))]\n",
    "        bl <- fread(bed)[, 1:3]\n",
    "        setnames(bl, c(\"chr\",\"start\",\"end\"))\n",
    "        bl[, `:=`(start = as.numeric(start), end = as.numeric(end))]\n",
    "        gr1 <- GRanges(peaks$chr, IRanges(peaks$start, peaks$end))\n",
    "        gr2 <- GRanges(bl$chr,    IRanges(bl$start,    bl$end))\n",
    "        blacklisted <- unique(queryHits(findOverlaps(gr1, gr2)))\n",
    "        if (length(blacklisted) > 0) {\n",
    "            message(\"Blacklisted peaks removed: \", length(blacklisted))\n",
    "            return(mat[-blacklisted, , drop=FALSE])\n",
    "        }\n",
    "        return(mat)\n",
    "    }\n",
    "\n",
    "    # ── Helper: predictOffset ─────────────────────────────────────────────────\n",
    "    predictOffset <- function(fit) {\n",
    "        D  <- fit$design\n",
    "        Dm <- D\n",
    "        for (col in colnames(D)) {\n",
    "            if (col == \"(Intercept)\") next\n",
    "            if (is.numeric(D[, col]) && !all(D[, col] %in% c(0, 1)))\n",
    "                Dm[, col] <- median(D[, col], na.rm=TRUE)\n",
    "            else\n",
    "                Dm[, col] <- 0\n",
    "        }\n",
    "        B <- fit$coefficients\n",
    "        B[is.na(B)] <- 0\n",
    "        B %*% t(Dm)\n",
    "    }\n",
    "\n",
    "    # ── Main loop ─────────────────────────────────────────────────────────────\n",
    "    cts <- c(${', '.join([f\"'{x}'\" for x in celltype])})\n",
    "\n",
    "    for (ct in cts) {\n",
    "        message(\"\\n\", paste(rep(\"=\", 40), collapse=\"\"))\n",
    "        message(\"Processing: \", ct)\n",
    "        message(\"Mode: \", ifelse(as.logical(\"${include_bio}\"), \"BIOvar\", \"noBIOvar\"))\n",
    "        message(\"Batch correction: \", ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\"))\n",
    "        message(paste(rep(\"=\", 40), collapse=\"\"))\n",
    "\n",
    "        outdir <- file.path(\"${output_dir}/2_residuals\", ct)\n",
    "        dir.create(outdir, recursive=TRUE, showWarnings=FALSE)\n",
    "\n",
    "        # ── 1. Load data ───────────────────────────────────────────────────\n",
    "        meta       <- fread(sprintf(\"${input_dir}/metadata_%s${suffix}.csv\", ct))\n",
    "        counts_raw <- fread(sprintf(\"${input_dir}/pseudobulk_peaks_counts_%s${suffix}.csv.gz\", ct))\n",
    "\n",
    "        counts <- as.matrix(counts_raw[, -1, with=FALSE])\n",
    "        rownames(counts) <- counts_raw[[1]]\n",
    "        rm(counts_raw)\n",
    "        n_original <- nrow(counts)\n",
    "        message(\"Loaded: \", n_original, \" peaks x \", ncol(counts), \" samples\")\n",
    "\n",
    "        # ── 2. Standardize metadata columns ───────────────────────────────\n",
    "        meta <- standardize_meta(meta)\n",
    "\n",
    "        # ── 3. Identify sample ID column ──────────────────────────────────\n",
    "        idcol <- intersect(c(\"sampleid\",\"sampleID\",\"individualID\",\"projid\"), colnames(meta))[1]\n",
    "        if (is.na(idcol)) stop(\"Cannot find sample ID column in metadata.\")\n",
    "\n",
    "        # ── 4. Nuclei filter ──────────────────────────────────────────────\n",
    "        if (\"n_nuclei\" %in% colnames(meta)) {\n",
    "            meta <- meta[meta$n_nuclei > ${min_nuclei}]\n",
    "            message(\"Samples after nuclei (>${min_nuclei}) filter: \", nrow(meta))\n",
    "        }\n",
    "        n_after_nuclei <- nrow(meta)\n",
    "\n",
    "        # ── 5. Align samples ───────────────────────────────────────────────\n",
    "        common <- intersect(meta[[idcol]], colnames(counts))\n",
    "        if (length(common) == 0) stop(\"Zero sample overlap between metadata and count matrix.\")\n",
    "        meta   <- meta[match(common, meta[[idcol]])]\n",
    "        counts <- counts[, common, drop=FALSE]\n",
    "        message(\"Samples after alignment: \", length(common))\n",
    "\n",
    "        # ── 6. Blacklist filtering ─────────────────────────────────────────\n",
    "        if (\"${blacklist_file}\" != \"\" && file.exists(\"${blacklist_file}\")) {\n",
    "            counts <- filter_blacklist(counts, \"${blacklist_file}\")\n",
    "            message(\"Peaks after blacklist filter: \", nrow(counts))\n",
    "        } else {\n",
    "            message(\"No blacklist file provided - skipping blacklist filtering.\")\n",
    "        }\n",
    "        n_after_blacklist <- nrow(counts)\n",
    "\n",
    "        # ── 7. Load and merge covariates ───────────────────────────────────\n",
    "        covs      <- fread(\"${covariates_file}\")\n",
    "        id2       <- intersect(c(\"#id\",\"id\",\"projid\",\"individualID\"), colnames(covs))[1]\n",
    "        bio_cols  <- if (as.logical(\"${include_bio}\")) c(\"msex\",\"age_death\",\"pmi\",\"study\") else c(\"pmi\",\"study\")\n",
    "        keep_cols <- c(id2, intersect(bio_cols, colnames(covs)))\n",
    "        covs      <- covs[, ..keep_cols]\n",
    "        meta      <- merge(meta, covs, by.x=idcol, by.y=id2, all.x=TRUE)\n",
    "\n",
    "        # ── CRITICAL: re-order meta back to common sample order ────────────\n",
    "        meta <- meta[match(common, meta[[idcol]])]\n",
    "\n",
    "        # ── 8. Impute missing covariate values ─────────────────────────────\n",
    "        for (col in intersect(c(\"pmi\",\"age_death\"), colnames(meta))) {\n",
    "            if (any(is.na(meta[[col]]))) {\n",
    "                message(\"Imputing missing values for: \", col)\n",
    "                meta[[col]][is.na(meta[[col]])] <- median(meta[[col]], na.rm=TRUE)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # ── 9. Compute technical metrics ──────────────────────────────────\n",
    "        meta$log_n_nuclei           <- log1p(meta$n_nuclei)\n",
    "        meta$log_med_n_tot_fragment <- log1p(meta$med_n_tot_fragment)\n",
    "        meta$log_total_unique_peaks <- log1p(colSums(counts > 0))\n",
    "\n",
    "        # ── 10. Select model variables ────────────────────────────────────\n",
    "        tech_vars <- c(\"log_n_nuclei\",\"med_nucleosome_signal\",\"med_tss_enrich\",\n",
    "                       \"log_med_n_tot_fragment\",\"log_total_unique_peaks\",\"pmi\",\"study\")\n",
    "        bio_vars  <- c(\"msex\",\"age_death\")\n",
    "        all_vars  <- if (as.logical(\"${include_bio}\")) c(tech_vars, bio_vars) else tech_vars\n",
    "        all_vars  <- intersect(all_vars, colnames(meta))\n",
    "        message(\"Model terms: \", paste(all_vars, collapse=\", \"))\n",
    "\n",
    "        # ── 11. Drop samples with NA in model variables ────────────────────\n",
    "        keep_rows <- complete.cases(meta[, ..all_vars])\n",
    "        meta      <- meta[keep_rows]\n",
    "        counts    <- counts[, meta[[idcol]], drop=FALSE]\n",
    "        message(\"Valid samples for modelling: \", nrow(meta))\n",
    "\n",
    "        # ── 12. Expression filtering ───────────────────────────────────────\n",
    "        dge <- DGEList(counts=counts, samples=meta)\n",
    "        dge$samples$group <- factor(rep(\"all\", ncol(dge)))\n",
    "        message(\"Peaks before expression filter: \", nrow(dge))\n",
    "\n",
    "        keep <- filterByExpr(dge, group=dge$samples$group,\n",
    "                             min.count=${min_count},\n",
    "                             min.total.count=${min_total_count},\n",
    "                             min.prop=${min_prop})\n",
    "        dge <- dge[keep,, keep.lib.sizes=FALSE]\n",
    "        n_after_expr <- nrow(dge)\n",
    "        message(\"Peaks after expression filter: \", n_after_expr)\n",
    "\n",
    "        # Save filtered raw counts\n",
    "        write.table(dge$counts,\n",
    "                    file.path(outdir, paste0(ct, \"_filtered_raw_counts.txt\")),\n",
    "                    sep=\"\\t\", quote=FALSE, col.names=NA)\n",
    "\n",
    "        # ── 13. TMM normalization ──────────────────────────────────────────\n",
    "        dge <- calcNormFactors(dge, method=\"TMM\")\n",
    "\n",
    "        # ── 14. Optional batch correction ─────────────────────────────────\n",
    "        if (as.logical(\"${batch_correction}\") && \"sequencingBatch\" %in% colnames(dge$samples)) {\n",
    "            batches       <- dge$samples$sequencingBatch\n",
    "            batch_counts  <- table(batches)\n",
    "            valid_batches <- names(batch_counts[batch_counts > 1])\n",
    "            keep_bc       <- batches %in% valid_batches\n",
    "            dge           <- dge[, keep_bc, keep.lib.sizes=FALSE]\n",
    "            batches       <- batches[keep_bc]\n",
    "            message(\"Samples after singleton batch removal: \", ncol(dge))\n",
    "\n",
    "            if (\"${batch_method}\" == \"combat\") {\n",
    "                dge$counts <- ComBat_seq(as.matrix(dge$counts), batch=batches)\n",
    "                message(\"ComBat-seq batch correction applied.\")\n",
    "            } else {\n",
    "                logCPM     <- cpm(dge, log=TRUE, prior.count=1)\n",
    "                logCPM     <- removeBatchEffect(logCPM, batch=factor(batches))\n",
    "                dge$counts <- round(pmax(2^logCPM, 0))\n",
    "                message(\"limma removeBatchEffect applied.\")\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # ── 15. Add sequencingBatch and Library to model if multi-level ───\n",
    "        # Insert after technical vars but before pmi/study to match original order\n",
    "        tech_only <- c(\"log_n_nuclei\",\"med_nucleosome_signal\",\"med_tss_enrich\",\n",
    "                       \"log_med_n_tot_fragment\",\"log_total_unique_peaks\")\n",
    "        other_vars <- setdiff(all_vars, tech_only)  # pmi, study, msex, age_death\n",
    "\n",
    "        batch_vars <- c()\n",
    "        if (\"sequencingBatch\" %in% colnames(dge$samples) &&\n",
    "            length(unique(dge$samples$sequencingBatch)) > 1) {\n",
    "            dge$samples$sequencingBatch_factor <- factor(dge$samples$sequencingBatch)\n",
    "            batch_vars <- c(batch_vars, \"sequencingBatch_factor\")\n",
    "        }\n",
    "\n",
    "        if (\"Library\" %in% colnames(dge$samples) &&\n",
    "            length(unique(dge$samples$Library)) > 1) {\n",
    "            dge$samples$Library_factor <- factor(dge$samples$Library)\n",
    "            batch_vars <- c(batch_vars, \"Library_factor\")\n",
    "        }\n",
    "\n",
    "        # Final order: technical + batch + other (pmi, study, bio)\n",
    "        all_vars <- c(tech_only, batch_vars, other_vars)\n",
    "        all_vars <- intersect(all_vars, c(colnames(dge$samples), colnames(meta)))\n",
    "\n",
    "        # ── 16. Build design matrix ────────────────────────────────────────\n",
    "        form   <- as.formula(paste(\"~\", paste(all_vars, collapse=\" + \")))\n",
    "        design <- model.matrix(form, data=dge$samples)\n",
    "        message(\"Formula: \", deparse(form))\n",
    "\n",
    "        if (!is.fullrank(design)) {\n",
    "            message(\"Design not full rank - trimming.\")\n",
    "            qr_d   <- qr(design)\n",
    "            design <- design[, qr_d$pivot[seq_len(qr_d$rank)], drop=FALSE]\n",
    "        }\n",
    "        message(\"Design matrix: \", nrow(design), \" x \", ncol(design))\n",
    "\n",
    "        # ── 17. Voom + lmFit + eBayes ─────────────────────────────────────\n",
    "        v   <- voom(dge, design, plot=FALSE)\n",
    "        fit <- lmFit(v, design)\n",
    "        fit <- eBayes(fit)\n",
    "\n",
    "        # ── 18. Offset + residuals ─────────────────────────────────────────\n",
    "        off   <- predictOffset(fit)\n",
    "        res   <- residuals(fit, v)\n",
    "        final <- off + res\n",
    "\n",
    "        # ── 19. Save outputs ───────────────────────────────────────────────\n",
    "        write.table(final,\n",
    "                    file.path(outdir, paste0(ct, \"_residuals.txt\")),\n",
    "                    sep=\"\\t\", quote=FALSE, col.names=NA)\n",
    "\n",
    "        saveRDS(list(\n",
    "            dge              = dge,\n",
    "            offset           = off,\n",
    "            residuals        = res,\n",
    "            final_data       = final,\n",
    "            valid_samples    = colnames(dge),\n",
    "            design           = design,\n",
    "            fit              = fit,\n",
    "            model            = form,\n",
    "            mode             = ifelse(as.logical(\"${include_bio}\"), \"BIOvar\", \"noBIOvar\"),\n",
    "            batch_correction = as.logical(\"${batch_correction}\"),\n",
    "            batch_method     = ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\")\n",
    "        ), file.path(outdir, paste0(ct, \"_results.rds\")))\n",
    "\n",
    "        # ── 20. Summary report ─────────────────────────────────────────────\n",
    "        sink(file.path(outdir, paste0(ct, \"_summary.txt\")))\n",
    "        cat(\"*** Processing Summary for\", ct, \"***\\n\\n\")\n",
    "\n",
    "        cat(\"=== Analysis Mode ===\\n\")\n",
    "        cat(\"Mode:\", ifelse(as.logical(\"${include_bio}\"), \"BIOvar\", \"noBIOvar\"), \"\\n\")\n",
    "        cat(\"Batch correction:\", ifelse(as.logical(\"${batch_correction}\"), \"${batch_method}\", \"none\"), \"\\n\")\n",
    "        cat(\"Model formula:\", deparse(form), \"\\n\\n\")\n",
    "\n",
    "        cat(\"=== Filtering Parameters ===\\n\")\n",
    "        cat(\"Nuclei cutoff: >\", ${min_nuclei}, \"\\n\")\n",
    "        cat(\"Blacklist filtering:\", ifelse(\"${blacklist_file}\" != \"\", \"TRUE\", \"FALSE\"), \"\\n\")\n",
    "        if (\"${blacklist_file}\" != \"\") cat(\"Blacklist file:\", \"${blacklist_file}\", \"\\n\")\n",
    "        cat(\"min_count:\", ${min_count}, \"\\n\")\n",
    "        cat(\"min_total_count:\", ${min_total_count}, \"\\n\")\n",
    "        cat(\"min_prop:\", ${min_prop}, \"\\n\\n\")\n",
    "\n",
    "        cat(\"=== Peak Counts ===\\n\")\n",
    "        cat(\"Original peak count:\", n_original, \"\\n\")\n",
    "        cat(\"Peaks after blacklist filtering:\", n_after_blacklist, \"\\n\")\n",
    "        cat(\"Peaks after expression filtering:\", n_after_expr, \"\\n\\n\")\n",
    "\n",
    "        cat(\"=== Sample Counts ===\\n\")\n",
    "        cat(\"Number of samples after nuclei (>\", ${min_nuclei}, \") filtering:\", n_after_nuclei, \"\\n\")\n",
    "        cat(\"Number of samples in final model:\", ncol(final), \"\\n\\n\")\n",
    "\n",
    "        cat(\"=== Technical Variables Used ===\\n\")\n",
    "        for (v in intersect(c(\"log_n_nuclei\",\"med_nucleosome_signal\",\"med_tss_enrich\",\n",
    "                               \"log_med_n_tot_fragment\",\"log_total_unique_peaks\"), all_vars))\n",
    "            cat(\"-\", v, \"\\n\")\n",
    "        if (\"sequencingBatch_factor\" %in% all_vars) cat(\"- sequencingBatch: Sequencing batch ID\\n\")\n",
    "        if (\"Library_factor\"         %in% all_vars) cat(\"- Library: Library ID\\n\")\n",
    "\n",
    "        if (as.logical(\"${include_bio}\")) {\n",
    "            cat(\"\\n=== Biological Variables Used ===\\n\")\n",
    "            for (v in intersect(c(\"msex\",\"age_death\"), all_vars))\n",
    "                cat(\"-\", v, \"\\n\")\n",
    "        } else {\n",
    "            cat(\"\\n=== Biological Variables Used ===\\n\")\n",
    "            cat(\"None (noBIOvar mode - biological variation preserved)\\n\")\n",
    "        }\n",
    "\n",
    "        cat(\"\\n=== Other Variables Used ===\\n\")\n",
    "        if (\"pmi\"   %in% all_vars) cat(\"- pmi: Post-mortem interval\\n\")\n",
    "        if (\"study\" %in% all_vars) cat(\"- study: Study cohort\\n\")\n",
    "        sink()\n",
    "\n",
    "        # ── 21. Variable explanation report ───────────────────────────────\n",
    "        sink(file.path(outdir, paste0(ct, \"_variable_explanation.txt\")))\n",
    "        cat(\"# ATAC-seq Technical Variables Explanation\\n\\n\")\n",
    "        cat(\"## Why Log Transformation?\\n\")\n",
    "        cat(\"Log transformation is applied to certain variables for several reasons:\\n\")\n",
    "        cat(\"1. To make the distribution more symmetric and closer to normal\\n\")\n",
    "        cat(\"2. To stabilize variance across the range of values\\n\")\n",
    "        cat(\"3. To match the scale of voom-transformed peak counts, which are on log2-CPM scale\\n\")\n",
    "        cat(\"4. To be consistent with the approach used in related studies like haQTL\\n\\n\")\n",
    "        cat(\"## Variables and Their Meanings\\n\\n\")\n",
    "        cat(\"### Technical Variables\\n\")\n",
    "        cat(\"- n_nuclei: Number of nuclei that contributed to this pseudobulk sample\\n\")\n",
    "        cat(\"  * Filtered to include only samples with >\", ${min_nuclei}, \"nuclei\\n\")\n",
    "        cat(\"  * Log-transformed because count data typically has a right-skewed distribution\\n\\n\")\n",
    "        cat(\"- med_n_tot_fragment: Median number of total fragments per cell\\n\")\n",
    "        cat(\"  * Represents sequencing depth\\n\")\n",
    "        cat(\"  * Log-transformed because sequencing depth typically has exponential effects\\n\\n\")\n",
    "        cat(\"- total_unique_peaks: Number of unique peaks detected in each sample\\n\")\n",
    "        cat(\"  * Log-transformed similar to 'TotalNumPeaks' in haQTL pipeline\\n\\n\")\n",
    "        cat(\"- med_nucleosome_signal: Median nucleosome signal\\n\")\n",
    "        cat(\"  * Measures the degree of nucleosome positioning\\n\")\n",
    "        cat(\"  * Not log-transformed as it is already a ratio/normalized metric\\n\\n\")\n",
    "        cat(\"- med_tss_enrich: Median transcription start site enrichment score\\n\")\n",
    "        cat(\"  * Indicates the quality of the ATAC-seq data\\n\")\n",
    "        cat(\"  * Not log-transformed as it is already a ratio/normalized metric\\n\\n\")\n",
    "        if (\"sequencingBatch_factor\" %in% all_vars)\n",
    "            cat(\"- sequencingBatch: Sequencing batch ID\\n  * Treated as a factor to account for batch effects\\n\\n\")\n",
    "        if (\"Library_factor\" %in% all_vars)\n",
    "            cat(\"- Library: Library preparation batch ID\\n  * Treated as a factor to account for library preparation effects\\n\\n\")\n",
    "        if (as.logical(\"${include_bio}\")) {\n",
    "            cat(\"### Biological Variables\\n\")\n",
    "            cat(\"- msex: Sex (male=1, female=0)\\n\")\n",
    "            cat(\"- age_death: Age at death\\n\\n\")\n",
    "        }\n",
    "        cat(\"### Other Variables\\n\")\n",
    "        cat(\"- pmi: Post-mortem interval (time between death and tissue collection)\\n\")\n",
    "        cat(\"- study: Study cohort (ROSMAP, MAP, ROS)\\n\\n\")\n",
    "        cat(\"## Relationship to voom Transformation\\n\")\n",
    "        cat(\"The voom transformation converts count data to log2-CPM (counts per million) values \")\n",
    "        cat(\"and estimates the mean-variance relationship. By log-transforming certain technical \")\n",
    "        cat(\"covariates, we ensure they are on a similar scale to the transformed expression data, \")\n",
    "        cat(\"which can improve the fit of the linear model used for removing unwanted variation.\\n\")\n",
    "        sink()\n",
    "\n",
    "        message(\"Completed: \", ct, \" -> \", outdir)\n",
    "        message(\"  Peaks: \", nrow(final), \" | Samples: \", ncol(final))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56ffb1-6c07-47ac-9a1a-abbd37f253c9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `phenotype_reformatting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef18e3e-fe77-486f-89e6-e724b7126b73",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[phenotype_formatting]\n",
    "parameter: celltype    = ['Ast','Ex','In','Mic','Oligo','OPC']\n",
    "parameter: input_dir   = str\n",
    "parameter: output_dir  = str\n",
    "\n",
    "input:  [f'{input_dir}/{ct}/{ct}_residuals.txt' for ct in celltype]\n",
    "output: [f'{output_dir}/{ct}_snatac_phenotype.bed.gz' for ct in celltype]\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '2:00:00', mem = '16G', cores = 2\n",
    "\n",
    "python: expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "\n",
    "    import os\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "\n",
    "    celltypes  = ${celltype}\n",
    "    input_dir  = \"${input_dir}\"\n",
    "    output_dir = \"${output_dir}\"\n",
    "\n",
    "    def read_residuals(path):\n",
    "        first_line = open(path).readline().rstrip(\"\\n\")\n",
    "        col_names  = first_line.split(\"\\t\")\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, skiprows=1)\n",
    "        if df.shape[1] > len(col_names):\n",
    "            peak_ids   = df.iloc[:, 0].values\n",
    "            df         = df.iloc[:, 1:]\n",
    "            df.columns = col_names\n",
    "        else:\n",
    "            peak_ids   = df.iloc[:, 0].values\n",
    "            df         = df.iloc[:, 1:]\n",
    "            df.columns = col_names[1:]\n",
    "        return peak_ids, df\n",
    "\n",
    "    def to_midpoint_bed(peak_ids, residuals):\n",
    "        parts  = pd.Series(peak_ids).str.split(\"-\", expand=True)\n",
    "        chrs   = parts[0].values\n",
    "        starts = parts[1].astype(int).values\n",
    "        ends   = parts[2].astype(int).values\n",
    "        mids   = ((starts + ends) // 2).astype(int)\n",
    "        bed = pd.DataFrame({\n",
    "            \"#chr\":  chrs,\n",
    "            \"start\": mids,\n",
    "            \"end\":   mids + 1,\n",
    "            \"ID\":    peak_ids\n",
    "        })\n",
    "        bed = pd.concat([bed, residuals.reset_index(drop=True)], axis=1)\n",
    "        return bed.sort_values([\"#chr\", \"start\"]).reset_index(drop=True)\n",
    "\n",
    "    def run_cmd(cmd, label):\n",
    "        r = subprocess.run(cmd, capture_output=True)\n",
    "        if r.returncode != 0:\n",
    "            print(f\"WARNING: {label} failed: {r.stderr.decode()}\")\n",
    "        else:\n",
    "            print(f\"{label}: OK\")\n",
    "\n",
    "    for ct in celltypes:\n",
    "        print(f\"\\n{'='*40}\\nPhenotype Formatting: {ct}\\n{'='*40}\")\n",
    "\n",
    "        out_dir = os.path.join(output_dir, \"3_pheno_reformat\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        res_path = os.path.join(input_dir, ct, f\"{ct}_residuals.txt\")\n",
    "        if not os.path.exists(res_path):\n",
    "            print(f\"WARNING: {res_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        peak_ids, residuals = read_residuals(res_path)\n",
    "        print(f\"Loaded {len(peak_ids)} peaks x {residuals.shape[1]} samples\")\n",
    "\n",
    "        bed     = to_midpoint_bed(peak_ids, residuals)\n",
    "        out_bed = os.path.join(out_dir, f\"{ct}_snatac_phenotype.bed\")\n",
    "        bed.to_csv(out_bed, sep=\"\\t\", index=False, float_format=\"%.15f\")\n",
    "        print(f\"Written: {out_bed}\")\n",
    "\n",
    "        run_cmd([\"bgzip\", \"-f\", out_bed],                \"bgzip\")\n",
    "        run_cmd([\"tabix\", \"-p\", \"bed\", f\"{out_bed}.gz\"], \"tabix\")\n",
    "\n",
    "        print(f\"Completed: {ct} -> {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bc2ab-c412-40ef-a9b6-f5dddf5292ee",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `region_filtering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd13567e-2d4c-48f6-83d7-62ab252421bf",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[region_filtering]\n",
    "parameter: celltype    = ['Ast','Ex','In','Mic','Oligo','OPC']\n",
    "parameter: input_dir   = str\n",
    "parameter: output_dir  = str\n",
    "parameter: regions     = \"chr7:28000000-28300000,chr11:85050000-86200000\"\n",
    "\n",
    "input:  [f'{input_dir}/{ct}/{ct}_filtered_raw_counts.txt' for ct in celltype]\n",
    "output: [f'{output_dir}/{ct}_filtered_regions_of_interest.txt' for ct in celltype]\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '1:00:00', mem = '16G', cores = 2\n",
    "\n",
    "python: expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    celltypes  = ${celltype}\n",
    "    input_dir  = \"${input_dir}\"\n",
    "    output_dir = \"${output_dir}\"\n",
    "\n",
    "    def parse_regions(region_str):\n",
    "        result = []\n",
    "        for r in region_str.split(\",\"):\n",
    "            chrom, coords = r.strip().split(\":\")\n",
    "            start, end    = coords.split(\"-\")\n",
    "            result.append({\"chr\": chrom, \"start\": int(start), \"end\": int(end)})\n",
    "        return result\n",
    "\n",
    "    regions = parse_regions(\"${regions}\")\n",
    "\n",
    "    def parse_peak_ids(peak_ids):\n",
    "        parts = pd.Series(peak_ids).str.split(\"-\", expand=True)\n",
    "        return pd.DataFrame({\n",
    "            \"chr\":   parts[0].values,\n",
    "            \"start\": parts[1].astype(int).values,\n",
    "            \"end\":   parts[2].astype(int).values\n",
    "        })\n",
    "\n",
    "    def overlaps_region(chr_col, start_col, end_col, reg):\n",
    "        return (\n",
    "            (chr_col   == reg[\"chr\"]) &\n",
    "            (start_col <  reg[\"end\"]) &\n",
    "            (end_col   >  reg[\"start\"])\n",
    "        )\n",
    "\n",
    "    for ct in celltypes:\n",
    "        print(f\"\\n{'='*40}\\nRegion Filtering: {ct}\\n{'='*40}\")\n",
    "\n",
    "        reg_dir = os.path.join(output_dir, \"3_region_filter\")\n",
    "        os.makedirs(reg_dir, exist_ok=True)\n",
    "\n",
    "        counts_path = os.path.join(input_dir, ct, f\"{ct}_filtered_raw_counts.txt\")\n",
    "        if not os.path.exists(counts_path):\n",
    "            print(f\"WARNING: {counts_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(counts_path, sep=\"\\t\", index_col=0)\n",
    "        df.index.name = \"peak_id\"\n",
    "        df = df.reset_index()\n",
    "\n",
    "        coords          = parse_peak_ids(df[\"peak_id\"].values)\n",
    "        df[\"chr\"]       = coords[\"chr\"].values\n",
    "        df[\"start\"]     = coords[\"start\"].values\n",
    "        df[\"end\"]       = coords[\"end\"].values\n",
    "        df[\"peakwidth\"] = df[\"end\"] - df[\"start\"]\n",
    "        df[\"midpoint\"]  = ((df[\"start\"] + df[\"end\"]) / 2).astype(int)\n",
    "\n",
    "        # Filter to regions of interest\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "        for reg in regions:\n",
    "            mask |= overlaps_region(df[\"chr\"], df[\"start\"], df[\"end\"], reg)\n",
    "\n",
    "        region_df = df[mask].copy()\n",
    "        print(f\"Peaks in regions of interest: {len(region_df)}\")\n",
    "\n",
    "        # Save full filtered data\n",
    "        full_out = os.path.join(reg_dir, f\"{ct}_filtered_regions_of_interest.txt\")\n",
    "        region_df.to_csv(full_out, sep=\"\\t\", index=False)\n",
    "        print(f\"Saved: {full_out}\")\n",
    "\n",
    "        # Save summary\n",
    "        meta_cols  = [\"peak_id\",\"chr\",\"start\",\"end\",\"peakwidth\",\"midpoint\"]\n",
    "        count_cols = [c for c in region_df.columns if c not in meta_cols]\n",
    "        count_mat  = region_df[count_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "        summary = region_df[meta_cols].copy()\n",
    "        summary[\"total_count\"]    = count_mat.sum(axis=1).values\n",
    "        summary[\"weighted_count\"] = (summary[\"total_count\"] / summary[\"peakwidth\"]).values\n",
    "\n",
    "        summary_out = os.path.join(reg_dir, f\"{ct}_filtered_regions_of_interest_summary.txt\")\n",
    "        summary.to_csv(summary_out, sep=\"\\t\", index=False)\n",
    "        print(f\"Saved: {summary_out}\")\n",
    "\n",
    "        print(f\"Completed: {ct} -> {reg_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "sos",
     "",
     ""
    ]
   ],
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
