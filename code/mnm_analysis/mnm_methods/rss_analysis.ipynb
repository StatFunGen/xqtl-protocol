{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-collins",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# High-dimensional regression with summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-thriller",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook take a list of LD reference files and a list of sumstat files from various association studies, and perform:\n",
    "\n",
    "1. Fine-mapping with SuSiE RSS model\n",
    "2. TWAS / PRS weights ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-sullivan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-digit",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "\n",
    "I. **GWAS Summary Statistics Files**\n",
    "- **Input**: Vector of files for one or more GWAS studies.\n",
    "- **Format**: \n",
    "  - Tab-delimited files.\n",
    "  - First 4 columns: `chrom`, `pos`, `A1`, `A2`\n",
    "  - Additional columns can be loaded using column mapping file see below  \n",
    "- **Column Mapping files (optional)**:\n",
    "  - Optional YAML file for custom column mapping.\n",
    "  - Required columns: `chrom`, `pos`, `A1`, `A2`, either `z` or (`beta` and `se`), `n_sample`, `n_case`, `n_control`. Note: You can only provide `n_sample` or (`n_case` & `n_control`, together for case control study), fill with 0 if do not provide them. If none of these are known, fill `n_sample`, `n_case`, `n_control` with 0.\n",
    "  - Optional columns: `var_y`.\n",
    "\n",
    "II. **GWAS Summary Statistics Meta-File**: this is optional and helpful when there are lots of GWAS summary statistics data to process via the same command. All studies will run at the same time. \n",
    "\n",
    "The meta file should contain 7 columns.\n",
    "- **Columns**: \n",
    "1. `study_id`: can be any string / number.\n",
    "2. `chrom`: the chromosome that summary statistics cover. Chromosome number `0` indicates a genome-wide summary statistics file.\n",
    "3. `file_path`: the path to the summary statistics file. The format of summary statistics file can be tsv, csv.\n",
    "4. `column_mapping_file`: served to unify the column names in file_path. Map those column names to a standard column name.\n",
    "5. `n_sample`: (effective) sample size of the study. If not known, fill with 0.\n",
    "6. `n_case`: (effective) case number in the study if it's case control study. If not known, fill with 0.\n",
    "7. `n_control`: control number in the study if it's case control study. If not known, fill with 0.\n",
    "\n",
    "**Note:** we only need n_sample or (n_case & n_control). If is a case control study but n_case n_control not known, using n_sample also works.\n",
    "\n",
    "eg: `gwas_meta.tsv`\n",
    "\n",
    "```\n",
    "study_id\tchrom\tfile_path\tcolumn_mapping_file\tn_sample\tn_case\tn_control\n",
    "AD_1\t0\tdata1.txt\tmapping1.yml\t0\t111326\t677663\n",
    "AD_2\t0\tdata2.txt\tmapping2.yml\t123455\t0\t0\n",
    "AD_3\t2\tdata3.txt\tmapping3.yml\t0\t21982\t41944\n",
    "```\n",
    "\n",
    "If both summary stats file (I) and meta data file (II) are specified we will take the union of the two.\n",
    "\n",
    "eg. `column_mapping.yml` left: standard name (please hold on to it). Right: original column name in your summary statistics file. (do not add space before and after \":\")\n",
    "\n",
    "```\n",
    "chrom:chromosome\n",
    "pos:base_pair_location\n",
    "A1:effect_allele\n",
    "A2:other_allele\n",
    "beta:beta\n",
    "se:standard_error\n",
    "pvalue:p_value\n",
    "maf:maf\n",
    "z:z\n",
    "n_case:n_cases\n",
    "n_control:n_controls\n",
    "n_sample:n\n",
    "```\n",
    "\n",
    "\n",
    "III. **LD Reference Metadata File**\n",
    "- **Format**: Single TSV file.\n",
    "- **Contents**:\n",
    "  - Columns: `#chrom`, `start`, `end`, path to the LD matrix, genomic build.\n",
    "  - LD matrix path format: comma-separated, first entry is the LD matrix, second is the bim file.\n",
    "- **Documentation**: Refer to our LD reference preparation document for detailed information (Tosin pending update).\n",
    "\n",
    "IV. For analyzing specific genomic regions, you can specify them using the `--region-names` option in the 'chr:start-end' format, where multiple regions are accepted. Alternatively, you may provide a file containing a list of regions through the `--region-list` option, also adhering to the 'chr:start-end' format. When both `--region-names` and `--region-list` are provided, union of these options will be used to analyze. In cases where neither option is specified, the analysis defaults to encompass all regions specified in the LD reference metadata. Use --comment_string to specify the comment symbol in the original names, enclosing your option in quotation marks (e.g., --comment_string \"#\"). The default is no comment symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-argument",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-lobby",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Each rds file is the finemapping result of one LD block. Including 10 elements in each rds file.\n",
    "1. single_effect_regression: Assume only one causal variant, simple regression.\n",
    "2. noqc: Susie finemapping, the sumstat is only allele_qced.\n",
    "3. qc_impute: Susie finemapping, the summary statistics are QCed (suspecious outliers of z scores removed) and imputed (all outliers and missing variants from reference panel).\n",
    "4. qc_only: Susie finemapping, the summary statistics are only QCed .\n",
    "5. conditional_regression_noqc: Bayesian conditional regression, original sumstat\n",
    "6. conditional_regression_qc_only: Bayesian conditional regression, sumstat QCed\n",
    "7. conditional_regression_qc_impute: Bayesian conditional regression, sumstat QCed and imputed.\n",
    "8. sumstats_allele_qc_only: a table of sumstat after allele qc (check each variant to see if ref/alt are flipped, and correct them)\n",
    "9. sumstats_qc_impute: a table of sumstat after qc and imputation.\n",
    "10. sumstats_qc_impute: a table of sumstat after qc and imputation, bad quality imputation variants removed by empirical thresholds. (the actual input of all qc_impute results)\n",
    "\n",
    "Each rds file is accompanied by 2 tsv files reporting elements 8 and 9 in tsv format for the sake of convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-groove",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-biodiversity",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run xqtl-protocol/pipeline/rss_analysis.ipynb univariate_rss \\\n",
    "    --ld-meta-data ADSP_R4_EUR.LD.list \\\n",
    "    --gwas-meta-data AD_sumstat_list.txt \\\n",
    "    --impute --qc-method dentist --finemapping-method susie_rss \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-lecture",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output/\")\n",
    "parameter: gwas_meta_data = path()\n",
    "parameter: ld_meta_data = path()\n",
    "parameter: gwas_name = []\n",
    "parameter: gwas_data = []\n",
    "parameter: column_mapping = []\n",
    "parameter: region_list = path()\n",
    "parameter: region_name = []\n",
    "parameter: container = ''\n",
    "parameter: skip_regions = []\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: job_size = 5\n",
    "parameter: walltime = \"10h\"\n",
    "parameter: mem = \"16G\"\n",
    "parameter: numThreads = 1\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "import os\n",
    "if (not os.path.isfile(region_list)) and len(region_name) == 0:\n",
    "    region_list = ld_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-limit",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "def file_exists(file_path, relative_path=None):\n",
    "    \"\"\"Check if a file exists at the given path or relative to a specified path.\"\"\"\n",
    "    if os.path.exists(file_path) and os.path.isfile(file_path):\n",
    "        return True\n",
    "    elif relative_path:\n",
    "        relative_file_path = os.path.join(relative_path, file_path)\n",
    "        return os.path.exists(relative_file_path) and os.path.isfile(relative_file_path)\n",
    "    return False\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def parse_region(region):\n",
    "    \"\"\"Parse a region string in 'chr:start-end' format into a list [chr, start, end].\"\"\"\n",
    "    chrom, rest = region.split(':')\n",
    "    start, end = rest.split('-')\n",
    "    return [int(chrom), int(start), int(end)]\n",
    "\n",
    "def load_regional_rss_data(gwas_meta_data, gwas_name, gwas_data, column_mapping, region_name=None, region_list=None):\n",
    "    \"\"\"\n",
    "    Extracts data from GWAS metadata files and additional GWAS data provided. \n",
    "    Optionally filters data based on specified regions.\n",
    "\n",
    "    Args:\n",
    "    - gwas_meta_data (str): File path to the GWAS metadata file.\n",
    "    - gwas_name (list): Vector of GWAS study names.\n",
    "    - gwas_data (list): Vector of GWAS data.\n",
    "    - column_mapping (list, optional): Vector of column mapping files.\n",
    "    - region_name (list, optional): List of region names in 'chr:start-end' format.\n",
    "    - region_list (str, optional): File path to a file containing regions.\n",
    "\n",
    "    Returns:\n",
    "    - GWAS Dictionary: Maps study IDs to a list containing chromosome number, \n",
    "      GWAS file path, and optional column mapping file path.\n",
    "    - Region Dictionary: Maps region names to lists [chr, start, end].\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If any specified file path does not exist.\n",
    "    - ValueError: If required columns are missing in the input files or vector lengths mismatch.\n",
    "    \"\"\"\n",
    "    # Check vector lengths\n",
    "    if len(gwas_name) != len(gwas_data):\n",
    "        raise ValueError(\"gwas_name and gwas_data must be of equal length\")\n",
    "    \n",
    "    if len(column_mapping) > 0 and len(column_mapping) != len(gwas_name):\n",
    "        raise ValueError(\"If column_mapping is provided, it must be of the same length as gwas_name and gwas_data\")\n",
    "\n",
    "    # Required columns for GWAS file type\n",
    "    required_gwas_columns = ['study_id', 'chrom', 'file_path']\n",
    "\n",
    "    # Base directory of the metadata files\n",
    "    gwas_base_dir = os.path.dirname(gwas_meta_data)\n",
    "    \n",
    "    # Reading the GWAS metadata file\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(gwas_df, required_gwas_columns)\n",
    "    gwas_dict = OrderedDict()\n",
    "\n",
    "    # Process additional GWAS data from vectors\n",
    "    for name, data, mapping in zip(gwas_name, gwas_data, column_mapping or [None]*len(gwas_name)):\n",
    "        gwas_dict[name] = {0: [data, mapping]}\n",
    "\n",
    "    for _, row in gwas_df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        mapping_file = row.get('column_mapping_file')\n",
    "        n_sample = row.get('n_sample')\n",
    "        n_case = row.get('n_case')\n",
    "        n_control = row.get('n_control')\n",
    "\n",
    "        # Check if the file and optional mapping file exist\n",
    "        if not file_exists(file_path, gwas_base_dir) or (mapping_file and not file_exists(mapping_file, gwas_base_dir)):\n",
    "            raise FileNotFoundError(f\"File {file_path} not found for {row['study_id']}\")\n",
    "        \n",
    "        # Adjust paths if necessary\n",
    "        file_path = file_path if file_exists(file_path) else os.path.join(gwas_base_dir, file_path)\n",
    "        if mapping_file:\n",
    "            mapping_file = mapping_file if file_exists(mapping_file) else os.path.join(gwas_base_dir, mapping_file)\n",
    "        \n",
    "        # Create or update the entry for the study_id\n",
    "        if row['study_id'] not in gwas_dict:\n",
    "            gwas_dict[row['study_id']] = {}\n",
    "\n",
    "        # Expand chrom 0 to chrom 1-22 or use the specified chrom\n",
    "        chrom_range = range(1, 23) if row['chrom'] == 0 else [row['chrom']]\n",
    "        for chrom in chrom_range:\n",
    "            if chrom in gwas_dict[row['study_id']]:\n",
    "                existing_entry = gwas_dict[row['study_id']][chrom]\n",
    "                raise ValueError(f\"Duplicate chromosome specification for study_id {row['study_id']}, chrom {chrom}. \"\n",
    "                                 f\"Conflicting entries: {existing_entry} and {[file_path, mapping_file]}\")\n",
    "            gwas_dict[row['study_id']][chrom] = [file_path, mapping_file, n_sample, n_case, n_control]\n",
    "\n",
    "    # Process region_list and region_name\n",
    "    region_dict = dict()\n",
    "    if region_list and os.path.isfile(region_list):\n",
    "        with open(region_list, 'r') as file:\n",
    "            for line in file:\n",
    "                # Skip empty lines\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 1:\n",
    "                    region = parse_region(parts[0])\n",
    "                elif len(parts) == 3:\n",
    "                    region = [int(parts[0].replace(\"chr\", \"\")), int(parts[1]), int(parts[2])]\n",
    "                elif len(parts) >= 4 and  region_list != ld_meta_data : # for eQTL where chr:start:end:gene_id:gene_name, and path if LD_meta are used.\n",
    "                    region = [int(parts[0].replace(\"chr\", \"\")), int(parts[1]), int(parts[2]),parts[3]]\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid region format in region_list\")\n",
    "        \n",
    "                region_dict[f\"{region[0]}:{region[1]}_{region[2]}\"] = region\n",
    "\n",
    "    if region_name:\n",
    "        for region in region_name:\n",
    "            parsed_region = parse_region(region)\n",
    "            region_key = f\"{parsed_region[0]}:{parsed_region[1]}_{parsed_region[2]}\"\n",
    "            if region_key not in region_dict:\n",
    "                region_dict[region_key] = parsed_region\n",
    "\n",
    "    return gwas_dict, region_dict\n",
    "\n",
    "gwas_dict, region_dict = load_regional_rss_data(gwas_meta_data, gwas_name, gwas_data, column_mapping, region_name, region_list)\n",
    "regional_data = dict([(\"GWAS\", gwas_dict), (\"regions\", region_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b72177c-e1c0-4d47-a62a-48e2b65d824d",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[univariate_rss]\n",
    "parameter: L = 5\n",
    "parameter: max_L = 10\n",
    "# If available the column that indicates sample size within the sumstats\n",
    "# filtering threshold for raiss imputation\n",
    "parameter: rcond = 0.01\n",
    "parameter: lamb = 0.01\n",
    "parameter: R2_threshold = 0.6\n",
    "parameter: l_step = 5\n",
    "parameter: pip_cutoff = 0.025\n",
    "parameter: skip_analysis_pip_cutoff = 0.025\n",
    "parameter: minimum_ld = 5\n",
    "parameter: coverage = [0.95, 0.7, 0.5]\n",
    "# Whether to impute the sumstat for all the snp in LD but not in sumstat.\n",
    "parameter: impute = False \n",
    "# summary stats QC methods: rss_qc, dentist, slalom\n",
    "parameter: qc_method = \"\"\n",
    "# analysis method: single_effect, susie_rss, bayesian_conditional_regression\n",
    "parameter: finemapping_method = \"single_effect\"\n",
    "parameter: extract_region_name = \"NULL\"\n",
    "parameter: region_name_col = \"NULL\"\n",
    "# compute LD from genotype\n",
    "parameter: compute_LD_from_genotype = False\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "parameter: imiss = 1.0\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.0025\n",
    "parameter: comment_string = \"NULL\"\n",
    "\n",
    "depends: sos_variable(\"regional_data\")\n",
    "regions = list(regional_data['regions'].keys())\n",
    "input: for_each = \"regions\"\n",
    "output: f'{cwd:a}/{step_name}/{\"%s\" % qc_method.upper() if qc_method else \"noQC\"}{\"_RAISS_imputed\" if impute else \"\"}.chr{_regions.replace(\":\", \"_\")}.univariate{\"_%s\" % finemapping_method if finemapping_method else \"\"}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    # library(pecotmr)\n",
    "    library(dplyr)\n",
    "    library(data.table)\n",
    "    skip_region = c(${', '.join(['\"{}\"'.format(item) for item in skip_regions])})\n",
    "    studies = c(${', '.join(['\"{}\"'.format(item) for item in regional_data[\"GWAS\"].keys()])})\n",
    "    sumstat_paths = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][0] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    column_file_paths = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][1] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_samples = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][2] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_cases = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][3] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_controls = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][4] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    if(\"${extract_region_name}\" == \"NULL\"){\n",
    "        extract_region_name = \"${regional_data['regions'][_regions][3] if len(regional_data['regions'][_regions]) > 3 else None}\"\n",
    "        if(extract_region_name == \"None\" | \"${region_name_col}\" == \"NULL\"){\n",
    "            extract_region_name = NULL\n",
    "        }\n",
    "    } else {\n",
    "        extract_region_name = \"${extract_region_name}\"\n",
    "    }\n",
    "\n",
    "    if( ${\"TRUE\" if compute_LD_from_genotype else \"FALSE\"}){\n",
    "        geno_path = readr::read_delim(\"${ld_meta_data}\",\"\\t\")%>% filter(`#chr` == \"${regional_data['regions'][_regions][0]}\", \n",
    "                                                                        start == \"${regional_data['regions'][_regions][1]}\", \n",
    "                                                                          end == \"${regional_data['regions'][_regions][2]}\")%>%pull(path)%>%stringr::str_replace(\".bed\",\"\")\n",
    "        LD_data = pecotmr:::filter_X(\n",
    "                                load_genotype_region(geno_path, \n",
    "                                            '${\"chr%s:%s-%s\" % (regional_data['regions'][_regions][0], regional_data['regions'][_regions][1], regional_data['regions'][_regions][2])}'),\n",
    "                                missing_rate_thresh = ${imiss}, maf_thresh=${maf})%>%\n",
    "                                cor\n",
    "    \n",
    "        ## When we compute LD from genotype adhoc, there could be snp with conventional Ref/Alt allele like  4:185875956:C:<INS:ME:ALU>, which will break the allele_qc function in the following steps.\n",
    "        ## The additional filtering remove these unconventional allele and keep only SNP that wont break the allele_qc function, which depends on the fact that there is exactly three \":\" in the snp ID.\n",
    "        correct_variants <-  rownames(LD_data)[sapply( rownames(LD_data), function(x) sum(strsplit(x, \"\", fixed = TRUE)[[1]] == \":\") == 3)]\n",
    "        LD_data = LD_data[correct_variants,correct_variants]\n",
    "        LD_data = list(combined_LD_matrix  = LD_data, combined_LD_variants  = rownames(LD_data) )\n",
    "    } else { \n",
    "        LD_data = load_LD_matrix(\"${ld_meta_data}\", '${\"chr%s:%s-%s\" % (regional_data['regions'][_regions][0], regional_data['regions'][_regions][1], regional_data['regions'][_regions][2])}')\n",
    "    }    \n",
    "\n",
    "\n",
    "    res = setNames(replicate(length(studies), list(), simplify = FALSE), studies)\n",
    "    for (r in 1:length(res)) {\n",
    "        tryCatch({\n",
    "        res[[r]] = rss_analysis_pipeline(sumstat_path = sumstat_paths[r], column_file_path = column_file_paths[r],\n",
    "                                        LD_data = LD_data, extract_region_name = extract_region_name, region_name_col = ${region_name_col}, n_sample = as.numeric(n_samples[r]), n_case = as.numeric(n_cases[r]), \n",
    "                                        n_control = as.numeric(n_controls[r]), skip_region = skip_region,\n",
    "                                        qc_method = ${\"NULL\" if len(qc_method) == 0 else \"'%s'\" % qc_method},\n",
    "                                        impute = ${\"TRUE\" if impute else \"FALSE\"},\n",
    "                                        impute_opts = list(rcond = ${rcond}, R2_threshold = ${R2_threshold}, minimum_ld = ${minimum_ld}, lamb=${lamb}),\n",
    "                                        finemapping_method = ${\"NULL\" if len(finemapping_method) == 0 else \"'%s'\" % finemapping_method}, \n",
    "                                        finemapping_opts = list(init_L = ${L}, max_L = ${max_L}, l_step = ${l_step}, coverage = c(${\",\".join([str(x) for x in coverage])}), signal_cutoff = ${pip_cutoff}),\n",
    "                                        pip_cutoff_to_skip = ${skip_analysis_pip_cutoff}, comment_string = ${\"NULL\" if comment_string == \"NULL\" else f\"'{comment_string}'\"}\n",
    "                                        )\n",
    "        fwrite(res[[r]]$rss_data_analyzed, file = paste0(\"${_output:nn}.\", extract_region_name, studies[r], \".sumstats.tsv.gz\"), sep = \"\\t\", col.names = TRUE, row.names = FALSE, quote = FALSE, compress = \"gzip\")\n",
    "        if(is.null(res[[r]][[1]])){\n",
    "              res[[r]] = list()\n",
    "        }\n",
    "      }, error = function(e) {\n",
    "        res[[r]] = list()\n",
    "        cat(paste(\"Error processing file \", studies[r], \": \", conditionMessage(e), \"\\n\"))\n",
    "      }\n",
    "      )}\n",
    "    region = \"chr${_regions.replace(\":\", \"_\")}\"\n",
    "    full_result = list()\n",
    "    full_result[[region]] = res\n",
    "    saveRDS(full_result, file = \"${_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-antarctica",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[univariate_plot]\n",
    "output: pip_plot = f\"{cwd}/{_input:bn}.png\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h', mem = '20G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container, expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint = entrypoint\n",
    "    res = readRDS(${_input:r})\n",
    "    png(${_output[0]:r}, width = 14, height=6, unit='in', res=300)\n",
    "    par(mfrow=c(1,2))\n",
    "    susieR::susie_plot(res, y= \"PIP\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\")\n",
    "    susieR::susie_plot(res, y= \"z\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\", ylab=\"-log10(p)\")\n",
    "    dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc6faa1-8e05-407b-a286-f27c320af56c",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[updated_rss]\n",
    "parameter: L = 5\n",
    "parameter: max_L = 10\n",
    "# If available the column that indicates sample size within the sumstats\n",
    "# filtering threshold for raiss imputation\n",
    "parameter: rcond = 0.01\n",
    "parameter: lamb = 0.01\n",
    "parameter: R2_threshold = 0.6\n",
    "parameter: l_step = 5\n",
    "parameter: pip_cutoff = 0.025\n",
    "parameter: skip_analysis_pip_cutoff = 0.025\n",
    "parameter: minimum_ld = 5\n",
    "parameter: coverage = [0.95, 0.7, 0.5]\n",
    "# Whether to impute the sumstat for all the snp in LD but not in sumstat.\n",
    "parameter: impute = False \n",
    "# summary stats QC methods: rss_qc, dentist, slalom\n",
    "parameter: qc_method = \"\"\n",
    "# analysis method: single_effect, susie_rss, bayesian_conditional_regression\n",
    "parameter: finemapping_method = \"single_effect\"\n",
    "parameter: extract_region_name = \"NULL\"\n",
    "parameter: region_name_col = \"NULL\"\n",
    "# compute LD from genotype\n",
    "parameter: compute_LD_from_genotype = False\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "parameter: imiss = 1.0\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.0025\n",
    "parameter: comment_string = \"NULL\"\n",
    "\n",
    "depends: sos_variable(\"regional_data\")\n",
    "regions = list(regional_data['regions'].keys())\n",
    "input: for_each = \"regions\"\n",
    "output: f'{cwd:a}/{step_name}/{\"%s\" % qc_method.upper() if qc_method else \"noQC\"}{\"_RAISS_imputed\" if impute else \"\"}.chr{_regions.replace(\":\", \"_\")}.univariate{\"_%s\" % finemapping_method if finemapping_method else \"\"}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(pecotmr)\n",
    "    library(dplyr)\n",
    "    library(data.table)\n",
    "    # Function to extract cs info and turn it into dataframe\n",
    "    process_cs <- function(data, cs_names, top_loci_table) {\n",
    "      # Function to calculate p-value from z-score\n",
    "      z_to_pvalue <- function(z) {\n",
    "        2 * pnorm(-abs(z))\n",
    "      }\n",
    "      \n",
    "      results <- map(seq_along(cs_names), function(i) {\n",
    "        cs_name <- cs_names[i]\n",
    "        indices <- data$susie_result_trimmed$sets$cs[[cs_name]]\n",
    "        \n",
    "        # Get variants for this CS using the full variant_names list\n",
    "        cs_variants <- data$variant_names[indices]\n",
    "        cs_data <- top_loci_table[top_loci_table$variant_id %in% cs_variants, ]\n",
    "        top_row <- which.max(cs_data$pip)\n",
    "        \n",
    "        top_variant <- cs_data$variant_id[top_row]\n",
    "        # Find the global index of the top variant\n",
    "        top_variant_global_index = which(data$variant_names == top_variant)\n",
    "        top_pip <- cs_data$pip[top_row]\n",
    "        top_z <- cs_data$z[top_row]\n",
    "        p_value <- z_to_pvalue(top_z)\n",
    "        \n",
    "        # Extract cs_corr\n",
    "        cs_corr <- if (length(cs_names) > 1) {\n",
    "          data$susie_result_trimmed$cs_corr[i,]\n",
    "        } else {\n",
    "          NA  # Use NA for the second CS or when there's only one CS\n",
    "        }\n",
    "        \n",
    "        # Return results for this CS as a one-row data.frame\n",
    "        result = tibble::tibble(\n",
    "          cs_name = cs_name,\n",
    "          variants_per_cs = length(cs_variants),\n",
    "          top_variant = top_variant,\n",
    "          top_variant_index = top_variant_global_index,\n",
    "          top_pip = top_pip,\n",
    "          top_z = top_z,\n",
    "          p_value = p_value,\n",
    "          cs_corr = list(cs_corr)  # list column if cs_corr is a vector\n",
    "        )\n",
    "        return(result)\n",
    "      })\n",
    "      # Combine all tibbles into one data frame\n",
    "      final_result <- dplyr::bind_rows(results)\n",
    "      return(final_result)\n",
    "    }\n",
    "    # Function to sparse cs_corr columns of \n",
    "    # Convert cs_corr into multiple columns, remaining its order\n",
    "    parse_cs_corr_dt <- function(df) {\n",
    "      # Convert to data.table if not already\n",
    "      if (!is.data.table(df)) {\n",
    "        setDT(df)\n",
    "      }\n",
    "      \n",
    "      # Function to extract correlations\n",
    "      extract_correlations <- function(x) {\n",
    "        if(is.na(x) || x == \"\" || !grepl(\"\\\\|\", x)) {\n",
    "          return(list(values = numeric(0), max_corr = NA, min_corr = NA))\n",
    "        }\n",
    "        \n",
    "        values <- as.numeric(unlist(strsplit(as.character(x), \"\\\\|\")))\n",
    "        \n",
    "        \n",
    "        if(length(values) == 0) {\n",
    "          return(list(values = numeric(0), max_corr = NA, min_corr = NA))\n",
    "        }\n",
    "        values_filtered <- abs(values[values != 1])\n",
    "        return(list(\n",
    "          values = values,\n",
    "          max_corr = max(values_filtered, na.rm = TRUE),\n",
    "          min_corr = min(values_filtered, na.rm = TRUE)\n",
    "        ))\n",
    "      }\n",
    "      \n",
    "      # Process correlations\n",
    "      processed_results <- lapply(df$cs_corr, extract_correlations)\n",
    "      \n",
    "      # Determine max number of correlations\n",
    "      max_corr_count <- max(sapply(processed_results, function(x) length(x$values)))\n",
    "      \n",
    "      # Create column names\n",
    "      col_names <- paste0(\"cs_corr_\", 1:max_corr_count)\n",
    "      \n",
    "      # Prepare a list of correlation values for each column\n",
    "      corr_list <- lapply(1:max_corr_count, function(j) {\n",
    "        sapply(processed_results, function(x) {\n",
    "          if(length(x$values) >= j) x$values[j] else NA_real_\n",
    "        })\n",
    "      })\n",
    "      \n",
    "      # Add columns to the data.table\n",
    "      for(i in seq_along(col_names)) {\n",
    "        df[, (col_names[i]) := corr_list[[i]]]\n",
    "      }\n",
    "      \n",
    "      # Add max and min columns\n",
    "      df[, cs_corr_max := sapply(processed_results, `[[`, \"max_corr\")]\n",
    "      df[, cs_corr_min := sapply(processed_results, `[[`, \"min_corr\")]\n",
    "      \n",
    "      return(df)\n",
    "    }\n",
    "    # Function to categorize study_blocks into different updating strategies\n",
    "    process_cs_info_group <- function(group_data, high_corr_cols) {\n",
    "      # Identify top_cs\n",
    "      top_cs_index <- which.max(abs(group_data$top_z))\n",
    "      group_data$top_cs <- FALSE\n",
    "      group_data$top_cs[top_cs_index] <- TRUE\n",
    "    \n",
    "      # Identify tagged_cs\n",
    "      group_data$tagged_cs <- sapply(1:nrow(group_data), function(i) {\n",
    "        if (group_data$top_cs[i]) return(FALSE)\n",
    "        if (group_data$p_value[i] > 1e-5) return(TRUE)\n",
    "        if (length(high_corr_cols) == 0) return(FALSE)\n",
    "        any(sapply(high_corr_cols, function(col) group_data[i, ..col] == 1))\n",
    "      })\n",
    "    \n",
    "      # Count total and remaining CS\n",
    "      total_cs <- nrow(group_data)\n",
    "      tagged_cs_count <- sum(group_data$tagged_cs)\n",
    "      remaining_cs <- total_cs - 1 - tagged_cs_count\n",
    "    \n",
    "      # Determine method\n",
    "      group_data$method <- case_when(\n",
    "        total_cs == 1 | tagged_cs_count == 0 ~ \"BVSR\",\n",
    "        remaining_cs == 0 ~ \"SER\",\n",
    "        remaining_cs > 0 ~ \"BCR\",\n",
    "        TRUE ~ NA_character_\n",
    "      )\n",
    "    \n",
    "      return(group_data)\n",
    "    }\n",
    "    skip_region = c(${', '.join(['\"{}\"'.format(item) for item in skip_regions])})\n",
    "    studies = c(${', '.join(['\"{}\"'.format(item) for item in regional_data[\"GWAS\"].keys()])})\n",
    "    sumstat_paths = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][0] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    column_file_paths = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][1] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_samples = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][2] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_cases = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][3] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_controls = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][4] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    if(\"${extract_region_name}\" == \"NULL\"){\n",
    "        extract_region_name = \"${regional_data['regions'][_regions][3] if len(regional_data['regions'][_regions]) > 3 else None}\"\n",
    "        if(extract_region_name == \"None\" | \"${region_name_col}\" == \"NULL\"){\n",
    "            extract_region_name = NULL\n",
    "        }\n",
    "    } else {\n",
    "        extract_region_name = \"${extract_region_name}\"\n",
    "    }\n",
    "\n",
    "    if( ${\"TRUE\" if compute_LD_from_genotype else \"FALSE\"}){\n",
    "        geno_path = readr::read_delim(\"${ld_meta_data}\",\"\\t\")%>% filter(`#chr` == \"${regional_data['regions'][_regions][0]}\", \n",
    "                                                                        start == \"${regional_data['regions'][_regions][1]}\", \n",
    "                                                                          end == \"${regional_data['regions'][_regions][2]}\")%>%pull(path)%>%stringr::str_replace(\".bed\",\"\")\n",
    "        LD_data = pecotmr:::filter_X(\n",
    "                                load_genotype_region(geno_path, \n",
    "                                            '${\"chr%s:%s-%s\" % (regional_data['regions'][_regions][0], regional_data['regions'][_regions][1], regional_data['regions'][_regions][2])}'),\n",
    "                                missing_rate_thresh = ${imiss}, maf_thresh=${maf})%>%\n",
    "                                cor\n",
    "    \n",
    "        ## When we compute LD from genotype adhoc, there could be snp with conventional Ref/Alt allele like  4:185875956:C:<INS:ME:ALU>, which will break the allele_qc function in the following steps.\n",
    "        ## The additional filtering remove these unconventional allele and keep only SNP that wont break the allele_qc function, which depends on the fact that there is exactly three \":\" in the snp ID.\n",
    "        correct_variants <-  rownames(LD_data)[sapply( rownames(LD_data), function(x) sum(strsplit(x, \"\", fixed = TRUE)[[1]] == \":\") == 3)]\n",
    "        LD_data = LD_data[correct_variants,correct_variants]\n",
    "        LD_data = list(combined_LD_matrix  = LD_data, combined_LD_variants  = rownames(LD_data) )\n",
    "    } else { \n",
    "        LD_data = load_LD_matrix(\"${ld_meta_data}\", '${\"chr%s:%s-%s\" % (regional_data['regions'][_regions][0], regional_data['regions'][_regions][1], regional_data['regions'][_regions][2])}')\n",
    "    }    \n",
    "\n",
    "\n",
    "    bvsr = setNames(replicate(length(studies), list(), simplify = FALSE), studies)\n",
    "    ser = setNames(replicate(length(studies), list(), simplify = FALSE), studies)\n",
    "    bcr = setNames(replicate(length(studies), list(), simplify = FALSE), studies)\n",
    "\n",
    "    for (r in 1:length(bvsr)) {\n",
    "        tryCatch({\n",
    "            bvsr[[r]] = rss_analysis_pipeline(sumstat_path = sumstat_paths[r], column_file_path = column_file_paths[r],\n",
    "                LD_data = LD_data, extract_region_name = extract_region_name, region_name_col = ${region_name_col}, n_sample = as.numeric(n_samples[r]), n_case = as.numeric(n_cases[r]), \n",
    "                n_control = as.numeric(n_controls[r]), skip_region = skip_region,\n",
    "                qc_method = ${\"NULL\" if len(qc_method) == 0 else \"'%s'\" % qc_method},\n",
    "                impute = ${\"TRUE\" if impute else \"FALSE\"},\n",
    "                impute_opts = list(rcond = ${rcond}, R2_threshold = ${R2_threshold}, minimum_ld = ${minimum_ld}, lamb=${lamb}),\n",
    "                finemapping_method = ${\"NULL\" if len(finemapping_method) == 0 else \"'%s'\" % finemapping_method}, \n",
    "                finemapping_opts = list(init_L = ${L}, max_L = ${max_L}, l_step = ${l_step}, coverage = c(${\",\".join([str(x) for x in coverage])}), signal_cutoff = ${pip_cutoff}),\n",
    "                pip_cutoff_to_skip = ${skip_analysis_pip_cutoff}, comment_string = ${\"NULL\" if comment_string == \"NULL\" else f\"'{comment_string}'\"}\n",
    "                )\n",
    "            fwrite(bvsr[[r]]$rss_data_analyzed, file = paste0(\"${_output:nn}.\", extract_region_name, studies[r], \".sumstats.tsv.gz\"), sep = \"\\t\", col.names = TRUE, row.names = FALSE, quote = FALSE, compress = \"gzip\")\n",
    "            if(is.null(bvsr[[r]][[1]])){\n",
    "                  bvsr[[r]] = list()\n",
    "            }\n",
    "            \n",
    "        }, error = function(e) {\n",
    "        bvsr[[r]] = list()\n",
    "        cat(paste(\"Error processing BVSR file \", studies[r], \": \", conditionMessage(e), \"\\n\"))\n",
    "        })\n",
    "        tryCatch({\n",
    "            bvsr_con_data = bvsr[[r]]$RSS_QC_RAISS_imputed\n",
    "            if (length(bvsr_con_data) > 0) {\n",
    "                bvsr_res = get_res(bvsr_con_data)\n",
    "                bvsr_cs_num = if(!is.null(bvsr_res)) length(bvsr_res$sets$cs) else NULL\n",
    "                if (isTRUE(bvsr_cs_num > 0)) {\n",
    "                    # Get the names of the credible sets\n",
    "                    cs_names_bvsr = names(bvsr_res$sets$cs)\n",
    "                    block_cs_metrics[[r]] = process_cs(data = bvsr_con_data, cs_names = cs_names_bvsr, top_loci_table = bvsr_con_data$top_loci)\n",
    "                    # Add file and study information to block_cs_metrics\n",
    "                    # if (!is.null(block_cs_metrics) && nrow(block_cs_metrics) > 0) {\n",
    "                    #     block_cs_metrics$study = r\n",
    "                    # }\n",
    "                }\n",
    "            }\n",
    "            block_cs_metrics_exp[[r]] = parse_cs_corr_dt(block_cs_metrics[[r]])\n",
    "            if (nrow(block_cs_metrics_exp[[r]]) > 0) {  # Check if there's data for this combination\n",
    "                # Identify high correlation columns for top_cs\n",
    "                top_cs_index <- which.max(abs(block_cs_metrics_exp[[r]]$top_z))\n",
    "                # Get the column names that match the pattern \"cs_corr_\" followed by numbers\n",
    "                cs_corr_cols <- grep(\"^cs_corr_\\\\d+$\", names(block_cs_metrics_exp[[r]]), value = TRUE)\n",
    "                # Initialize an empty list to store high correlation columns\n",
    "                high_corr_cols <- list()\n",
    "                # Iterate through the correlation columns and check for correlations > 0.3\n",
    "                for (col in cs_corr_cols) {\n",
    "                  if (!is.na(block_cs_metrics_exp[[r]][top_cs_index, ..col]) && abs(block_cs_metrics_exp[[r]][top_cs_index, ..col]) > 0.3) {\n",
    "                    high_corr_cols <- c(high_corr_cols, col)\n",
    "                  }\n",
    "                }\n",
    "                # Process the data\n",
    "                descision[[r]] <- process_cs_info_group(block_cs_metrics_exp[[r]], high_corr_cols)\n",
    "                \n",
    "        }, error = function(e) {\n",
    "        block_cs_metrics[[r]] = list()\n",
    "        descision[[r]] = list()\n",
    "        cat(paste(\"Error processing diagnostic table \", studies[r], \": \", conditionMessage(e), \"\\n\"))\n",
    "        })\n",
    "        if (decision[[r]]$method == 'SER') {\n",
    "            tryCatch({\n",
    "                ser[[r]] = rss_analysis_pipeline(sumstat_path = sumstat_paths[r], column_file_path = column_file_paths[r],\n",
    "                    LD_data = LD_data, extract_region_name = extract_region_name, region_name_col = ${region_name_col}, n_sample = as.numeric(n_samples[r]), n_case = as.numeric(n_cases[r]), \n",
    "                    n_control = as.numeric(n_controls[r]), skip_region = skip_region,\n",
    "                    qc_method = ${\"NULL\" if len(qc_method) == 0 else \"'%s'\" % qc_method},\n",
    "                    impute = ${\"TRUE\" if impute else \"FALSE\"},\n",
    "                    impute_opts = list(rcond = ${rcond}, R2_threshold = ${R2_threshold}, minimum_ld = ${minimum_ld}, lamb=${lamb}),\n",
    "                    finemapping_method = 'SER', \n",
    "                    finemapping_opts = list(init_L = ${L}, max_L = ${max_L}, l_step = ${l_step}, coverage = c(${\",\".join([str(x) for x in coverage])}), signal_cutoff = ${pip_cutoff}),\n",
    "                    pip_cutoff_to_skip = ${skip_analysis_pip_cutoff}, comment_string = ${\"NULL\" if comment_string == \"NULL\" else f\"'{comment_string}'\"}\n",
    "                    )\n",
    "                if(is.null(ser[[r]][[1]])){\n",
    "                      ser[[r]] = list()\n",
    "                }\n",
    "            }, error = function(e) {\n",
    "            ser[[r]] = list()\n",
    "            cat(paste(\"Error processing SER file \", studies[r], \": \", conditionMessage(e), \"\\n\"))\n",
    "            })\n",
    "            # Extract the content of ser[r]\n",
    "            ser_content <- ser[[r]]$RSS_QC_RAISS_imputed\n",
    "            # Create a new list with the desired structure\n",
    "            new_ser <- list(\n",
    "              SER = ser_content\n",
    "            )\n",
    "            # Assign this new list to bvsr[[r]]$SER\n",
    "            bvsr[[r]]$SER <- new_ser$SER\n",
    "\n",
    "        } elif (decision[[r]]$method == 'BCR') {\n",
    "            tryCatch({\n",
    "                ser[[r]] = rss_analysis_pipeline(sumstat_path = sumstat_paths[r], column_file_path = column_file_paths[r],\n",
    "                    LD_data = LD_data, extract_region_name = extract_region_name, region_name_col = ${region_name_col}, n_sample = as.numeric(n_samples[r]), n_case = as.numeric(n_cases[r]), \n",
    "                    n_control = as.numeric(n_controls[r]), skip_region = skip_region,\n",
    "                    qc_method = ${\"NULL\" if len(qc_method) == 0 else \"'%s'\" % qc_method},\n",
    "                    impute = ${\"TRUE\" if impute else \"FALSE\"},\n",
    "                    impute_opts = list(rcond = ${rcond}, R2_threshold = ${R2_threshold}, minimum_ld = ${minimum_ld}, lamb=${lamb}),\n",
    "                    finemapping_method = 'SER', \n",
    "                    finemapping_opts = list(init_L = ${L}, max_L = ${max_L}, l_step = ${l_step}, coverage = c(${\",\".join([str(x) for x in coverage])}), signal_cutoff = ${pip_cutoff}),\n",
    "                    pip_cutoff_to_skip = ${skip_analysis_pip_cutoff}, comment_string = ${\"NULL\" if comment_string == \"NULL\" else f\"'{comment_string}'\"}\n",
    "                    )\n",
    "                if(is.null(ser[[r]][[1]])){\n",
    "                      ser[[r]] = list()\n",
    "                }\n",
    "            }, error = function(e) {\n",
    "            ser[[r]] = list()\n",
    "            cat(paste(\"Error processing SER file \", studies[r], \": \", conditionMessage(e), \"\\n\"))\n",
    "            })\n",
    "            # Extract the content of ser[r]\n",
    "            ser_content <- ser[[r]]$RSS_QC_RAISS_imputed\n",
    "            # Create a new list with the desired structure\n",
    "            new_ser <- list(\n",
    "              SER = ser_content\n",
    "            )\n",
    "            # Assign this new list to bvsr[[r]]$SER\n",
    "            bvsr[[r]]$SER <- new_ser$SER\n",
    "\n",
    "            tryCatch({\n",
    "                bcr[[r]] = rss_analysis_pipeline(sumstat_path = sumstat_paths[r], column_file_path = column_file_paths[r],\n",
    "                    LD_data = LD_data, extract_region_name = extract_region_name, region_name_col = ${region_name_col}, n_sample = as.numeric(n_samples[r]), n_case = as.numeric(n_cases[r]), \n",
    "                    n_control = as.numeric(n_controls[r]), skip_region = skip_region,\n",
    "                    qc_method = ${\"NULL\" if len(qc_method) == 0 else \"'%s'\" % qc_method},\n",
    "                    impute = ${\"TRUE\" if impute else \"FALSE\"},\n",
    "                    impute_opts = list(rcond = ${rcond}, R2_threshold = ${R2_threshold}, minimum_ld = ${minimum_ld}, lamb=${lamb}),\n",
    "                    finemapping_method = 'BCR', \n",
    "                    finemapping_opts = list(init_L = ${L}, max_L = ${max_L}, l_step = ${l_step}, coverage = c(${\",\".join([str(x) for x in coverage])}), signal_cutoff = ${pip_cutoff}),\n",
    "                    pip_cutoff_to_skip = ${skip_analysis_pip_cutoff}, comment_string = ${\"NULL\" if comment_string == \"NULL\" else f\"'{comment_string}'\"}\n",
    "                    )\n",
    "                if(is.null(bcr[[r]][[1]])){\n",
    "                      bcr[[r]] = list()\n",
    "                }\n",
    "            }, error = function(e) {\n",
    "            bcr[[r]] = list()\n",
    "            cat(paste(\"Error processing BCR file \", studies[r], \": \", conditionMessage(e), \"\\n\"))\n",
    "            })\n",
    "            # Extract the content of BCR[r]\n",
    "            bcr_content <- bcr[[r]]$RSS_QC_RAISS_imputed\n",
    "            # Create a new list with the desired structure\n",
    "            new_bcr <- list(\n",
    "              BCR = bcr_content\n",
    "            )\n",
    "            # Assign this new list to bvsr[[r]]$BCR\n",
    "            bvsr[[r]]$BCR <- new_bcr$BCR\n",
    "            # Extract the content of bcr[r]\n",
    "            bcr_content <- bcr[[r]]$RSS_QC_RAISS_imputed\n",
    "            # Create a new list with the desired structure\n",
    "            new_bcr <- list(\n",
    "              BCR = bcr_content\n",
    "            )\n",
    "            # Assign this new list to bvsr[[r]]$BCR\n",
    "            bvsr[[r]]$BCR <- new_bcr$BCR\n",
    "        } else { ## method == 'BVSR'\n",
    "            index <- which(names(bvsr[[r]]) == \"RSS_QC_RAISS_imputed\")\n",
    "            names(bvsr[[r]])[index] <- \"BVSR\"\n",
    "        }\n",
    "    region = \"chr${_regions.replace(\":\", \"_\")}\"\n",
    "    full_result = list()\n",
    "    full_result[[region]] = bvsr\n",
    "    diag_tb = list()\n",
    "    diag_tb[[region]] = block_cs_metrics\n",
    "    final_metrics_per_cs = list()\n",
    "    final_metrics_per_cs[[region]] = decision\n",
    "    df <- do.call(rbind, lapply(final_metrics_per_cs, data.frame))\n",
    "    df$region <- names(final_metrics_per_cs)\n",
    "    fwrite(df, file = paste0(\"${_output:nn}.\", \".diagnostics.tsv.gz\"), sep = \"\\t\", col.names = TRUE, row.names = FALSE, quote = FALSE, compress = \"gzip\")\n",
    "\n",
    "    saveRDS(full_result, file = \"${_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
