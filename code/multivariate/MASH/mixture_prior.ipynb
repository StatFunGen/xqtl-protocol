{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# A multivariate EBNM approach for mixture multivariate distribution estimate\n",
    "\n",
    "This is a updated version from the bioworkflow/mixture_prior analysis, the steps where mixture components were generated are removed to not duplicate with that of the mashr_flashr_workflow. In this notebook, we compute a prior independent of the specific analysis method chosen for the data. This foundational step enables the application of various techniques, such as UDR, ED, TED, and initialization with FLASHier, among others. Essentially, our goal is to establish a mixture model to extract meaningful signals from the data.\n",
    "\n",
    "An earlier version of the approach is outlined in Urbut et al 2019. This workflow implements a few improvements including using additional EBMF methods as well as the new `udr` package (latest update: May 2023) to fit the mixture model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview of approach\n",
    "\n",
    "1. Factor analysis;\n",
    "2. Estimate residual variance;\n",
    "3. Compute MASH prior;\n",
    "4. _prior plot_\n",
    "\n",
    "## Input\n",
    "formatted QTL summary statistics \n",
    "\n",
    "## Output\n",
    "1. residual variance\n",
    "2. MASH prior\n",
    "3. _prior plot_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Multivariate adaptive shrinkage (MASH) analysis of eQTL data\n",
    "\n",
    "Below is a \"blackbox\" implementation of the `mashr` eQTL workflow -- blackbox in the sense that you can run this pipeline as an executable, without thinking too much about it, if you see your problem fits our GTEx analysis scheme. However when reading it as a notebook it is a good source of information to help developing your own `mashr` analysis procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Since the submission to biorxiv of Urbut 2017 we have improved implementation of MASH algorithm and made a new R package, [`mashr`](https://github.com/stephenslab/mashr). Major improvements compared to Urbut 2019 are:\n",
    "\n",
    "1. Faster computation of likelihood and posterior quantities via matrix algebra tricks and a C++ implementation.\n",
    "2. Faster computation of MASH mixture via convex optimization.\n",
    "3. Replace `SFA` with `FLASH`, a new sparse factor analysis method to generate prior covariance candidates.\n",
    "4. Improve estimate of residual variance $\\hat{V}$.\n",
    "\n",
    "At this point, the input data have already been converted from the original eQTL summary statistics to a format convenient for analysis in MASH, as a result of running the data conversion pipeline in `fastqtl_to_mash.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## [UD](https://stephenslab.github.io/udr/) \n",
    "Implement fast statistical algorithms for solving the multivariate normal means problem via empirical Bayes, building on the Extreme Deconvolution method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "/home/rf2872/Work/Multivariate/MASH/MWE/output/MWE.rds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### prepare the MWE sample list\n",
    "cd /home/rf2872/Work/Multivariate/MASH/MWE\n",
    "\n",
    "head -n 100 /mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MASH_test_csg/output/ALL_Ast_End_Exc_Inh_Mic_OPC_Oli.merged_rds.list > MWE.list\n",
    "sed -i 's@/mnt/vast/hpc/csg/rf2872/Work/@/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/@g' MWE.list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mextract_effects_1\u001b[0m: extract data for MASH from summary stats\n",
      "INFO: \u001b[32mextract_effects_1\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mextract_effects_1\u001b[0m output:   \u001b[32moutput/MWE/cache/MWE_1.rds\u001b[0m\n",
      "INFO: Running \u001b[32mextract_effects_2\u001b[0m: \n",
      "INFO: \u001b[32mextract_effects_2\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mextract_effects_2\u001b[0m output:   \u001b[32moutput/MWE.rds\u001b[0m\n",
      "INFO: Workflow extract_effects (ID=w94c40e27c1948fc0) is executed successfully with 2 completed steps.\n"
     ]
    }
   ],
   "source": [
    "sos run pipeline/Signal_Extraction.ipynb extract_effects     \\\n",
    "    --name  MWE    \\\n",
    "    --analysis_units   <(cat MWE.list | cut -f 2 ) \\\n",
    "    --need_genename TRUE \\\n",
    "    --sum_stat  MWE.list \\\n",
    "    --exclude_condition 1,3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Generate prior with MWE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mvhat_simple\u001b[0m: V estimate: \"simple\" method (using null z-scores)\n",
      "INFO: \u001b[32mvhat_simple\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mvhat_simple\u001b[0m output:   \u001b[32m/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MWE/MWE_udr/MWE_udr.EZ.V_simple.rds\u001b[0m\n",
      "INFO: Running \u001b[32mud\u001b[0m: Latest update: May/2023\n",
      "INFO: \u001b[32mud\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mud\u001b[0m output:   \u001b[32m/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MWE/MWE_udr/MWE_udr.EZ.prior.rds\u001b[0m\n",
      "INFO: Workflow ud (ID=wb4fb2e1096c2b52e) is executed successfully with 2 completed steps.\n"
     ]
    }
   ],
   "source": [
    "#1: mixture_prior\n",
    "sos run pipeline/mixture_prior.ipynb ud \\\n",
    "    --container /mnt/vast/hpc/csg/containers_xqtl/stephenslab.sif \\\n",
    "    --output_prefix MWE_udr \\\n",
    "    --data output/MWE.rds \\\n",
    "    --cwd MWE_udr --vhat mle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32med_bovy\u001b[0m: \n",
      "INFO: Running \u001b[32mvhat_simple\u001b[0m: V estimate: \"simple\" method (using null z-scores)\n",
      "INFO: \u001b[32mvhat_simple\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mvhat_simple\u001b[0m output:   \u001b[32m/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MWE/MWE_ed_bovy/MWE_ed_bovy.EZ.V_simple.rds\u001b[0m\n",
      "INFO: Running \u001b[32mflash\u001b[0m: Perform FLASH analysis with non-negative factor constraint (time estimate: 20min)\n",
      "INFO: \u001b[32mflash\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mflash\u001b[0m output:   \u001b[32m/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MWE/MWE_ed_bovy/MWE_ed_bovy.flash.rds\u001b[0m\n",
      "INFO: Running \u001b[32mflash_nonneg\u001b[0m: Perform FLASH analysis with non-negative factor constraint (time estimate: 20min)\n",
      "INFO: \u001b[32mflash_nonneg\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mflash_nonneg\u001b[0m output:   \u001b[32m/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MWE/MWE_ed_bovy/MWE_ed_bovy.flash_nonneg.rds\u001b[0m\n",
      "INFO: Running \u001b[32mpca\u001b[0m: \n",
      "INFO: \u001b[32mpca\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mpca\u001b[0m output:   \u001b[32m/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MWE/MWE_ed_bovy/MWE_ed_bovy.pca.rds\u001b[0m\n",
      "INFO: Running \u001b[32mcanonical\u001b[0m: \n",
      "INFO: \u001b[32mcanonical\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32mcanonical\u001b[0m output:   \u001b[32m/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MWE/MWE_ed_bovy/MWE_ed_bovy.canonical.rds\u001b[0m\n",
      "INFO: \u001b[32med_bovy\u001b[0m is \u001b[32mcompleted\u001b[0m.\n",
      "INFO: \u001b[32med_bovy\u001b[0m output:   \u001b[32m/mnt/vast/hpc/csg/rf2872/Work/Multivariate/MASH/MWE/MWE_ed_bovy/MWE_ed_bovy.EZ.prior.rds\u001b[0m\n",
      "INFO: Workflow ed_bovy (ID=wd3de480c7c7fbce8) is executed successfully with 6 completed steps.\n"
     ]
    }
   ],
   "source": [
    "#1: mixture_prior\n",
    "sos run pipeline/mixture_prior.ipynb ed_bovy \\\n",
    "    --container /mnt/vast/hpc/csg/containers_xqtl/stephenslab.sif \\\n",
    "    --output_prefix MWE_ed_bovy \\\n",
    "    --data output/MWE.rds \\\n",
    "    --cwd MWE_ed_bovy --vhat mle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('./mashr_flashr_workflow_output')\n",
    "# Input summary statistics data\n",
    "parameter: data = path(\"fastqtl_to_mash_output/FastQTLSumStats.mash.rds\")\n",
    "# Prefix of output files. If not specified, it will derive it from data.\n",
    "# If it is specified, for example, `--output-prefix AnalysisResults`\n",
    "# It will save output files as `{cwd}/AnalysisResults*`.\n",
    "parameter: output_prefix = ''\n",
    "parameter: output_suffix = 'all'\n",
    "# Exchangable effect (EE) or exchangable z-scores (EZ)\n",
    "parameter: effect_model = 'EZ'\n",
    "# Identifier of $\\hat{V}$ estimate file\n",
    "# Options are \"identity\", \"simple\", \"mle\", \"vhat_corshrink_xcondition\", \"vhat_simple_specific\"\n",
    "parameter: vhat = 'simple'\n",
    "parameter: mixture_components = ['flash', 'flash_nonneg', 'pca',\"canonical\"]\n",
    "parameter: container = \"\"\n",
    "parameter: entrypoint={('micromamba run -a \"\" -n' + ' ' + container.split('/')[-1][:-4]) if container.endswith('.sif') else f''}\n",
    "data = data.absolute()\n",
    "cwd = cwd.absolute()\n",
    "if len(output_prefix) == 0:\n",
    "    output_prefix = f\"{data:bn}\"\n",
    "prior_data = file_target(f\"{cwd:a}/{output_prefix}.{effect_model}.prior.rds\")\n",
    "vhat_data = file_target(f\"{cwd:a}/{output_prefix}.{effect_model}.V_{vhat}.rds\")\n",
    "\n",
    "def sort_uniq(seq):\n",
    "    seen = set()\n",
    "    return [x for x in seq if not (x in seen or seen.add(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Factor analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Perform FLASH analysis with non-negative factor constraint (time estimate: 20min)\n",
    "[flash]\n",
    "input: data\n",
    "output: f\"{cwd}/{output_prefix}.flash.rds\"\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = mashr::cov_flash(dat, factors=\"default\", remove_singleton=${\"TRUE\" if \"canonical\" in mixture_components else \"FALSE\"}, output_model=\"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Perform FLASH analysis with non-negative factor constraint (time estimate: 20min)\n",
    "[flash_nonneg]\n",
    "input: data\n",
    "output: f\"{cwd}/{output_prefix}.flash_nonneg.rds\"\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = mashr::cov_flash(dat, factors=\"nonneg\", remove_singleton=${\"TRUE\" if \"canonical\" in mixture_components else \"FALSE\"}, output_model=\"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[pca]\n",
    "# Number of components in PCA analysis for prior\n",
    "# set to 3 as in mash paper\n",
    "parameter: npc = 2\n",
    "input: data\n",
    "output: f\"{cwd}/{output_prefix}.pca.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = mashr::cov_pca(dat, ${npc})\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[canonical]\n",
    "input: data\n",
    "output: f\"{cwd}/{output_prefix}.canonical.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    library(\"mashr\")\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = mashr::cov_canonical(dat)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Estimate residual variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"identity\" method\n",
    "[vhat_identity]\n",
    "input: data\n",
    "output: f'{vhat_data:nn}.V_identity.rds'\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    dat = readRDS(${_input:r})\n",
    "    saveRDS(diag(ncol(dat$random.b)), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"simple\" method (using null z-scores)\n",
    "[vhat_simple]\n",
    "\n",
    "input: data\n",
    "output: f'{vhat_data:nn}.V_simple.rds'\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input:r})\n",
    "    vhat = estimate_null_correlation_simple(mash_set_data(dat$random.b, Shat=dat$random.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3))\n",
    "    saveRDS(vhat, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"mle\" method\n",
    "[vhat_mle]\n",
    "# number of samples to use\n",
    "parameter: n_subset = 6000\n",
    "# maximum number of iterations\n",
    "parameter: max_iter = 6\n",
    "\n",
    "input: data, prior_data\n",
    "output: f'{vhat_data:nn}.V_mle.rds'\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    # choose random subset\n",
    "    set.seed(1)\n",
    "    random.subset = sample(1:nrow(dat$random.b), min(${n_subset}, nrow(dat$random.b)))\n",
    "    random.subset = mash_set_data(dat$random.b[random.subset,], dat$random.s[random.subset,], alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    # estimate V mle\n",
    "    vhatprior = mash_estimate_corr_em(random.subset, readRDS(${_input[1]:r}), max_iter = ${max_iter})\n",
    "    vhat = vhatprior$V\n",
    "    saveRDS(vhat, ${_output:r})\n",
    "    saveRDS(vhat, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Estimate each V separately via corshrink\n",
    "[vhat_corshrink_xcondition_1]\n",
    "# Utility script\n",
    "parameter: util_script = path('/project/mstephens/gtex/scripts/SumstatQuery.R')\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "fail_if(not util_script.is_file() and len(str(util_script)), msg = 'Please specify valid path for --util-script')\n",
    "genes = sort_uniq([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "\n",
    "depends: R_library(\"CorShrink\")\n",
    "input: data, for_each = 'genes'\n",
    "output: f'{vhat_data:nn}/{vhat_data:bnn}_V_corshrink_{_genes}.rds'\n",
    "task: trunk_workers = 1, walltime = '3m', trunk_size = 500, mem = '3G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    source(${util_script:r})\n",
    "    CorShrink_sum = function(gene, database, z_thresh = 2){\n",
    "      print(gene)\n",
    "      dat <- GetSS(gene, database)\n",
    "      z = dat$\"z-score\"\n",
    "      max_absz = apply(abs(z), 1, max)\n",
    "      nullish = which(max_absz < z_thresh)\n",
    "      # if (length(nullish) < ncol(z)) {\n",
    "        # stop(\"not enough null data to estimate null correlation\")\n",
    "      # }\n",
    "      if (length(nullish) <= 1){\n",
    "        mat = diag(ncol(z))\n",
    "      } else {\n",
    "        nullish_z = z[nullish, ]  \n",
    "        mat = as.matrix(CorShrink::CorShrinkData(nullish_z, ash.control = list(mixcompdist = \"halfuniform\"))$cor)\n",
    "      }\n",
    "      return(mat)\n",
    "    }\n",
    "    V = Corshrink_sum(\"${_genes}\", ${data:r})\n",
    "    saveRDS(V, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Estimate each V separately via \"simple\" method\n",
    "[vhat_simple_specific_1]\n",
    "# Utility script\n",
    "parameter: util_script = path('/project/mstephens/gtex/scripts/SumstatQuery.R')\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "fail_if(not util_script.is_file() and len(str(util_script)), msg = 'Please specify valid path for --util-script')\n",
    "genes = sort_uniq([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "depends: R_library(\"Matrix\")\n",
    "input: data, for_each = 'genes'\n",
    "output: f'{vhat_data:nn}/{vhat_data:bnn}_V_simple_{_genes}.rds'\n",
    "\n",
    "task: trunk_workers = 1, walltime = '1m', trunk_size = 500, mem = '3G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    source(${util_script:r})\n",
    "    simple_V = function(gene, database, z_thresh = 2){\n",
    "      print(gene)\n",
    "      dat <- GetSS(gene, database)\n",
    "      z = dat$\"z-score\"\n",
    "      max_absz = apply(abs(z), 1, max)\n",
    "      nullish = which(max_absz < z_thresh)\n",
    "      # if (length(nullish) < ncol(z)) {\n",
    "        # stop(\"not enough null data to estimate null correlation\")\n",
    "      # }\n",
    "      if (length(nullish) <= 1){\n",
    "        mat = diag(ncol(z))\n",
    "      } else {\n",
    "        nullish_z = z[nullish, ]\n",
    "        mat = as.matrix(Matrix::nearPD(as.matrix(cov(nullish_z)), conv.tol=1e-06, doSym = TRUE, corr=TRUE)$mat)\n",
    "      }\n",
    "      return(mat)\n",
    "    }\n",
    "    V = simple_V(\"${_genes}\", ${data:r})\n",
    "    saveRDS(V, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Consolidate Vhat into one file\n",
    "[vhat_corshrink_xcondition_2, vhat_simple_specific_2]\n",
    "depends: R_library(\"parallel\")\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "genes = paths([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "\n",
    "input: group_by = 'all'\n",
    "output: f\"{vhat_data:nn}.V_{step_name.rsplit('_',1)[0]}.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    library(parallel)\n",
    "    files = sapply(c(${genes:r,}), function(g) paste0(c(${_input[0]:adr}), '/', g, '.rds'), USE.NAMES=FALSE)\n",
    "    V = mclapply(files, function(i){ readRDS(i) }, mc.cores = 1)\n",
    "    R = dim(V[[1]])[1]\n",
    "    L = length(V)\n",
    "    V.array = array(as.numeric(unlist(V)), dim=c(R, R, L))\n",
    "    saveRDS(V.array, ${_output:ar})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Compute MASH priors \n",
    "\n",
    "Main reference are our `mashr` vignettes [this for mashr eQTL outline](https://stephenslab.github.io/mashr/articles/eQTL_outline.html) and [this for using FLASH prior](https://github.com/stephenslab/mashr/blob/master/vignettes/flash_mash.Rmd). \n",
    "\n",
    "The outcome of this workflow should be found under `./mashr_flashr_workflow_output` folder (can be configured). File names have pattern `*.mash_model_*.rds`. They can be used to computer posterior for input list of gene-SNP pairs (see next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Latest update: May/2023\n",
    "[ud]\n",
    "# Options are \"ted\", \"ed\", \n",
    "parameter: unconstrained_prior = \"ted\"\n",
    "input:[data, vhat_data if vhat != \"mle\" else f'{vhat_data:nn}.V_simple.rds'] \n",
    "output: prior_data\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '10G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    library(stringr)\n",
    "    library(udr)\n",
    "    library(mashr)\n",
    "\n",
    "    rds_files = c(${_input:r,})\n",
    "    # mash data \n",
    "    dat = readRDS(rds_files[1])\n",
    "    vhat = readRDS(rds_files[2])\n",
    "\n",
    "    # Fit mixture model using udr package\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, V=vhat, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    # Canonical matrices\n",
    "    U.can = cov_canonical(mash_data)    \n",
    "\n",
    "    set.seed(999)\n",
    "    # Penalty strength\n",
    "    lambda = ncol(dat$strong.z)\n",
    "    # Initialize udr\n",
    "    fit0 <- ud_init(mash_data, n_unconstrained = 50)\n",
    "    # Fit udr and use penalty as default as suggested by Yunqi\n",
    "    # penalty is necessary in small sample size case, and there won't be a difference in large sample size \n",
    "    fit2 = ud_fit(fit0, control = list(unconstrained.update = \"${unconstrained_prior}\", scaled.update = \"fa\", resid.update = 'none', \n",
    "                                   lambda =lambda, penalty.type = \"iw\", maxiter=2e3, tol = 1e-2, tol.lik = 1e-2), verbose=TRUE)\n",
    "    # extract data-driven covariance from udr model. (A list of covariance matrices)\n",
    "    U.ud <- lapply(fit2$U,function (e) \"[[\"(e,\"mat\")) \n",
    "\n",
    "    saveRDS(list(U=c(U.ud, U.can), w=fit2$w, loglik=fit2$loglik), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ted]\n",
    "# Method is `ed` or `ted`\n",
    "parameter: ud_method = \"ed\"\n",
    "# A typical choice is to estimate scales only for canonical components\n",
    "parameter: scale_only = []\n",
    "# Tolerance for change in likelihood\n",
    "parameter: ud_tol_lik = 1e-3\n",
    "input: [data, vhat_data if vhat != \"mle\" else f'{vhat_data:nn}.V_simple.rds'] + [f\"{cwd}/{output_prefix}.{m}.rds\" for m in mixture_components]\n",
    "output: prior_data\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '10G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    library(stringr)\n",
    "    rds_files = c(${_input:r,})\n",
    "    dat = readRDS(rds_files[1])\n",
    "    vhat = readRDS(rds_files[2])\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, V=vhat, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    # U is different from mashr pipeline, here I kept the later one\n",
    "    # U = list(XtX = dat$XtX)\n",
    "    U = list(XtX = t(mash_data$Bhat) %*% mash_data$Bhat / nrow(mash_data$Bhat))\n",
    "\n",
    "    U_scaled = list()\n",
    "    mixture_components =  c(${paths(mixture_components):r,})\n",
    "    scale_only =  c(${paths(scale_only):r,})\n",
    "    scale_idx = which(mixture_components %in% scale_only )\n",
    "    for (f in 3:length(rds_files) ) {\n",
    "        if ((f - 1) %in% scale_idx ) {\n",
    "          U_scaled = c(U_scaled, readRDS(rds_files[f]))\n",
    "        } else {\n",
    "          U = c(U, readRDS(rds_files[f]))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Fit mixture model using udr package\n",
    "    library(udr)\n",
    "    message(paste(\"Running ${ud_method.upper()} via udr package for\", length(U), \"mixture components\"))\n",
    "    f0 = ud_init(X = as.matrix(dat$strong.z), V = V, U_scaled = U_scaled, U_unconstrained = U, n_rank1=0)\n",
    "    res = ud_fit(f0, X = na.omit(f0$X), control = list(unconstrained.update = \"ed\", resid.update = 'none', scaled.update = \"fa\", maxiter=5000, tol.lik = ${ud_tol_lik}), verbose=TRUE)\n",
    "    res_ted =  ud_fit(f0, X = na.omit(f0$X), control = list(unconstrained.update = \"ted \", resid.update = 'none', scaled.update = \"fa\", maxiter=5000, tol.lik = ${ud_tol_lik}), verbose=TRUE)\n",
    "\n",
    "    saveRDS(list(U=res$U, w=res$w, loglik=res$loglik), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ed_bovy]\n",
    "parameter: ed_tol = 1e-6\n",
    "input: [data, vhat_data if vhat != \"mle\" else f'{vhat_data:nn}.V_simple.rds'] + [f\"{cwd}/{output_prefix}.{m}.rds\" for m in mixture_components]\n",
    "output: prior_data\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '40G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container, entrypoint=entrypoint\n",
    "    library(mashr)\n",
    "    rds_files = c(${_input:r,})\n",
    "    dat = readRDS(rds_files[1])\n",
    "    vhat = readRDS(rds_files[2])\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, V=vhat, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    # U is different from mashr pipeline, here I kept the later one\n",
    "    # U = list(XtX = dat$XtX)\n",
    "    U = list(XtX = t(mash_data$Bhat) %*% mash_data$Bhat / nrow(mash_data$Bhat))\n",
    "    \n",
    "    for (f in rds_files[3:length(rds_files)]) U = c(U, readRDS(f))\n",
    "    # Fit mixture model using ED code by J. Bovy\n",
    "    message(paste(\"Running ED via J. Bovy's code for\", length(U), \"mixture components\"))\n",
    "    res = mashr:::bovy_wrapper(mash_data, U, logfile=${_output:nr}, tol = ${ed_tol})\n",
    "    saveRDS(list(U=res$Ulist, w=res$pi, loglik=scan(\"${_output:n}_loglike.log\")), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Plot patterns of sharing\n",
    "\n",
    "This is a simple utility function that takes the output from the pipeline above and make some heatmap to show major patterns of multivariate effects. The plots will be ordered by their mixture weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[plot_U]\n",
    "parameter: model_data = path\n",
    "# number of components to show\n",
    "parameter: max_comp = -1\n",
    "# whether or not to convert to correlation\n",
    "parameter: to_cor = False\n",
    "parameter: tol = \"1E-6\"\n",
    "parameter: remove_label = False\n",
    "parameter: name = \"\"\n",
    "input: model_data\n",
    "output: f'{cwd:a}/{_input:bn}{(\"_\" + name.replace(\"$\", \"_\")) if name != \"\" else \"\"}.pdf'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    library(reshape2)\n",
    "    library(ggplot2)\n",
    "    plot_sharing = function(X, col = 'black', to_cor=FALSE, title=\"\", remove_names=F) {\n",
    "        clrs <- colorRampPalette(rev(c(\"#D73027\",\"#FC8D59\",\"#FEE090\",\"#FFFFBF\",\n",
    "            \"#E0F3F8\",\"#91BFDB\",\"#4575B4\")))(128)\n",
    "        if (to_cor) lat <- cov2cor(X)\n",
    "        else lat = X/max(diag(X))\n",
    "        lat[lower.tri(lat)] <- NA\n",
    "        n <- nrow(lat)\n",
    "        if (remove_names) {\n",
    "            colnames(lat) = paste('t',1:n, sep = '')\n",
    "            rownames(lat) = paste('t',1:n, sep = '')\n",
    "        }\n",
    "        melted_cormat <- melt(lat[n:1,], na.rm = TRUE)\n",
    "        p = ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+\n",
    "            geom_tile(color = \"white\")+ggtitle(title) + \n",
    "            scale_fill_gradientn(colors = clrs, limit = c(-1,1), space = \"Lab\") +\n",
    "            theme_minimal()+ \n",
    "            coord_fixed() +\n",
    "            theme(axis.title.x = element_blank(),\n",
    "                  axis.title.y = element_blank(),\n",
    "                  axis.text.x = element_text(color=col, size=8,angle=45,hjust=1),\n",
    "                  axis.text.y = element_text(color=rev(col), size=8),\n",
    "                  title =element_text(size=10),\n",
    "                  # panel.grid.major = element_blank(),\n",
    "                  panel.border = element_blank(),\n",
    "                  panel.background = element_blank(),\n",
    "                  axis.ticks = element_blank(),\n",
    "                  legend.justification = c(1, 0),\n",
    "                  legend.position = c(0.6, 0),\n",
    "                  legend.direction = \"horizontal\")+\n",
    "            guides(fill = guide_colorbar(title=\"\", barwidth = 7, barheight = 1,\n",
    "                   title.position = \"top\", title.hjust = 0.5))\n",
    "        if(remove_names){\n",
    "            p = p + scale_x_discrete(labels= 1:n) + scale_y_discrete(labels= n:1)\n",
    "        }\n",
    "        return(p)\n",
    "    }\n",
    "  \n",
    "    dat = readRDS(${_input:r})\n",
    "    name = \"${name}\"\n",
    "    if (name != \"\") {\n",
    "      if (is.null(dat[[name]])) stop(\"Cannot find data ${name} in ${_input}\")\n",
    "        dat = dat[[name]]\n",
    "    }\n",
    "    if (is.null(names(dat$U))) names(dat$U) = paste0(\"Comp_\", 1:length(dat$U))\n",
    "    meta = data.frame(names(dat$U), dat$w, stringsAsFactors=F)\n",
    "    colnames(meta) = c(\"U\", \"w\")\n",
    "    tol = ${tol}\n",
    "    n_comp = length(meta$U[which(meta$w>tol)])\n",
    "    meta = head(meta[order(meta[,2], decreasing = T),], ${max_comp if max_comp > 1 else \"nrow(meta)\"})\n",
    "    message(paste(n_comp, \"components out of\", length(dat$w), \"total components have weight greater than\", tol))\n",
    "    res = list()\n",
    "    for (i in 1:n_comp) {\n",
    "        title = paste(meta$U[i], \"w =\", round(meta$w[i], 6))\n",
    "        ##Handle updated udr data structure\n",
    "        if(is.list(dat$U[[meta$U[i]]])){\n",
    "          res[[i]] = plot_sharing(dat$U[[meta$U[i]]]$mat, to_cor = ${\"T\" if to_cor else \"F\"}, title=title, remove_names = ${\"TRUE\" if remove_label else \"FALSE\"})\n",
    "        } else if(is.matrix(dat$U[[meta$U[i]]])){\n",
    "          res[[i]] = plot_sharing(dat$U[[meta$U[i]]], to_cor = ${\"T\" if to_cor else \"F\"}, title=title, remove_names = ${\"TRUE\" if remove_label else \"FALSE\"})\n",
    "        }\n",
    "    }\n",
    "    unit = 4\n",
    "    n_col = 5\n",
    "    n_row = ceiling(n_comp / n_col)\n",
    "    pdf(${_output:r}, width = unit * n_col, height = unit * n_row)\n",
    "    do.call(gridExtra::grid.arrange, c(res, list(ncol = n_col, nrow = n_row, bottom = \"Data source: readRDS(${_input:br})${('$'+name) if name else ''}\")))\n",
    "    dev.off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "R"
    ],
    [
     "SoS"
    ]
   ],
   "version": "0.24.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
