{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "# EMS Prediction\n",
    "\n",
    "This notebook demonstrates how to use trained sc-EMS (single-cell Expression Modifier Scores) models to predict functional genetic variants. Once you have completed the **EMS Training** tutorial, you can use the trained models as a \"black box\" to score your own variant lists.\n",
    "\n",
    "## What You'll Learn\n",
    "- Load and inspect trained CatBoost models\n",
    "- Prepare variant lists for prediction\n",
    "- Generate EMS scores for your variants\n",
    "- Interpret prediction outputs\n",
    "\n",
    "## Prerequisites\n",
    "- Completed EMS training (see [EMS Training Tutorial](https://statfungen.github.io/xqtl-protocol/code/xqtl_modifier_score/ems_training.html))\n",
    "- Trained model files in `.joblib` format\n",
    "- Your variant list in the required format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4-f5a6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Step 1: Load Required Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4-a5b6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3g4-h5i6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Step 2: Load and Inspect Trained Model\n",
    "\n",
    "### Model Overview\n",
    "The trained model is a **CatBoost classifier** that predicts whether genetic variants are functional (affect gene expression) or non-functional. The model was trained using:\n",
    "- **Algorithm**: Feature-weighted CatBoost with gradient boosting\n",
    "- **Training data**: 3,056 variants from chromosome 2\n",
    "- **Features**: 4,839 genomic annotations per variant\n",
    "- **Performance**: 89.78% AUC, 50.5% Average Precision\n",
    "\n",
    "For detailed training methodology, refer to the [EMS Training Tutorial](https://statfungen.github.io/xqtl-protocol/code/xqtl_modifier_score/ems_training.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2g3h4-i5j6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these paths for your setup\n",
    "MODEL_PATH = \"../../data/Mic_mega_eQTL/model_results/model_standard_subset_weighted_chr_chr2_NPR_10.joblib\"\n",
    "CONFIG_PATH = \"data_config.yaml\"\n",
    "FEATURE_WEIGHTS_PATH = \"../../data/Mic_mega_eQTL/model_results/feature_weights_chr_2_NPR_10.pkl\"\n",
    "SUBSET_COLUMNS_PATH = \"../../data/Mic_mega_eQTL/model_results/subset_columns_chr_2_NPR_10.pkl\"\n",
    "\n",
    "# Load trained model\n",
    "print(\"Loading trained CatBoost model...\")\n",
    "try:\n",
    "    trained_model = joblib.load(MODEL_PATH)\n",
    "    print(f\"‚úÖ Model loaded successfully: {trained_model.__class__.__name__}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Model file not found at {MODEL_PATH}\")\n",
    "    print(\"Please ensure you have completed the EMS training tutorial first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1g2h3i4-j5k6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect model properties (black box overview)\n",
    "print(\"üìä TRAINED MODEL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model Type: {type(trained_model).__name__}\")\n",
    "print(f\"Number of features: {trained_model.feature_count_}\")\n",
    "print(f\"Number of classes: {trained_model.classes_}\")\n",
    "print(f\"Tree count: {trained_model.tree_count_}\")\n",
    "\n",
    "# Show top 10 most important features\n",
    "feature_names = trained_model.feature_names_\n",
    "feature_importances = trained_model.feature_importances_\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüéØ TOP 10 PREDICTIVE FEATURES:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (feature, importance) in enumerate(importance_df.head(10).values, 1):\n",
    "    print(f\"{i:2d}. {feature:<30} {importance:>8.3f}\")\n",
    "\n",
    "print(f\"\\nüìà MODEL PERFORMANCE (from training):\")\n",
    "print(\"   - AUC Score: 89.78%\")\n",
    "print(\"   - Average Precision: 50.5%\")\n",
    "print(\"   - Training variants: 3,056\")\n",
    "print(\"   - Test variants: 761\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1h2i3j4-k5l6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load supporting files for prediction pipeline\n",
    "print(\"Loading prediction pipeline components...\")\n",
    "\n",
    "# Load configuration\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "# Load feature weights and subset columns\n",
    "try:\n",
    "    with open(FEATURE_WEIGHTS_PATH, 'rb') as f:\n",
    "        feature_weights = pickle.load(f)\n",
    "    \n",
    "    with open(SUBSET_COLUMNS_PATH, 'rb') as f:\n",
    "        column_info = pickle.load(f)\n",
    "        subset_columns = column_info['subset_columns']\n",
    "        abs_columns = column_info['abs_columns']\n",
    "    \n",
    "    print(f\"‚úÖ Pipeline components loaded:\")\n",
    "    print(f\"   - Feature weights: {len(feature_weights)} features\")\n",
    "    print(f\"   - Subset columns: {len(subset_columns)} columns\")\n",
    "    print(f\"   - Absolute value columns: {len(abs_columns)} columns\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Pipeline component missing: {e}\")\n",
    "    print(\"Some files may not be available - continuing with basic prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1i2j3k4-l5m6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Your Variant List\n",
    "\n",
    "### Input Requirements\n",
    "Your variant list should be a **TSV/CSV file** with the following required columns:\n",
    "- `variant_id`: Format \"chr:pos:ref:alt\" (e.g., \"2:12345:A:T\")\n",
    "- Additional genomic annotation columns (will be handled automatically)\n",
    "\n",
    "### Example Input Formats\n",
    "We'll demonstrate with toy examples that you can adapt for your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1j2k3l4-m5n6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create toy variant list (Katie's example format)\n",
    "print(\"üìã EXAMPLE 1: Creating toy variant list\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "toy_variants = {\n",
    "    'variant_id': [\n",
    "        '2:12345:A:T',\n",
    "        '2:67890:G:C', \n",
    "        '2:11111:T:A',\n",
    "        '2:22222:C:G',\n",
    "        '2:33333:A:G'\n",
    "    ]\n",
    "}\n",
    "\n",
    "toy_df = pd.DataFrame(toy_variants)\n",
    "print(\"Toy variant list created:\")\n",
    "print(toy_df)\n",
    "print(f\"\\nNumber of variants to score: {len(toy_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1k2l3m4-n5o6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Load your own variant file (uncomment and modify as needed)\n",
    "print(\"üìã EXAMPLE 2: Loading your own variant file\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Uncomment and modify these lines to use your own data:\n",
    "# YOUR_VARIANT_FILE = \"path/to/your/variants.tsv\"\n",
    "# user_variants_df = pd.read_csv(YOUR_VARIANT_FILE, sep='\\t')\n",
    "# print(f\"Loaded {len(user_variants_df)} variants from your file\")\n",
    "# print(user_variants_df.head())\n",
    "\n",
    "# For this tutorial, we'll use the toy example\n",
    "user_variants_df = toy_df.copy()\n",
    "print(\"Using toy variants for demonstration\")\n",
    "print(\"\\nüí° TIP: Replace 'toy_df' with your loaded data in the code above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4-o5p6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering Pipeline\n",
    "\n",
    "The model requires specific genomic features to make predictions. This pipeline will:\n",
    "1. Parse variant information from variant_id\n",
    "2. Create variant-specific features (SNP type, indel classification)\n",
    "3. Add genomic annotations (population genetics, gene constraint)\n",
    "4. Apply feature transformations used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1m2n3o4-p5q6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_variants_for_prediction(df, data_config):\n",
    "    \"\"\"\n",
    "    Prepare variant dataframe for EMS prediction by adding required features.\n",
    "    \n",
    "    This function replicates the feature engineering from the training pipeline.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"üîß FEATURE ENGINEERING PIPELINE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Step 1: Parse variant_id to create variant features\n",
    "    print(\"1. Parsing variant information...\")\n",
    "    df[['chr','pos','ref','alt']] = df['variant_id'].str.split(':', expand=True)\n",
    "    \n",
    "    # Calculate variant features\n",
    "    df['length_diff'] = df['ref'].str.len() - df['alt'].str.len()\n",
    "    df['is_SNP'] = df['length_diff'].apply(lambda x: 1 if x == 0 else 0)\n",
    "    df['is_indel'] = df['length_diff'].apply(lambda x: 1 if x != 0 else 0)\n",
    "    df['is_insertion'] = df['length_diff'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['is_deletion'] = df['length_diff'].apply(lambda x: 1 if x < 0 else 0)\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    df.drop(columns=['chr','pos','ref','alt'], inplace=True)\n",
    "    \n",
    "    print(f\"   ‚úÖ Created variant features: length_diff, is_SNP, is_indel, is_insertion, is_deletion\")\n",
    "    \n",
    "    # Step 2: Add gene constraint data (if available)\n",
    "    print(\"2. Adding gene constraint features...\")\n",
    "    try:\n",
    "        # Load gene constraint data\n",
    "        constraint_config = data_config[\"input_files\"][\"gene_constraint\"]\n",
    "        gene_lof_df = pd.read_excel(constraint_config[\"file_path\"], constraint_config[\"xlsx_sheet\"])\n",
    "        \n",
    "        # Apply column mapping\n",
    "        constraint_mapping = constraint_config[\"column_mapping\"]\n",
    "        source_gene_col = constraint_mapping[\"source_gene_id\"]\n",
    "        target_gene_col = constraint_mapping[\"target_gene_id\"]\n",
    "        source_value_col = constraint_mapping[\"source_value\"]\n",
    "        target_value_col = constraint_mapping[\"target_value\"]\n",
    "        \n",
    "        gene_lof_df = gene_lof_df[[source_gene_col, source_value_col]]\n",
    "        gene_lof_df = gene_lof_df.rename(columns={source_gene_col: target_gene_col, source_value_col: target_value_col})\n",
    "        gene_lof_df[target_value_col] = np.log2(gene_lof_df[target_value_col])\n",
    "        \n",
    "        # Note: In real application, you'd need gene mapping for your variants\n",
    "        # For demonstration, we'll add placeholder values\n",
    "        df[target_value_col] = -10.0  # Placeholder median value from training\n",
    "        print(f\"   ‚úÖ Added gene constraint feature: {target_value_col}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Gene constraint data not available: {e}\")\n",
    "        df['gene_lof'] = -10.0  # Use training median\n",
    "    \n",
    "    # Step 3: Add population genetics data\n",
    "    print(\"3. Adding population genetics features...\")\n",
    "    try:\n",
    "        # Note: In real application, you'd load MAF data for your chromosome\n",
    "        # For demonstration, we'll add placeholder values\n",
    "        df['gnomad_MAF'] = 0.1  # Placeholder median value from training\n",
    "        print(f\"   ‚úÖ Added population genetics feature: gnomad_MAF\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Population genetics data not available: {e}\")\n",
    "        df['gnomad_MAF'] = 0.1  # Use training median\n",
    "    \n",
    "    print(f\"\\nüìä Feature engineering complete. DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to user variants\n",
    "processed_variants = prepare_variants_for_prediction(user_variants_df, data_config)\n",
    "print(\"\\nProcessed variants preview:\")\n",
    "print(processed_variants.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1n2o3p4-q5r6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Step 5: Generate EMS Predictions\n",
    "\n",
    "Now we'll use the trained model to generate Expression Modifier Scores for your variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1o2p3q4-r5s6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ems_predictions(df, model, subset_columns=None):\n",
    "    \"\"\"\n",
    "    Generate EMS predictions for variants using trained model.\n",
    "    \n",
    "    Returns dataframe with original variant info plus prediction scores.\n",
    "    \"\"\"\n",
    "    print(\"üéØ GENERATING EMS PREDICTIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create features for prediction\n",
    "    prediction_df = df.copy()\n",
    "    \n",
    "    # For demonstration, we need to create mock features that match training\n",
    "    # In real application, you'd have all genomic annotations\n",
    "    required_features = model.feature_names_\n",
    "    \n",
    "    print(f\"Model expects {len(required_features)} features\")\n",
    "    print(f\"Available in input: {list(prediction_df.columns)}\")\n",
    "    \n",
    "    # Add missing features with placeholder values (based on training statistics)\n",
    "    for feature in required_features:\n",
    "        if feature not in prediction_df.columns:\n",
    "            # Use training-based defaults for missing features\n",
    "            if 'distance' in feature.lower():\n",
    "                prediction_df[feature] = 8.0  # Log distance placeholder\n",
    "            elif 'abc' in feature.lower():\n",
    "                prediction_df[feature] = 0.1  # ABC score placeholder\n",
    "            elif 'diff' in feature.lower():\n",
    "                prediction_df[feature] = 0.0  # Differential signal placeholder\n",
    "            else:\n",
    "                prediction_df[feature] = 0.0  # Generic placeholder\n",
    "    \n",
    "    # Select only model features in correct order\n",
    "    X_pred = prediction_df[required_features]\n",
    "    \n",
    "    # Handle missing values\n",
    "    X_pred = X_pred.replace([np.inf, -np.inf], 0)\n",
    "    X_pred = X_pred.fillna(0)\n",
    "    \n",
    "    print(f\"Prepared feature matrix: {X_pred.shape}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    pred_probs = model.predict_proba(X_pred)[:, 1]  # Probability of functional variant\n",
    "    pred_labels = model.predict(X_pred)  # Binary predictions\n",
    "    \n",
    "    # Add predictions to original dataframe\n",
    "    result_df = df.copy()\n",
    "    result_df['ems_score'] = pred_probs\n",
    "    result_df['predicted_functional'] = pred_labels\n",
    "    result_df['confidence_level'] = pd.cut(pred_probs, \n",
    "                                         bins=[0, 0.3, 0.7, 1.0], \n",
    "                                         labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    print(f\"‚úÖ Predictions generated for {len(result_df)} variants\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Generate predictions\n",
    "predictions_df = generate_ems_predictions(processed_variants, trained_model, subset_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1p2q3r4-s5t6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Step 6: Interpret Results\n",
    "\n",
    "### EMS Score Interpretation\n",
    "- **EMS Score Range**: 0.0 - 1.0 (higher = more likely to be functional)\n",
    "- **Threshold Guidelines**:\n",
    "  - **High confidence (>0.7)**: Very likely functional variant\n",
    "  - **Medium confidence (0.3-0.7)**: Uncertain, requires further investigation\n",
    "  - **Low confidence (<0.3)**: Likely non-functional variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1q2r3s4-t5u6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display prediction results\n",
    "print(\"üèÜ EMS PREDICTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show results table\n",
    "results_display = predictions_df[['variant_id', 'ems_score', 'predicted_functional', 'confidence_level']].copy()\n",
    "results_display['ems_score'] = results_display['ems_score'].round(4)\n",
    "\n",
    "print(\"Variant Predictions:\")\n",
    "print(results_display.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà PREDICTION SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Total variants scored: {len(predictions_df)}\")\n",
    "print(f\"Predicted functional: {sum(predictions_df['predicted_functional'])}\")\n",
    "print(f\"Predicted non-functional: {sum(1 - predictions_df['predicted_functional'])}\")\n",
    "print(f\"Average EMS score: {predictions_df['ems_score'].mean():.4f}\")\n",
    "print(f\"Highest scoring variant: {predictions_df.loc[predictions_df['ems_score'].idxmax(), 'variant_id']}\")\n",
    "\n",
    "print(\"\\nConfidence level distribution:\")\n",
    "print(predictions_df['confidence_level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q1r2s3t4-u5v6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed interpretation\n",
    "print(\"üîç DETAILED VARIANT INTERPRETATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for idx, row in predictions_df.iterrows():\n",
    "    variant = row['variant_id']\n",
    "    score = row['ems_score']\n",
    "    functional = row['predicted_functional']\n",
    "    confidence = row['confidence_level']\n",
    "    \n",
    "    print(f\"\\nüß¨ {variant}\")\n",
    "    print(f\"   EMS Score: {score:.4f}\")\n",
    "    print(f\"   Prediction: {'Functional' if functional else 'Non-functional'}\")\n",
    "    print(f\"   Confidence: {confidence}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if score > 0.7:\n",
    "        interpretation = \"Strong evidence for regulatory function. Consider for experimental validation.\"\n",
    "    elif score > 0.3:\n",
    "        interpretation = \"Moderate evidence for function. May warrant further investigation.\"\n",
    "    else:\n",
    "        interpretation = \"Limited evidence for regulatory function. Likely neutral variant.\"\n",
    "    \n",
    "    print(f\"   Interpretation: {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1s2t3u4-v5w6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "## Step 7: Export Results\n",
    "\n",
    "Save your predictions for downstream analysis or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1t2u3v4-w5x6-7890-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "output_file = \"ems_predictions_output.tsv\"\n",
    "predictions_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "print(f\"\\nOutput contains {len(predictions_df)} variants with columns:\")\n",
    "for col in predictions_df.columns:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_stats = {\n",
    "    'total_variants': len(predictions_df),\n",
    "    'functional_predicted': sum(predictions_df['predicted_functional']),\n",
    "    'non_functional_predicted': sum(1 - predictions_df['predicted_functional']),\n",
    "    'mean_ems_score': float(predictions_df['ems_score'].mean()),\n",
    "    'max_ems_score': float(predictions_df['ems_score'].max()),\n",
    "    'min_ems_score': float(predictions_df['ems_score'].min()),\n",
    "    'high_confidence_variants': len(predictions_df[predictions_df['ems_score'] > 0.7]),\n",
    "    'low_confidence_variants': len(predictions_df[predictions_df['ems_score'] < 0.3])\n",
    "}\n",
    "\n",
    "summary_file = \"ems_prediction_summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    import json\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"üìä Summary statistics saved to: {summary_file}\")"
   ]
  }
