{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprised-creek",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Reference data standardization\n",
    "\n",
    "This module provides reference data download, indexing and preprocessing (if necessary), in preparation for use throughout the pipeline.\n",
    "\n",
    "We have included the PDF document compiled by data standardization subgroup in the [minimal working example folder on Google Drive](https://drive.google.com/file/d/1R5sw5o8vqk_mbQQb4CGmtH3ldu1T3Vu0/view?usp=sharing). It contains the reference data to use for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-aerospace",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This module is based on the [TOPMed workflow from Broad](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md).\n",
    "\n",
    "Workflows implemented include:\n",
    "\n",
    "### Convert transcript feature file gff3 to gtf\n",
    "\n",
    "- Input: an uncompressed gff3 file.(i.e. can be view via cat)\n",
    "- Output: a gtf file.\n",
    "\n",
    "### Collapse transcript features into genes\n",
    "\n",
    "- Input: a gtf file.\n",
    "- Output: a gtf file with collapesed gene model.\n",
    "\n",
    "### Generate STAR index based on gtf and reference fasta\n",
    "\n",
    "- Input: a gtf file and an acompanying fasta file.\n",
    "- Output: A folder of STAR index.\n",
    "\n",
    "\n",
    "### Generate RSEM index based on gtf and reference fasta\n",
    "\n",
    "- Input: a gtf file and an acompanying fasta file.\n",
    "- Output: A folder of RSEM index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-salad",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example commands\n",
    "\n",
    "To download reference data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-namibia",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb download_hg_reference --cwd reference_data\n",
    "sos run reference_data.ipynb download_gene_annotation --cwd reference_data\n",
    "sos run reference_data.ipynb download_ercc_reference --cwd reference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-ability",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To format reference data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-patrol",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb hg_reference \\\n",
    "    --cwd reference_data \\\n",
    "    --ercc-reference reference_data/ERCC92.fa \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.fa \\\n",
    "    --container container/rna_quantification.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-container",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To format gene feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-specification",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb gene_annotation \\\n",
    "    --cwd reference_data \\\n",
    "    --ercc-gtf reference_data/ERCC92.gtf \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.gtf \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --container container/rna_quantification.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-hearing",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Notice that for stranded RNA-seq protocol please add `--is-stranded` to the command above. More details can be found later in the document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-pantyhose",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate STAR index using the GTF annotation file before gene model collapse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-maker",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb STAR_index \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container container/rna_quantification.sif \\\n",
    "    --mem 40G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-acrobat",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Notice that command above requires at least 40G of memory, and takes quite a while to complete**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-intro",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate RSEM index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-communication",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb RSEM_index \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container container/rna_quantification.sif \\\n",
    "    --mem 40G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-lincoln",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tight-tanzania",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run reference_data.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  download_hg_reference\n",
      "  download_gene_annotation\n",
      "  download_ercc_reference\n",
      "  gff3_to_gtf\n",
      "  hg_reference\n",
      "  hg_gtf\n",
      "  ercc_gtf\n",
      "  gene_annotation\n",
      "  STAR_index\n",
      "  RSEM_indexing\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd VAL (as path, required)\n",
      "                        The output directory for generated files.\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  download_hg_reference:\n",
      "  download_gene_annotation:\n",
      "  download_ercc_reference:\n",
      "  gff3_to_gtf:\n",
      "    Workflow Options:\n",
      "      --gff3-file VAL (as path, required)\n",
      "  hg_reference_1:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "                        Path to HG reference file\n",
      "  hg_reference_2:\n",
      "    Workflow Options:\n",
      "      --ercc-reference VAL (as path, required)\n",
      "  hg_reference_3:\n",
      "  hg_gtf_1:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "      --hg-gtf VAL (as path, required)\n",
      "  hg_gtf_2:\n",
      "    Workflow Options:\n",
      "      --[no-]collapse-only (default to False)\n",
      "                        Use this for stranded protocol (optional)\n",
      "  ercc_gtf:\n",
      "    Workflow Options:\n",
      "      --ercc-gtf VAL (as path, required)\n",
      "  gene_annotation:\n",
      "  STAR_index:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "      --hg-reference VAL (as path, required)\n",
      "      --sjdbOverhang 100 (as int)\n",
      "                        Specifies the length of the genomic sequence around the\n",
      "                        annotated junction to be used in constructing the splice\n",
      "                        junctions database. Ideally, this length should be equal\n",
      "                        to the ReadLength-1, where ReadLength is the length of\n",
      "                        the reads. Default choice follows from TOPMed pipeline\n",
      "                        recommendation.\n",
      "  RSEM_indexing:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "      --hg-reference VAL (as path, required)\n"
     ]
    }
   ],
   "source": [
    "sos run reference_data.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-necklace",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files.\n",
    "parameter: cwd = path\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "cwd = path(f'{cwd:a}')\n",
    "from sos.utils import expand_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-guidance",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-batch",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_hg_reference]\n",
    "output: f\"{cwd:a}/GRCh38_full_analysis_set_plus_decoy_hla.fa\"\n",
    "download: dest_dir = cwd\n",
    "    ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-accessory",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_gene_annotation]\n",
    "output: f\"{cwd:a}/Homo_sapiens.GRCh38.103.chr.gtf\"\n",
    "download: dest_dir = cwd, decompress=True\n",
    "    http://ftp.ensembl.org/pub/release-103/gtf/homo_sapiens/Homo_sapiens.GRCh38.103.chr.gtf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-england",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_ercc_reference]\n",
    "output: f\"{cwd:a}/ERCC92.gtf\", f\"{cwd:a}/ERCC92.fa\"\n",
    "download: dest_dir = cwd, decompress=True\n",
    "    https://tools.thermofisher.com/content/sfs/manuals/ERCC92.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-clone",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## GFF3 to GTF formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-transportation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[gff3_to_gtf]\n",
    "parameter: gff3_file = path\n",
    "input: gff3_file\n",
    "output: f'{cwd}/{_input:bn}.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "        gffread ${_input} -T -o ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-sample",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## HG reference file preprocessing\n",
    "1. Remove the HLA/ALT/Decoy record from the fasta\n",
    "2. Adding in ERCC information to the fasta file\n",
    "3. Generating index for the fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-cooperation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_reference_1 (HLA ALT Decoy removal)]\n",
    "# Path to HG reference file\n",
    "parameter: hg_reference = path\n",
    "input: hg_reference\n",
    "output:  f'{cwd}/{_input:bn}.noALT_noHLA_noDecoy.fasta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    with open('${_input}', 'r') as fasta:\n",
    "        contigs = fasta.read()\n",
    "        contigs = contigs.split('>')\n",
    "        contig_ids = [i.split(' ', 1)[0] for i in contigs]\n",
    "\n",
    "        # exclude ALT, HLA and decoy contigs\n",
    "        filtered_fasta = '>'.join([c for i,c in zip(contig_ids, contigs)\n",
    "        if not (i[-4:]=='_alt' or i[:3]=='HLA' or i[-6:]=='_decoy')])\n",
    "    \n",
    "    with open('${_output}', 'w') as fasta:\n",
    "        fasta.write(filtered_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-sydney",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_reference_2 (merge with ERCC reference)]\n",
    "parameter: ercc_reference = path\n",
    "output: f'{cwd}/{_input:bn}_ERCC.fasta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    sed 's/ERCC-/ERCC_/g' ${ercc_reference} >  ${ercc_reference:n}.patched.fa\n",
    "    cat ${_input} ${ercc_reference:n}.patched.fa > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-arctic",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[hg_reference_3 (index the fasta file)]\n",
    "output: f'{cwd}/{_input:bn}.dict'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    samtools faidx ${_input}\n",
    "    java -jar /opt/picard-tools/picard.jar \\\n",
    "    CreateSequenceDictionary \\\n",
    "    R=${_input} \\\n",
    "    O=${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-generator",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Transcript and gene model reference processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-burning",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This step modify the `gtf` file for following reasons:\n",
    "\n",
    "1. RSEM require GTF input to have the same chromosome name format (with `chr` prefix) as the fasta file. **although for STAR, this problem can be solved by the now commented --sjdbGTFchrPrefix \"chr\" option, we have to add `chr` to it for use with RSEM**. \n",
    "2. Gene model collapsing script `collapse_annotation.py` from GTEx require the gtf have `transcript_type` instead `transcript_biotype` in its annotation. We rename it here, although **this problem can also be solved by modifying the collapse_annotation.py while building the docker, since we are doing 1 above we think it is better to add in another customization here.**\n",
    "3. Adding in ERCC information to the `gtf` reference.\n",
    "\n",
    "We may reimplement 1 and 2 if the problem with RSEM is solved, or when RSEM is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-danger",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_gtf_1 (add chr prefix to gtf file)]\n",
    "parameter: hg_reference = path\n",
    "parameter: hg_gtf = path\n",
    "input: hg_reference, hg_gtf\n",
    "output: f'{cwd}/{_input[1]:bn}.reformatted.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"dplyr\")\n",
    "    options(scipen = 999)\n",
    "    con <- file(\"${_input[0]}\",\"r\")\n",
    "    fasta <- readLines(con,n=1)\n",
    "    close(con)\n",
    "    gtf = read_delim(\"${_input[1]}\", \"\\t\",  col_names  = F, comment = \"#\", col_types=\"ccccccccc\")\n",
    "    if(!str_detect(fasta,\">chr\")) {\n",
    "        gtf_mod = gtf%>%mutate(X1 = str_remove_all(X1,\"chr\"))\n",
    "    } else if (!any(str_detect(gtf$X1[1],\"chr\"))) {\n",
    "        gtf_mod = gtf%>%mutate(X1 = paste0(\"chr\",X1))\n",
    "    }\n",
    "    if(any(str_detect(gtf_mod$X9, \"transcript_biotype\"))) {\n",
    "      gtf_mod = gtf_mod%>%mutate(X9 = str_replace_all(X9,\"transcript_biotype\",\"transcript_type\"))\n",
    "    }\n",
    "    gtf_mod%>%write.table(\"${_output}\",sep = \"\\t\",quote = FALSE,col.names = F,row.names = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-manor",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**Text below is taken from https://github.com/broadinstitute/gtex-pipeline/tree/master/gene_model**\n",
    "\n",
    "\n",
    "Gene-level expression and eQTLs from the GTEx project are calculated based on a collapsed gene model (i.e., combining all isoforms of a gene into a single transcript), according to the following rules:\n",
    "\n",
    "1. Transcripts annotated as “retained_intron” or “read_through” are excluded. Additionally, transcripts that overlap with annotated read-through transcripts may be blacklisted (blacklists for GENCODE v19, 24 & 25 are provided in this repository; no transcripts were blacklisted for v26).\n",
    "2. The union of all exon intervals of each gene is calculated.\n",
    "3. Overlapping intervals between genes are excluded from all genes.\n",
    "\n",
    "\n",
    "The purpose of step 3 is primarily to exclude overlapping regions from genes annotated on both strands, which can't be unambiguously quantified from unstranded RNA-seq (GTEx samples were sequenced using an unstranded protocol). For stranded protocols, this step can be skipped by adding the `--collapse_only` flag.\n",
    "\n",
    "Further documentation is available on the [GTEx Portal](https://gtexportal.org/home/documentationPage#staticTextAnalysisMethods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-ancient",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_gtf_2 (collapsed gene model)]\n",
    "parameter: is_stranded = False\n",
    "output: f'{_input:n}{\".collapse_only\" if is_stranded else \"\"}.gene.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    collapse_annotation.py ${\"--collapse_only\" if is_stranded else \"\"} ${_input} ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-dressing",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ercc_gtf (Preprocess ERCC gtf file)]\n",
    "parameter: ercc_gtf = path\n",
    "input: ercc_gtf\n",
    "output: f'{cwd}/{_input:bn}.genes.patched.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    with open('${_input}') as exon_gtf, open('${_output}', 'w') as gene_gtf:\n",
    "        for line in exon_gtf:\n",
    "            f = line.strip().split('\\t')\n",
    "            f[0] = f[0].replace('-','_')  # required for RNA-SeQC/GATK (no '-' in contig name)\n",
    "        \n",
    "            attr = f[8]\n",
    "            if attr[-1]==';':\n",
    "                attr = attr[:-1]\n",
    "            attr = dict([i.split(' ') for i in attr.replace('\"','').split('; ')])\n",
    "            # add gene_name, gene_type\n",
    "            attr['gene_name'] = attr['gene_id']\n",
    "            attr['gene_type'] = 'ercc_control'\n",
    "            attr['gene_status'] = 'KNOWN'\n",
    "            attr['level'] = 2\n",
    "            for k in ['id', 'type', 'name', 'status']:\n",
    "                attr['transcript_'+k] = attr['gene_'+k]\n",
    "        \n",
    "            attr_str = []\n",
    "            for k in ['gene_id', 'transcript_id', 'gene_type', 'gene_status', 'gene_name',\n",
    "                'transcript_type', 'transcript_status', 'transcript_name']:\n",
    "                attr_str.append('{0:s} \"{1:s}\";'.format(k, attr[k]))\n",
    "            attr_str.append('{0:s} {1:d};'.format('level', attr['level']))\n",
    "            f[8] = ' '.join(attr_str)\n",
    "        \n",
    "            # write gene, transcript, exon\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['gene']+f[3:])+'\\n')\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['transcript']+f[3:])+'\\n')\n",
    "            f[8] = ' '.join(attr_str[:2])\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['exon']+f[3:])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-husband",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[gene_annotation]\n",
    "input: output_from(\"hg_gtf_1\"), output_from(\"hg_gtf_2\"), output_from(\"ercc_gtf\")\n",
    "output: f'{cwd}/{_input[0]:bn}.ERCC.gtf', f'{cwd}/{_input[1]:bn}.ERCC.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container\n",
    "    cat ${_input[0]} ${_input[2]} > ${_output[0]}\n",
    "    cat ${_input[1]} ${_input[2]} > ${_output[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-increase",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Generating index file for `STAR` \n",
    "\n",
    "This step generate the index file for STAR alignment. This file just need to generate once and can be re-used. \n",
    "\n",
    "**At least 40GB of memory is needed**.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `gtf` and `fasta`: path to reference sequence. Both of them needs to be unzipped. `gtf` should be the one prior to collapse by gene.\n",
    "* `sjdbOverhang`: specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads. We use 100 here as recommended by the TOPMed pipeline. See here for [some additional discussions](https://groups.google.com/g/rna-star/c/h9oh10UlvhI/m/BfSPGivUHmsJ). \n",
    "\n",
    "### Step Output\n",
    "\n",
    "* Indexing file stored in `{cwd}/STAR_index`, which will be used by `STAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-significance",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_index]\n",
    "parameter: hg_gtf = path\n",
    "parameter: hg_reference = path\n",
    "# Specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads.\n",
    "# Default choice follows from TOPMed pipeline recommendation.\n",
    "parameter: sjdbOverhang = 100\n",
    "fail_if(expand_size(mem) < expand_size('40G'), msg = \"At least 40GB of memory is required for this step\")\n",
    "input: hg_reference, hg_gtf\n",
    "output: f\"{cwd}/STAR_Index/chrName.txt\", f\"{cwd}/STAR_Index/Log.out\", \n",
    "        f\"{cwd}/STAR_Index/transcriptInfo.tab\", f\"{cwd}/STAR_Index/exonGeTrInfo.tab\", \n",
    "        f\"{cwd}/STAR_Index/SAindex\", f\"{cwd}/STAR_Index/SA\", f\"{cwd}/STAR_Index/genomeParameters.txt\", \n",
    "        f\"{cwd}/STAR_Index/chrStart.txt\", f\"{cwd}/STAR_Index/sjdbList.out.tab\", \n",
    "        f\"{cwd}/STAR_Index/exonInfo.tab\", f\"{cwd}/STAR_Index/sjdbList.fromGTF.out.tab\", \n",
    "        f\"{cwd}/STAR_Index/chrLength.txt\", f\"{cwd}/STAR_Index/sjdbInfo.txt\", \n",
    "        f\"{cwd}/STAR_Index/Genome\", f\"{cwd}/STAR_Index/chrNameLength.txt\", \n",
    "        f\"{cwd}/STAR_Index/geneInfo.tab\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:d}.stderr', stdout = f'{_output[0]:d}.stdout'\n",
    "    STAR --runMode genomeGenerate \\\n",
    "         --genomeDir ${_output:d} \\\n",
    "         --genomeFastaFiles ${_input[0]} \\\n",
    "         --sjdbGTFfile ${_input[1]} \\\n",
    "         --sjdbOverhang ${sjdbOverhang} \\\n",
    "         --runThreadN ${numThreads} #--sjdbGTFchrPrefix \"chr\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-cabinet",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Generating index file for `RSEM`\n",
    "\n",
    "This step generate the indexing file for `RSEM`. This file just need to generate once.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `gtf` and `fasta`: path to reference sequence. `gtf` should be the one prior to collapse by gene.\n",
    "* `sjdbOverhang`: specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads.\n",
    "\n",
    "### Step Outputs\n",
    "* Indexing file stored in `RSEM_index_dir`, which will be used by `RSEM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-barrier",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[RSEM_index]\n",
    "parameter: hg_gtf = path\n",
    "parameter: hg_reference = path\n",
    "input: hg_reference, hg_gtf\n",
    "output: f\"{cwd}/RSEM_Index/rsem_reference.n2g.idx.fa\", f\"{cwd}/RSEM_Index/rsem_reference.grp\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.idx.fa\", f\"{cwd}/RSEM_Index/rsem_reference.ti\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.chrlist\", f\"{cwd}/RSEM_Index/rsem_reference.seq\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.transcripts.fa\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:d}.stderr', stdout = f'{_output[0]:d}.stdout'\n",
    "    rsem-prepare-reference \\\n",
    "            ${_input[0]} \\\n",
    "            ${_output:nn} \\\n",
    "            --gtf ${_input[1]} \\\n",
    "            --num-threads ${numThreads}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
