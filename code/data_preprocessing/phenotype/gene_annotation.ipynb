{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "considerable-permit",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Gene coordinate annotation\n",
    "\n",
    "\n",
    "This workflow adds genomic coordinate annotation to gene-level molecular phenotype files generated, eg in GCT format, converting them to `bed` format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-saudi",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This pipeline is based on [`pyqtl`, as demonstrated here](https://github.com/broadinstitute/gtex-pipeline/blob/master/qtl/src/eqtl_prepare_expression.py).\n",
    "\n",
    "### Alternative implementation\n",
    "\n",
    "Previously we use `biomaRt` package in R instead of code from `pyqtl`. The core function calls are:\n",
    "\n",
    "```r\n",
    "    ensembl = useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\", version = \"$[ensembl_version]\")\n",
    "    ensembl_df <- getBM(attributes=c(\"ensembl_gene_id\",\"chromosome_name\", \"start_position\", \"end_position\"),mart=ensembl)\n",
    "```\n",
    "\n",
    "We require ENSEMBL version to be specified explicitly in this pipeline. As of 2021 for the Brain xQTL project, we use ENSEMBL version 103."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-toolbox",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "1. Molecular phenotype data with the first column being ENSEMBL ID and other columns being sample names. \n",
    "2. GTF for collapsed gene model\n",
    "    - the gene names must be consistent with the GCT matrices (eg ENSG00000000003 vs. ENSG00000000003.1 will not work) \n",
    "3. (Optional) Meta-data to match between sample names in expression data and genotype files\n",
    "    - Required input\n",
    "    - Tab delimited with header\n",
    "    - Only 2 columns: first column is sample name in expression data, 2nd column is sample name in genotype data\n",
    "    - **must contains all the sample name in expression matrices even if they don't existing in genotype data**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-issue",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "Molecular phenotype data in `bed` format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-tracy",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "\n",
    "**FIXME: please complete it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-monster",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run gene_annotation.ipynb annotate_coord \\\n",
    "    --cwd output \\\n",
    "    --phenoFile data/... \\\n",
    "    --annotation-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.gene.ERCC.gtf  \\\n",
    "    --sample-participant-lookup data/sampleSheetAfterQC.txt \\\n",
    "    --container container/rna_quantification.sif \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-portable",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suspected-frontier",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run gene_annotation.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  annotate_coord\n",
      "  annotate_coord_biomart\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd VAL (as path, required)\n",
      "                        Work directory & output directory\n",
      "  --phenoFile VAL (as path, required)\n",
      "                        Molecular phenotype matrix\n",
      "  --annotation-gtf VAL (as path, required)\n",
      "                        gene gtf annotation table\n",
      "  --sample-participant-lookup . (as path)\n",
      "                        A file to map sample ID from expression to genotype,\n",
      "                        must contain two columns, sample_id and participant_id,\n",
      "                        mapping IDs in the expression files to IDs in the\n",
      "                        genotype (these can be the same).\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 1 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "\n",
      "Sections\n",
      "  annotate_coord:\n",
      "  annotate_coord_biomart:\n",
      "    Workflow Options:\n",
      "      --ensembl-version VAL (as int, required)\n"
     ]
    }
   ],
   "source": [
    "sos run gene_annotation.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "improving-crown",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path\n",
    "# Molecular phenotype matrix\n",
    "parameter: phenoFile = path\n",
    "#  gene gtf annotation table\n",
    "parameter: annotation_gtf = path\n",
    "# A file to map sample ID from expression to genotype, must contain two columns, sample_id and participant_id, mapping IDs in the expression files to IDs in the genotype (these can be the same).\n",
    "parameter: sample_participant_lookup = path()\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "parameter: container = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-sunrise",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Implementation using `pyqtl`\n",
    "\n",
    "Implementation based on [GTEx pipeline](https://github.com/broadinstitute/gtex-pipeline/blob/master/qtl/src/eqtl_prepare_expression.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-upper",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord]\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input:bn}.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    \n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "\n",
    "    def prepare_bed(df, bed_template_df, chr_subset=None):\n",
    "        bed_df = pd.merge(bed_template_df, df, left_index=True, right_index=True)\n",
    "        # sort by start position\n",
    "        bed_df = bed_df.groupby('chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "        if chr_subset is not None:\n",
    "            # subset chrs from VCF\n",
    "            bed_df = bed_df[bed_df.chr.isin(chr_subset)]\n",
    "        return bed_df\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(${_input:r}, sep='\\t', skiprows=0, index_col=0)\n",
    "    sample_participant_lookup = \"${sample_participant_lookup}\"\n",
    "    sample_participant_lookup_s = ${\"None\" if not sample_participant_lookup.is_file() else \"pd.read_csv(sample_participant_lookup, sep='\\t', index_col=0, dtype={0:str,1:str}, squeeze=True)\"}\n",
    "\n",
    "    # change sample IDs to participant IDs\n",
    "    if sample_participant_lookup_s:\n",
    "        df.rename(columns=sample_participant_lookup_s.to_dict(), inplace=True)\n",
    "    bed_template_df = qtl.io.gtf_to_tss_bed(${annotation_gtf:ar}, feature='transcript')\n",
    "    bed_df = prepare_bed(df, bed_template_df)\n",
    "    qtl.io.write_bed(bed_df, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-meditation",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Implementation using biomaRt\n",
    "This workflow adds the annotations of chr pos(TSS where start = end -1) and gene_ID to the `bed` file. **This workflow is obsolete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-bachelor",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord_biomart]\n",
    "parameter: ensembl_version=int\n",
    "input: phenoFile\n",
    "output: f'{cwd:a}/{_input:bn}.bed.gz',\n",
    "        f'{cwd:a}/{_input:bn}.region_list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "R:  expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout' ,container = container\n",
    "    library(\"biomaRt\")\n",
    "    library(dplyr)\n",
    "    library(readr)\n",
    "    biomartCacheClear()\n",
    "    gene_exp = readr::read_delim(\"$[_input[0]]\",delim = \"\\t\")\n",
    "    if(\"#chr\" %in% colnames(gene_exp) ){\n",
    "      # need to re-annotate\n",
    "      gene_exp = gene_exp[,4:ncol(gene_exp)]\n",
    "    }\n",
    "    ensembl = useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\", version = \"$[ensembl_version]\")\n",
    "    ensembl_df <- getBM(attributes=c(\"ensembl_gene_id\",\"chromosome_name\", \"start_position\", \"end_position\"),mart=ensembl)\n",
    "    my_genes = gene_exp$gene_ID\n",
    "    keep_genes =  my_genes\n",
    "    my_genes_ann = ensembl_df[match(my_genes, ensembl_df$ensembl_gene_id),]%>%filter(chromosome_name%in%1:23)%>%dplyr::rename( \"#chr\" = chromosome_name, \"start\" = start_position, \"end\" = end_position,\"gene_ID\" = ensembl_gene_id)%>%filter(gene_ID!=\"NA\", gene_ID%in%keep_genes)\n",
    "    my_genes_ann%>%select(`#chr`,start,end,gene_ID)%>%write_delim(path = \"$[_output[1]]\",\"\\t\")\n",
    "    my_gene_bed = inner_join(my_genes_ann %>%mutate(end = start + 1) %>%select(`#chr`,start,end,gene_ID),gene_exp,by = \"gene_ID\" )%>%arrange(`#chr`,start) \n",
    "    my_gene_bed%>%readr::write_tsv( path = \"$[_output[0]:n]\", na = \"NA\", append = FALSE, col_names = TRUE, quote_escape = \"double\")\n",
    "\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "        bgzip -f $[_output[0]:n]\n",
    "        tabix -p bed $[_output[0]] -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
